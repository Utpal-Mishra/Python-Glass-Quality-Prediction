{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GlassQualityPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cZJ-h-LSye6I",
        "XZF_bu1byp_7",
        "JhNKDyAO1MiO",
        "ni0WiBNW1VdB",
        "S7qiblRq15S8",
        "zFcVKYY7V715",
        "Ltwo4bpdfUJB",
        "nZyd_Hfz2oLo",
        "Mzr01v1ThHp3",
        "MNRXrNXEH6wv",
        "o2NV_zuNP3vE",
        "HWm6WYhm4Fgj",
        "b2qB1D_KtmhL"
      ],
      "mount_file_id": "1qrqGvtw7q2n9KAsxJJsynyOQQJFyub0S",
      "authorship_tag": "ABX9TyOvRqcWc/LsGgz5bqCVDErR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utpal-Mishra/Python-Glass-Quality-Prediction/blob/main/GlassQualityPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zll5E-oEyKZC"
      },
      "source": [
        "#IMPORTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWc9WHyryF8w"
      },
      "source": [
        "###LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDVbe1u4wkpq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dense, Activation, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6A4uWh7A26m"
      },
      "source": [
        "###MOUNTING DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFd0sGgNIlUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "7b787cfa-42c9-4970-9e92-ad7b0fd71949"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAR_wjW_A6i4"
      },
      "source": [
        "###IMPORT TRAINING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciXzTV1lVO4D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "3bd7796a-3e70-4f26-c983-fb9cd5f539d5"
      },
      "source": [
        "path = \"/content/drive/My Drive/Glass Quality Prediction/Train.csv\"\n",
        "train = pd.read_csv(path)\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1358, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>36</td>\n",
              "      <td>144</td>\n",
              "      <td>172</td>\n",
              "      <td>947225</td>\n",
              "      <td>947332</td>\n",
              "      <td>439</td>\n",
              "      <td>439.09927</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>55</td>\n",
              "      <td>1144</td>\n",
              "      <td>1152</td>\n",
              "      <td>2379058</td>\n",
              "      <td>2379624</td>\n",
              "      <td>329</td>\n",
              "      <td>329.20562</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>65</td>\n",
              "      <td>950</td>\n",
              "      <td>974</td>\n",
              "      <td>1038442</td>\n",
              "      <td>1036754</td>\n",
              "      <td>300</td>\n",
              "      <td>300.12060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>123</td>\n",
              "      <td>35</td>\n",
              "      <td>41</td>\n",
              "      <td>220</td>\n",
              "      <td>1705580</td>\n",
              "      <td>1705604</td>\n",
              "      <td>6803</td>\n",
              "      <td>6803.77862</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>69</td>\n",
              "      <td>462</td>\n",
              "      <td>466</td>\n",
              "      <td>1088124</td>\n",
              "      <td>1086579</td>\n",
              "      <td>251</td>\n",
              "      <td>251.40194</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grade_A_Component_1  grade_A_Component_2  ...  x_component_5  class\n",
              "0                    0                    1  ...              0      1\n",
              "1                    1                    0  ...              0      1\n",
              "2                    1                    0  ...              0      2\n",
              "3                    0                    1  ...              0      1\n",
              "4                    1                    0  ...              0      2\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4bDWVO9Cb2n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "ab0a58f0-0dec-4f8f-ca7b-737c8df15769"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1.358000e+03</td>\n",
              "      <td>1.358000e+03</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.406480</td>\n",
              "      <td>0.593520</td>\n",
              "      <td>137.893225</td>\n",
              "      <td>78.977909</td>\n",
              "      <td>567.370398</td>\n",
              "      <td>614.032401</td>\n",
              "      <td>1.660107e+06</td>\n",
              "      <td>1.660139e+06</td>\n",
              "      <td>1903.402798</td>\n",
              "      <td>1903.896240</td>\n",
              "      <td>0.081738</td>\n",
              "      <td>0.106038</td>\n",
              "      <td>0.197349</td>\n",
              "      <td>0.035346</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>1.346834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.491357</td>\n",
              "      <td>0.491357</td>\n",
              "      <td>20.480512</td>\n",
              "      <td>55.324842</td>\n",
              "      <td>522.013094</td>\n",
              "      <td>500.505513</td>\n",
              "      <td>1.778153e+06</td>\n",
              "      <td>1.778177e+06</td>\n",
              "      <td>3839.156721</td>\n",
              "      <td>3839.163241</td>\n",
              "      <td>0.274066</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>0.398145</td>\n",
              "      <td>0.184721</td>\n",
              "      <td>0.164983</td>\n",
              "      <td>0.476138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.527000e+03</td>\n",
              "      <td>7.453000e+03</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.445290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>4.662742e+05</td>\n",
              "      <td>4.666918e+05</td>\n",
              "      <td>234.000000</td>\n",
              "      <td>234.335953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>407.500000</td>\n",
              "      <td>457.000000</td>\n",
              "      <td>1.216168e+06</td>\n",
              "      <td>1.214700e+06</td>\n",
              "      <td>346.000000</td>\n",
              "      <td>346.044490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>1041.750000</td>\n",
              "      <td>1064.000000</td>\n",
              "      <td>2.210012e+06</td>\n",
              "      <td>2.210076e+06</td>\n",
              "      <td>915.250000</td>\n",
              "      <td>915.367815</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>305.000000</td>\n",
              "      <td>1692.000000</td>\n",
              "      <td>1717.000000</td>\n",
              "      <td>1.291748e+07</td>\n",
              "      <td>1.291731e+07</td>\n",
              "      <td>37392.000000</td>\n",
              "      <td>37392.672970</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       grade_A_Component_1  grade_A_Component_2  ...  x_component_5        class\n",
              "count          1358.000000          1358.000000  ...    1358.000000  1358.000000\n",
              "mean              0.406480             0.593520  ...       0.027982     1.346834\n",
              "std               0.491357             0.491357  ...       0.164983     0.476138\n",
              "min               0.000000             0.000000  ...       0.000000     1.000000\n",
              "25%               0.000000             0.000000  ...       0.000000     1.000000\n",
              "50%               0.000000             1.000000  ...       0.000000     1.000000\n",
              "75%               1.000000             1.000000  ...       0.000000     2.000000\n",
              "max               1.000000             1.000000  ...       1.000000     2.000000\n",
              "\n",
              "[8 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwtT91wkA_lO"
      },
      "source": [
        "###IMPORT TESTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMzhQKLBC_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "4bf0ae29-82db-4718-dd2a-17d80ec96377"
      },
      "source": [
        "path = \"/content/drive/My Drive/Glass Quality Prediction/Test.csv\"\n",
        "test = pd.read_csv(path)\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(583, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>65</td>\n",
              "      <td>33</td>\n",
              "      <td>54</td>\n",
              "      <td>1646439</td>\n",
              "      <td>1646893</td>\n",
              "      <td>632</td>\n",
              "      <td>632.39175</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>99</td>\n",
              "      <td>125</td>\n",
              "      <td>132</td>\n",
              "      <td>189874</td>\n",
              "      <td>189529</td>\n",
              "      <td>421</td>\n",
              "      <td>421.92861</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>75</td>\n",
              "      <td>12986873</td>\n",
              "      <td>12986862</td>\n",
              "      <td>272</td>\n",
              "      <td>272.21221</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>39</td>\n",
              "      <td>57</td>\n",
              "      <td>177</td>\n",
              "      <td>309634</td>\n",
              "      <td>310824</td>\n",
              "      <td>3312</td>\n",
              "      <td>3312.31058</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>142</td>\n",
              "      <td>75</td>\n",
              "      <td>23</td>\n",
              "      <td>79</td>\n",
              "      <td>5368307</td>\n",
              "      <td>5367467</td>\n",
              "      <td>862</td>\n",
              "      <td>862.49918</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grade_A_Component_1  grade_A_Component_2  ...  x_component_4  x_component_5\n",
              "0                    1                    0  ...              0              0\n",
              "1                    0                    1  ...              0              0\n",
              "2                    0                    1  ...              0              0\n",
              "3                    0                    1  ...              0              0\n",
              "4                    1                    0  ...              0              0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpUbv4CyOu8"
      },
      "source": [
        "#DATA VISUALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y91jBSk0B0OK"
      },
      "source": [
        "###TRAINING DATA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptVLVHiECEhd"
      },
      "source": [
        "NULL VALUES CHECK UP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz5ZCQtZXMbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bbca6c1-a4ae-4cfb-9661-f4f637f76e88"
      },
      "source": [
        "train.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLQZDFxMB2R0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40af47c6-71f1-4b2a-f144-fea6e8b378d4"
      },
      "source": [
        "train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN3KX3HOaqEo"
      },
      "source": [
        "#train.fillna(0, inplace = True)\n",
        "#train.isnull().values.any()\n",
        "#train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aTNXViZEbLZ"
      },
      "source": [
        "CLASS LABEL ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-cyxkE4CjWr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0f2aaca1-29f6-4767-e631-15fac1553f6d"
      },
      "source": [
        "train['class'].plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff40eeac438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-g3FwtbCjaX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b1270848-4138-479f-af82-0c14afce2af3"
      },
      "source": [
        "train['class'].plot.density()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff40edf70b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmRpLlR9CjUj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "de143428-fe07-4e6c-93c2-78a301c09c9b"
      },
      "source": [
        "train['class'].plot.bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff40e930b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAt3QUAjCqkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "23c986dd-d8a9-4939-b59a-4734e99247ed"
      },
      "source": [
        "train['class'].plot.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff40dc67588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SLtu_z0EqT3"
      },
      "source": [
        "CLASS LABEL VALUE COUNT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWZbXNknfle",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9966337d-17a4-4f8d-ade7-04d8df4217ff"
      },
      "source": [
        "train['class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    887\n",
              "2    471\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWSglRRDkbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "6ad54169-5fa6-4c99-91d6-637906a97c66"
      },
      "source": [
        "train['class'].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff40cab46d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Ovar5FE03B"
      },
      "source": [
        "TRAINING CORRELATION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZe7frLpxTzO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f506b3ba-ad20-4b60-b81a-8986dbe90eb8"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "cmap = cm.get_cmap('jet', 30)\n",
        "cax = ax1.imshow(train.corr(), interpolation=\"none\", cmap=cmap)\n",
        "ax1.grid(True)\n",
        "plt.title('Glass Quality Attributes Correlation')\n",
        "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcmADmiAIvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "bf3cf02c-85c9-4767-b5b6-89c815d95b6c"
      },
      "source": [
        "corr = train.corr()\n",
        "ax = sns.heatmap(corr, vmin =- 1, vmax = 1, center = 0, \n",
        "                  cmap = sns.diverging_palette(20, 220, n=200),\n",
        "                  square = True)\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right'\n",
        ");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAFLCAYAAACUQIglAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd7gdVdWH39+9qZCQApEuifQeIKGIQICAQWkiAoJSBANiQRSRojRFI9gbEDEEBOkgAUF6kR56lc4nIIJUQWqS9f2x9smdnJxzz5y5dW7W+zz7uXNm9pq9Z865s2bvvfZvy8wIgiAIgp6ipacrEARBECzYhCMKgiAIepRwREEQBEGPEo4oCIIg6FHCEQVBEAQ9SjiiIAiCoEcJRxQEQbCAIWmapJclPVTnuCT9WtKTkh6QtG7m2F6Snkhpr86oTziiIAiCBY/pwKR2jm8DrJjSZOAkAEkjgaOBDYD1gaMljehoZcIRBUEQLGCY2U3Aa+1k2QE4w5zbgeGSlgQ+CVxtZq+Z2evA1bTv0HLRr6MnCMrPhGN+27S8xg8fObtQWWv/8JSmbd4Y9dFCZX04a3Yhu4H9mv+36Nfafe90rY/c2W1lDV1x9UJ21zzX3jOuPmOfu7dpm2Grr1OorH4LD23a5sOFFilUFsDQoUNV2Jjm/k9vPPbr++MtmQpTzWxqE8UtDTyX+fx82ldvf4cIRxQEQdDHSE6nGcfTo0TXXBAEQVDNC8Cymc/LpH319neIcERBEAQloLWlJXfqBGYAe6bouQ2BN83sReBKYGtJI1KQwtZpX4eIrrkgCIISoA6NMFWfS2cDE4DFJD2PR8L1BzCzk4HLgU8BTwLvAPukY69J+gEwM53qODMrNiCYodsdkaRngXFm9koB27HAvcA2Zva3BnmXAH4JjAfeAF4Cvmlmjzdd6V6CpAnAB2Z2azt5NsWvey1gNzO7oJuqFwRBSTCzzzc4bsBX6xybBkzrzPp0ShtOUnc5tM8DN6e/dZEk4GLgBjNb3szWAw4HFu/6KnYpE4CPN8jzT2Bv4M9dXZkgCLqPbu6a61Zy1VjS9yU9JulmSWdLOkTSDZJ+Keku4CBJ20m6Q9K9kq6RtHiyXVTSVZIelnQqoMx5vyDpTkn3STpFUms7dRDwOfwhu5WkQe1UeXPgw9TEBMDM7jezv6c+zxMlPSTpQUm7pvNPkHSjpEskPS1piqQ9Uv0elLR8yjdd0smS7pL0uKRt0/5Bkk5Lee+VtHnav7ekiyT9Lc1EPiFzTVtLuk3SPZLOlzQk7X9W0rFp/4OSVpE0GjgAODjdr01qXbiZPWtmDwBz2rk/SJqcruGuf919S3tZgyDoBUjKncpGQ0ckaTzwWWBtfLbtuMzhAWY2zsx+hrdUNjSzdYBzgENTnqOBm81sdbyV8tF03lWBXYGNzWwsMBvYo52qfBx4xsyeAm4APt1O3jWAu+sc2wkYm65nInBimqhF2ncAsCrwRWAlM1sfOBX4euYco/FZxZ8GTk5O8at4i3ZNvMV2esZZjk3Xuiawq6RlJS0GfA+YaGbrAncB38qU8UrafxJwiJk9C5wM/MLMxprZ39u5/oaY2dT03Y1bar2NO3KqIAi6gRYpdyobebrUNgYuMbP3gPckXZo5dm5mexng3PRQHwA8k/Zvij/8MbO/Sno97d8SWA+YmTz4YODldurxedzBkf7uCVyYo/7VfAI428xmAy9JuhEfR/ovMDNFhiDpKeCqZPMg3sqqcJ6ZzQGekPQ0sEo672/Sdf5D0v8BK6X815rZm+m8jwDLAcOB1YBb0vUPAG7LlHFR+ns36f4FQbDg0tJSPgeTl46O7fwvs/0b4OdmNiMNqh/TwFbA6WZ2eKNCUpfdZ4EdJB2ZbBeVNNTM3qph8jCwc476V/N+ZntO5vMc5r1X1TOcG814zp53djqXcKmMeuNd71flD4JgAaaMLZ285BkjugXYLo2BDAG2rZNvGG0Tm7KKrDcBuwNI2gaoCORdC+ws6SPp2EhJy9U595bAA2a2rJmNNrPl8NbQZ+rkvw4YKGmuxIWktdK4yt/x7rFWSaPwFluzmimfk9SSxo0+BjyWzrtHKmslvAvysXbOcTuwsaQVks3Cya493gKa1yUJgqD09OWuuYaOyMxm4pObHgCuwLup3qyR9RjgfEl3A9nQ7GOBTSU9jHcx/TOd9xF8jOQqSQ/g4nlLUpvP4+NLWS6kTvRcCj38DDBR0lOp7B8D/07neQC4H3dYh5rZv+tdfx3+iTuvK4ADUrfl74EWSQ/iXZZ7m9n79U5gZv/BAy/OTtd/G97F1x6XAp9pL1hB0nj5vIDPAaekaw+CoOT05WAF+TO7QSZpiJm9LWkhvIUz2czu6fLa9UIkTQcu60vzc0L0dF5C9LSNED2dl54UPf3MT6fl/j+9+JAvlcob5f2PmyppNWAQPq6zQDqhvkoRp/K91dqdylWX3w9pfirXmHdfb5ypBnM+/LCQXcuAgU3bqLX7hvHuHD6628padWCxnuD1Xr29kN1No1Zr2mbswqMKlTV0UPPf85CGw8FdR+uCHqxgZrt3dUUqSLoDqP6FfNHMHqyRd1F8rKmaLc3s1a6on5nt3RXnbZYUtPG5qt3nm9nxPVGfIAi6ljJOVM1Lr4vGMrMNmsj7Kj5HZ4EjOZxwOkGwgNCivuuI+u6VBUEQBKWg2x1Rkq9ZrKDtWEkmqeHStJKWkHROipq7W9LlOcKjezVyGaJ2teYkfUvSI5IekHRtOyHxQRCUiBblT2UjRE/LxQQai57ei6ubrwVcAJzQIH8QBCWgL4dv53Igkr4PfAH4D75e+d34xNb7SJI5kh7H5wUNAF4F9jCzl1JAwdn4uua3USV6Cnwj2dwBHJikd2rVoSJ6uhXwd0mD0vydWtQUPc2c5wRcN8+AH5rZuUkN4lh8yYg1gfPwOVMH4fJDO5rZUyl8+z1cc28R4FtmdlnSlTsp7Z+V9l8vaW9ge2AhYHngYjM7NNVl61TmQOApYJ8UJv8scDqwHb5GyOdSmQcAs9N9+3otvTkzuz7z8Xb8ewuCoOT0b62rCV16QvS0b4ue7otPup0PZdS3Zzzd7HzeIAiCziNET/uo6GlqNY0DNqt13MymAlMBbt5l456bHBEEQS7K2OWWlxA9nZc+IXoqaSJwJLBZezJDQRCUhzIGIeQlRE/7mOippHWAU4Dtzay9FmYQBCWipaUld8qDpEnyBU+flHRYjeO/SLqW98kXAX0jc2x25tiMDl9bowwhelqTXit6CpwIDMG/i075kQRB0LdIvUy/w8f9VwM+n2Tc5mJmB6fx6LF4j9dFmcPvVo6Z2fYdrk+InjZHXxQ9LTJGVFhrbvKuTduM4Z1CZfVZrbnX6gWLdj6rLrNEIbvZd1xdyO6mhZuf9jZ2zDKFyiqkNddSfDi1o6KnXzn1gtyFn7Tfzu2WJWkj4Bgz+2T6fDiAmf24Tv5bgaPN7Or0+W0zG5K3Po0I0dOgkCJ2EfFSgAOnnts4UxV/+0q9Htj26VfAoQDQy6VUll+80HzwQizyfq0h2Ma8t1QxxfSxI5t3KsMGD2qcqQYLaU4hu56ik2MVlsan4lR4Hqgpr5aGTMbgPUgVBkm6C5+qMsXM/tKRyoToaZOE6GkQBD1BvyZET9P4+OTMrqkpUrYIuwEXVM3xXM7MXpD0MeA6SQ+mqTWFCNHTkhKip0EQ1CM7PaMOLwDLZj4vQ1uwWTW74fMks+d/If19WtINwDr4pPxC9O4+iCAIggDodImfmcCKksZIGoA7m/kCmyStgkc635bZN0LSwLS9GD7X9JGOXFuvaxEFQRAE89OvEyV+zGyWpK8BVwKtwDQze1jSccBdZlZxSrsB59i8UW2rAqdImoM3ZqakKOjChCOqQ9KIG2dmX+uEcx0AvGNmZ3S4Ym3nvNXMPi5pNPBxM/tzZ507CILeR2dPaDWzy4HLq/YdVfX5mBp2t+JyZZ1GOKJuICu+2onnrKhwj8YnDIcjCoKglJRujEjSaEn/kDQ9zfY9S9JESbdIekLS+indJuleSbdKWjnZHixpWtpeU9JDaW5UozKnS9o58/nt9HeCpBslXSLpaUlTJO0h6U5JDyblBSQdI+mQtH2DpJ+kPI9XJqYm5YrTkt29kjZP+1dPee+TrzG0YrYOwBRgk3T8YEk3SRqbqevNktaucU1zRU9PO/f8Il9FEATdSCePEfUqytoiWgEPXf4SPui2Oy46uj1wBC6IuknqB50I/AjXqvsVcIOkz+BabPubWbHZkm2sjfeZvgY8DZxqZutLOghX7P5mDZt+Kc+ncHXyiWTUu9MA4VVyyZ8DgF+Z2VlpULG6o/gwXJ17WwBJr+GKDd9M9oMqS2BkyUbVvPX4QyF6GgS9nDI6mLyU1RE9U5lXJJfvudbMTC6vMxrXvTs9tR4MX9MHM5uTxn4eAE4xs1s6oS55FbuzZJW1R6fteurdtwFHSloGuMjMnmhQn/OB70v6Du6opzd7QUEQ9D6amUdUNsp6ZY1Usn8AXG9ma+CLy2WnXq8IvA0s1UR5s0j3SlILvmRD3rq0V/+GytopCGF74F3gcklbNMj/Dq7btwOwC3BWe/mDICgHfblrrqyOqBFZJfC9KzslDQN+jStuL5od92nAs/jaSeBOoX+n1HJeaqp3y2cuP21mvwYuAdaqsqulyH0qfp0zzex1giAoPVL+VDb6qiM6AfixpHuZt8XxC+B3ZvY4vnrpFKVlKBrwB2AzSfcDGzHvOkydRT317l2AhyTdh688Wx0C/gC+fPj9kg4GMLO78YX+TuuCegZB0AO0trTkTmUjl/p2UC4kLYUvp75KWkm2XYoEKzxXAtFTFZ0A2MtFT19pGdxtZS02591Cdu+9/K9Cdi+NXLZxpirKInraUfXt4y68Kvf/6VGf3bpU7aKyBisEdZC0J65B9608TgjgjVHNKyWPebdYj18RpzLppOqlqPIx7at7FLLrX8CBDX3h8UJlFWHhOd338vjmmNUL2Q1eZvlCdsPuurFpm0VWmW92Qi5mDRnetM0tzxRzsACT1lq5sG1fZ4F3RJL2AQ6q2n2LmX21Vv7eTlJv6DQFhyAIegdlDELIywLviMzsNGIsJQiCXk6EbwdBEARBFxGOqAkkDZd0YNqeIOmyOvlOVdX671XH50r+BEEQ5KGlRblT2QhH1BzDgQMbZTKz/Toqix4EQZAlJrQGFaYAy6c5PScCQyRdIBdhPUvpF5CETcel7UmS7knzfOZb1lzSlyVdIWlwO4KorZJOlDQzCZ/un/YvmURO75MLuG6S8k5Pnx+szC0KgqDctEq5U9kIR9QchwFPmdlY4Dv48rjfBFYDPoavVDgXSaPwybCfNbO1caHW7PGvAdsCO5pZZcJGPzNbP5336LRvX+BNMxsPjAe+LGkMLvZ6ZarP2sB9+NLpS5vZGma2JnUCMbLq22dNj1iNIOjttEi5U9lY4KPmOsidZvY8QGoljQZuzhzfELjJzJ4BMLPXMsf2BJ7DndCHmf21BFG3BtbKSBINwzXzZgLTJPUH/mJm90l6GviYpN8Af6VNhHUesurbz73+35jVHAS9nDJ2ueUlWkQdIyt42lDAtIqKUvgydc6ZPZ+Ar5vZ2JTGmNlVZnYTrpv3AjBd0p5JW25tXFnhAFx3LgiCkhNjREGFWgKj7XE7sGnqRkPSyMyxe4H9gRlJkqc9rgS+klo+SFpJ0sKSlgNeMrM/4A5nXUmLAS1mdiHwPWDdJuobBEEvpbVFuVPZCEfUBGb2KnCLpIfwYIVG+f8DTAYuSoKp51Ydvxk4BPhrciD1OBV4BLgnlX0K3lqaANyfxF13xRf+Wxpf/O8+4Ezg8KYuMgiCXklnt4hSINVjkp6UdFiN43tL+k8KhrpP0n6ZY3vJV8R+QtJeHb22GCNqEjPbvc7+r2W2J2S2rwCuqMp7TGb7SrzFA+5YKvtfIY0RJc24I1LKcnpK1UQrKAiCukhqBX4HbAU8D8yUNKPGtJNzs8+2ZDsSD6Qahy88eneyLbzkTDiigA9nzW7aZs6HHzbOVIN+AwY2bVNUvPRLv+u+NQHPP2Tfbitr1i1XNM7USby75AqF7Pq/8Fghu9nvNa/2/d+FRzbOVIP3P5zVtM0Ki48qVFZnUESMtx3WB540s6cBJJ2DL6aZZ/7jJ4GrK8FXkq4GJgFnF61MdM0FQRCUgGbCt7PTM1KaXHW6pfGo3QrPp33VfDbNXbxAUmWNjry2uYkWURAEQR8jOz2jA1wKnG1m76dJ9KcDW3S4cjWIFlEQBEEJ6ORghReA7CqEy6R9czGzV9Mq0eABU+vltW2WcER9AEkHyBfEC4KgjyLlTzmYCawoaYykAcBuwIx5y9OSmY/bA4+m7SuBrSWNkDQCn3B/JR0guub6AGZ2ck/XIQiCrqW1E9cjMrNZSWLsSqAVmGZmD0s6DrjLzGYA35C0PTALeA3YO9m+JukHuDMDOK5KNaZpwhH1EiSNB/6IR7O0AncCJ+H6dG8AawLn4YoMBwGDcXmgpyQdA7xtZj+VdANwB7A5rha+r5n9vXuvJgiCzqazFRPM7HLg8qp9R2W2D6fOPEQzmwZM66y6RNdcL8HMZuJN4x8CJ+CTUR/C5XoOAFYFvgislERRTwW+Xud0tYRTgyAoMS3Kn8pGOKLexXH4BLNxuDMCmGlmL6ZBw6doEzGtaNXVopZw6jxkwzvPPmN6x2seBEGX0trSkjuVjeia610sCgwB+gOD0r6ssOqczOc51P/+agmnzkM2vPPp/7we6ttB0Msp4/IOeQlH1Ls4Bfg+MAb4CXBBz1YnCILeQhlVtfMSjqiXkMKvPzSzPycdqFtp62ILgmABJxxR0OWY2RnAGWl7NrBBOnRdJs+EzPYN+JpD1SKq2TxzhVODICg3ZVzeIS/hiIIgCEpAtIiCPs3Afs3/DFoKqGgDoOYjejpZdbhLMOu+eI8BI7tPAfrD1mIRWHM+eL9xphr0H7JI80YFo8SKPNhb+nCrpCcJRxQEQVACWui7TjAcURAEQQloKeH8oLyEIwqCICgB/Vr7bouo77rYIAiCoBREiygIgqAE9OWouWgR9SCSxqdleAdJWljSw5K+JulGSZdIelrSFEl7SLpT0oOSlk+220m6Q9K9kq6RtHja/ytJR6XtT0q6SSoQqhYEQa+iVS25U9koX437EB1U3L4Z2NDM1gHOAQ5N+w8HdpW0OfBrYB8zm1Nddlb09Mzpp3XVJQZB0El08gqtvYromut5jsMXmHoP+AawCUlxG0BSteL25ml7GeDctIriAOAZADN7R9KXgZuAg83sqVqFZkVPX3j9rRA9DYJeTl+ewhQtop6norg9lOYUt38D/NbM1gT2z9iCL6L3KrBUF9U5CIJupqWlJXcqG+Wrcd+jorh9Fq64nZdhwAtpe6/KTknLAd8G1gG2kbRBDdsgCEpGi5Q7lY1wRD1IVnEbmAKMJ/93cgxwvqS7gVfS+YQvN36Imf0L2Bc4VdKgumcJgqAUdLYjkjRJ0mOSnpR0WI3j35L0SAqouja95FaOzZZ0X0ozOnptMUbUg3RQcfsS4JIap52YyX833k0XBEEwl7TUzO/wFaGfB2ZKmmFmj2Sy3QuMS+POX8EDqnZNx941s7GdVZ9wRAH9CghbqrX7fjpDX3i8kN35h+xbyK6IgOkuP5tWqKwiXPrtvRpn6iQGFLTrP3qlQnYfDh7StM2A998qVBYDhzZtMqh/f4a89Uqx8hhR0M7p5Gi49YEnzezpdO5zgB2AuY7IzK7P5L8d+EJnViBLdM0FQRDkpLgT6jj9Wltyp+z0jJQmV51uaeC5zOfn07567Atckfk8KJ33dkk7dvjaOnqCIAiCoOtpJgghOz2jo0j6AjAO2Cyzezkze0HSx4DrJD1Yb6pIHqJFFARBsODxArBs5vMytEXhzkXSROBIYHszmzutxMxeSH+fxset1+lIZcIRBUEQlICWFuVOOZgJrChpjKQBwG64ystcJK2DTy/Z3sxezuwfIWlg2l4M2JjM2FIRomuuDyDpAOCdFIUXBEEfpDPnB5nZLElfA64EWoFpZvawpOOAu8xsBnAiPtn+/BQo8U8z2x6XHjtF0hy8MTOlKtquacIR9QHM7OSerkMQBF1LZ2vImdnlwOVV+47KbE+cz8j330onTwsJR9RLSG8ir5nZL9Pn44GXgZ2AN/Av/jxcb+4gYDCwo5k9JekY4G0z+6mkG4A7cE264cC+Zvb3br6cIAg6mTIqJuQlxoh6D9OAPQHSsg274SGVeZS4q+mX8nwTOLpWhmx4559O6745MEEQFKO1pSV3KhvRIuolmNmzkl5NA4SL47OaXyWfEnc1F6W/dwOj65Q3N7zzpf/+L9S3g6CXU8blHfISjqh3cSqwN7AE3kKCfErc1VTyzG4nTxAEJSKWgQi6i4uBSbj46ZU9XJcgCIJuId6WexFm9oGk64E3zGx2X26KB0HQHP1aWnu6Cl1GOKJeRApS2BD4HMyrtp0+T8hszz1mZsfUyfMKdcaIgiAoF335xTS65noJklYDngSuNbMnero+QRD0LqT8qWxEi6iXkGYmf6yn6xEEQdDdhCMKgiAoAUXWDSsL4YiCIAhKQF8eIwpHFARBUAJaCEcUBEEQ9CA5l3coJX2307EESDpO0jczn4+XdJCkGyVdIulpSVMk7SHpTkkPSlo+5d1O0h2S7pV0jaTF0/5fSToqbX9S0k0pLDwIghLTIuVOZSMeUD1LR4RObwY2NLN1gHOAQ9P+w4FdJW0O/BrYx8zmVBccoqdBUC4k5U5lI7rmepAOCp0uA5wraUlgAPBMOuc7kr4M3AQcXG8d+RA9DYJyUUYHk5doEfU8FaHTfWhO6PQ3wG/NbE1gf2BQxmZN3KEt1TVVDoIg6DzCEfU8RYVOhwEvpO29KjslLQd8G1gH2EbSBp1UzyAIepBBNit3KhvhiHoYM/sAuB44z8xmN2F6DL6W/N3AKwDytvsfgUPM7F/AvsCpkgbVPUsQBAskkiZJekzSk5IOq3F8oKRz0/E7JI3OHDs87X9M0ic7WpcYI+phOiB0eglwSY1TTszkv5tOXls+CILyI6kV+B2wFR4gNVPSjCQ1VmFf4HUzW0HSbsBP8ECo1fDAqtXx7v9rJK3U5Iv0PESLqAcJodMgCHqI9YEnzezp1CtzDrBDVZ4dgNPT9gXAlqnXZQfgHDN738yewZ9h63ekMtEi6kF6i9Bp6yN3Nm1z5/DRhcpafvHFmrZZeE6xoL5Zt1xRyG7AyFFN21z67b0aZ+oktvvZ6Y0zdRJX7F/9bMrHnNnFXo6nXHdv0zbf26xYo3+R+Wc1NOTdV18qVBbA4CWXLWzbLJImA5Mzu6amSNkKSwPPZT4/D1SPJ8/NY2azJL0JLJr2315lu3RH6huOKAiCoI+RnZ5RBqJrLgiCYMHjBSDbRFuGtijc+fJI6odH6r6a07YpwhEFQRAseMwEVpQ0RtIAPPhgRlWeGbRNDdkZuM7MLO3fLUXVjQFWBJrv388QXXNBEAQLGGnM52v43MVWYJqZPSzpOOAuM5uBTwX5k6QngddwZ0XKdx7wCDAL+GpHIuYgHFFhJJ0K/Lwq3DGP3WjgMjNboyvqFQRB32TgrPcbZ5rL0IY5zOxy4PKqfUdltt8jTSupYXs8cHwTFWqXcEQFMbP9eqpsSf3MSjh9OgiCwtic5qP8ykKMETVA0mhJ/5B0lqRHJV0gaSFJN0gaJ2k5SU9IWkxSi6S/S9paUqukEyXNlPSApP2bKO/vku5J6eNp/4S0fwbwSL3zSxoi6dpk+6CkmvG3WfXtM/5yWafdryAIugibkz+VjGgR5WNlYF8zu0XSNODAygEz+z9JPwFOwgfsHjGzq1Ic/5tmNl7SQOAWSVcBjSbFvAxsZWbvSVoROBsYl46tC6xhZs+0c/7ngM+Y2X8lLQbcnmZMz1NuNrzzlduvD/XtIOjlWMH5dGUgHFE+njOzW9L2mcA3sgfN7FRJn8PXEBqbdm8NrCVp5/R5GB5d8niDsvoDv5U0FpgNrJQ5dmeaydze+Z8HfiRpU1yte2l8iYl/573YIAh6ISVs6eQlHFE+ql9F5vksaSE8lh5gCPAWIODrZnZlVd7RDco6GHgJXxyvBXgvc+x/2VPVOf/ewChgPTP7UNKzzLtERBAEJSTGiIKPStoobe+Or46a5SfAWcBRwB/SviuBr0jqDyBpJUkL5yhrGPBiWlX1i3hoZS3qnX8Y8HJyQpsDy+W6wiAIgh4iWkT5eAz4ahofegQfD9oOQNJm+FpCG5vZbEmflbQPvuDdaOCeJBT4H2DHHGX9HrhQ0p7A35i3FZSl3vnPAi6V9CBwF/CP5i83CIJeR3TNLfDMMrMvVO2bkNnesLJhZjtl9h+RUpY3gbpziJIK91qZXd9N+29g3uUh5tQ5P8BGNfYFQVBi5nz4YU9XocuIrrkgCIKgR4kWUQPM7FnaacEUJa1q+JOq3c+Y2Wc6u6wgCPoAFuHbQSeTot2ubJgxCIIAsHBEQRAEQY8SwQpBe0h628yG9HQ9giDou1jBVW/LQDiikhLCp0GwgNGHu+Yiaq4TkXOipIeS4OiuaX+LpN8n8dSrJV2ekeapdZ6jkpjpQ5KmpnlCJKHVX0q6CzhI0nqSbpR0t6QrJS2Z8n052d8v6cKk/BAEQYkxs9ypbIQj6lx2wrXm1gYmAicm57ATPvl0NVwtodE8n9+a2fi0ZtFgYNvMsQFmNg74NfAbYGczWw+YRtv6IBcl+7WBR4F9qwsI9e0gKBc2Z3buVDaia65z+QRwdlqt8CVJN+KqC58Azk+TUP8t6foG59lc0qHAQsBI4GHg0nTs3PR3ZTys/OrUYGoFXkzH1pD0Q2A4rn03X3ReqG8HQdBbCEfUy5A0CJf5GWdmz0k6hnlFSyuSPwIeNrNaravpwI5mdn8SQZ3QZRUOgqB7CNHTICd/B3ZNi9aNAjbF1yi6BfhsGitanPYdQ8XpvCJpCFBvLOkxYFRFjFVSf0mrp2NDgReTIOoeHbqiIAh6BTZ7du7UESSNTGPZT6S/I2rkGSvpNkkPp4U5d80cmy7pGUn3pTS22r6aaBF1Lhfj4z/340tFHEojg0sAACAASURBVGpm/5Z0IbAlLpj6HHAPrjk3H2b2hqQ/AA/hawjNrJPvgxTw8GtJw/Dv8pd4N973gTtwIdQ7yLOAfRAEvZpuHPs5DLjWzKZIOix9/m5VnneAPc3sCUlLAXdLutLM3kjHv2NmF+QtMBxRJ1CZQ5RWQf1OStnjcyQdYmZvS1oUbyU92M75vgd8r8b+CVWf78NbXdX5TsIVwoMgCJplB9p6bU7HxZbncURm9nhm+1+SXsbXQXuDAqiMoX5lRNINePDAAOAEM5veoxXK8P6rLzf9I/jvwGKNrEXef6tpmzcHFJsr/PZ77xey69fafI/1iP7d18vd7+1C/+uF2OaUSwrZXTF5u0J2s99v/jvrv8jwQmUV4d0X/1nYdvga49SRsl+/97bc/6cj1/34/sDkzK6pKUCpIZLeMLPhaVvA65XPdfKvjzus1dNL93S8Z+h94FrgMDNr94uNFlE3Ud2aAZB0MTCmavd3q1ddDYIgsCYkfrJRsbWQdA2wRI1DR1adxyTVdYBpesqfgL2srYKH48MKA1Idvgsc1159wxH1IKG0HQRBXmx250XNmdnEesckvSRpSTN7MTmal+vkWwT4K3Ckmd2eOXdlGsn7kk4DDmlUn4iaC4IgKAM2J3/qGDOAvdL2XsB8/bOSBuDBWWdUByVkFF6Erxr9UKMCwxEFQRCUAJszJ3fqIFOArSQ9gSvETAGQNE7SqSnPLnig1N41wrTPkvQgHpC1GPDDRgVG11yJkHSEmf2oQZ5puCTQy0kiKAiCvkA3BZaZ2av4dJPq/XcB+6XtM4Ez69hv0WyZ0SIqF0fkyDMdmNTF9QiCoJvpy1pzpXNEksanmbyDJC2cZvbWfPOX9N2kgn2/pErzcqyk29M5Lq7MGk7K1r9IQqCPpnIuSrOLf5jyjE4K2melPBdUlK0lbSnp3lTeNEkD0/5nJR0r6Z50bJW0f+GU785kt0Pav3cq92+p7BPS/inA4NQEPqve/TGzm4DXctzHuaKnp55+Rv4vIAiCoJMpnSMys5n4YNoPgROAM81svsEwSdvgE7M2SCrUJ6RDZ+Ah0mvhfZhHZ8w+SMrWJ+MDdF/FhUX3ThNRwcVGf29mqwL/BQ5M+nDTgV3NbE28y/MrmfO+Ymbr4pNMKxEkRwLXmdn6wOa4UvfC6dhYYFdgTVwyaFkzOwx418zGmlmHZXvMbKqZjTOzcfvttWdHTxcEQRfTXRI/PUHpHFHiOGArYBxtDqaaicBpZvYOgJm9lqRwhpvZjSnP6cyrTDAj/X0QFxR9MU3EehpYNh17zsxuSdtn4sraKwPPZGYbV5/3ovT3bnw5CICtgcMk3YfPXB4EfDQdu9bM3jSz93BZoOXauRdBECwA9OWuubIGKyyKL2/QH3+A/6/97LmpzP6dk9mufK7cq+oRwzwjiJVzzc6cR8BnzeyxbEZJG1SVnbUJgmBBpQ+r4JS1RXQKLux5FvCTOnmuBvbJjOGMNLM3gdclbZLyfBG4sY59PT5aUbwGdgduxpWwR0taoYnzXgl8PcXaI2mdHGV/mBS1gyAI+gylc0SS9gQ+NLM/4/Ht4yXNFy5oZn/Du9ruSt1flbGZvfDxmAfwsZh2pSdq8BjwVUmPAiOAk1IX2j7A+Sl+fg4+ztQeP8BbdA9Iejh9bsTUlL9usIKks4HbgJUlPS9pvtVZgyAoH904j6jbCdHTJpA0Grisr83P+et9/2j6R7Deq483zlSDwUt9tHGmKlqWWb5QWbOeeriQ3ZwPmhfeXHj0SoXKKkKR+nUEtTSv1bnN1EsbZ6rBpXts3rTNgBGLNs5UCzX/Hv6//3uiWFnAyHU37pDo6b+v+Uvu/9MlJu7YobK6mxh7CIKgLkWcUNA12KxZPV2FLqP0jkjSmrj6a5b3zWyDzi7LzJ7Fw7l7lBRKfm2NQ1umWdFBEASlofSOyMwexMd6FhiSs1mgrjkIFnT68jBK6R1REATBgkAZJ6rmpXRRcwsyktrVmpO0rKTrJT2SpI8O6q66BUHQxZjlTyUjHFG5aCR6Ogv4tpmtBmyIh5mv1vXVCoIgKE7pHFGIntYXPU2SRPek7beAR4GlO+3mB0HQY/RliZ/SOaIQPc0neprmPK0D3FHn+Fz17b9deF6j0wVB0MPY7Fm5U9konSNKhOhpO0gaAlwIfNPM/lsrT1Z9e9Jnd2nm9EEQ9ABmljuVjbJGzYXoaR2SFt2FwFlmdlGj/EEQBD1NWVtEIXpag3SuPwKPmtnPc5wvCIKyMMfypw4gaaSkq9MY9dWVcfQa+WanMev7JM3I7B8j6Q5JT0o6V9KARmWWzhGF6Gm7oqcb405wi8wP5FM5zhsEQS/HZn+YO3WQw/DhgRVxBZfD6uSrjFmPNbPtM/t/AvzCzFYAXgcaCi+H6GkThOhpGyF6Oi99VfS0qNZciJ7OT0dFT589++Tc/6ejP39A4bIkPQZMMLMXJS0J3GBmK9fI97aZDanaJ+A/wBJmNiv1Hh1jZp9sr8yyjhEFncjY5+5t2uamUcWmJ40duUzTNsPuarb31Jn93ruF7PoPWaRpmw8HD2mcqZOYcl3z31dRDt9ghcaZalDEoQBsd9b1Tdtcsf8OhcpSS/OO6IPX/lOorO5G0mRgcmbXVDObmtN8cTN7MW3/G1i8Tr5Bku7C5y9OMbO/4OP3b5hZJXTveXJMISm9IwrR03kI0dMg6Ks0MfaTnE5dxyPpGmCJGoeOrDqPSapX8HJm9oKkjwHXpWGJN3NXMkPpHVGIngZBsCDQmRNVzWxivWOSXpK0ZKZr7uU653gh/X1a0g34vMULgeGS+qVW0TLAC43qU7pghSAIggWRblyhdQYe1EX6e0l1BkkjMuoxi+GBUo+YBx1cD+zcnn014YiCIAjKgM3JnzrGFGArSU/gwgAVebRxkk5NeVbFI5Lvxx3PFDN7JB37LvAtSU/iY0Z/bFRg6bvmFiQkHWFmP2rn+CDgJmAg/t1eYGZH18sfBEF56ISWTr5yvOt/yxr77wL2S9u34hJkteyfBtZvpsxoEZWLRurb7wNbJG29scAkSRt2fbWCIOhy5szJn0pG6RyRQn27PfVtM7O308f+KdWMeFFG9PTMv9UKwAuCoDfRl7XmSueIQn27ffVtSa1JSeJl4Gozq6m+nRU9/cKk+VrhQRAE3UbpHFEi1LfrYGazzWwsHja5fr3WYhAE5aIvLwNR1mCFUN9ugJm9Iel6YBIwX4sxCIKSUcKxn7yUtUUU6ts1kDRK0vC0PRhvNf4jx3mDIAh6jNK1iLLq25JagVslbWFm12XzmdnfJI3FY90/AC7Ho872Ak5ODuppXDW7GSrq29PwbrOTzOw9SRX17X7ATPKpb/8SV9NuAZ4Btm1gU1HfvqfOONGSwOnpvrQA55nZZbmvLAiCXot1fH5Qr6V0jsjMzsADDjCz2UBdTTkzm0KajJXZdx8wX0izmU3IbN+Aj9vMcyypb88ysy/UsL8Wl7io3j86s30XMCFtvwvsXyP/dDzwofJ528z2d/HJYjUxswdq1SEIgvJjsztP4qe3UTpHFHQ+w1Zv3neNXXhUsbIGD2raZpFV1i5U1n8XHlnIjgKqzAPef6tYWQX43mY15xF2CS0DBhYzLPj2XkRJe5tTGirI1OTSb+/VOFMVQ1dYvVBZnUIJw7LzUnpHFOrb8xDq20HQRynj/KC8lN4Rhfp2EAQLBDFGFARBEPQkMUYUBEEQ9Ch9uWuurPOIFkgkNRI9reRrTfp1EbodBH0Fs/ypZIQjKhe5HBFwEPBoV1YkCIKgsyidIwr17frq2ynfMsCngVPr5Un55qpvTzv7vKa+gyAIuh+b9WHuVDZK54hCfbt99W1creFQXB+vLln17S99fpf2sgZB0Aswm5M7lY3SOaJEqG/XQNK2wMtmdnee/EEQBL2Bsjqiivr2UPwB3ln0hPr22JQ+amaPVuWvtmnExsD2kp4FzgG2kHRmTtsgCHozcyx/6gCSRkq6Og0NXF0ZvqjKs3kaJqik9yTtmI5Nl/RM5ljDOY9ldUShvl0DMzvczJZJ+na74V1/8+niBUFQPmzO7NypgxyG98qsiCu4HDZfXcyur7xEA1sA7wBXZbJ8J/OSfV+jAkvniLLq27ig6XhJW1TnM7O/4V1td6Xur8rYzF74eMwD+FjMcU1WoaK+/SgwgqS+jat4ny/pQbwFlUd9uz+upv1w+tyIivp23WCFIAj6Jt3oiHbAhxdIf3dskH9n4IrKMEgR1JcnSXU2SX37MjPrcb25zuS9l15o+kfw+qBhhcoa0tJ9v7fXPyw2aJsaqU0xqH/dhmqns8h7b3ZbWah731WLRHx9uNAihcra7menN85UxV/3mVSoLICFl1uh+R9WhgeO/kruf561jj2pcFmS3jCzyrpmAl6vfK6T/zrg55UlZyRNBzbChxiuBQ5LY+11CWWFIAiCEtCMxI+kycDkzK6pZjY1c/waYIkapkfOU6aZSarrACUtiUf3XpnZfTjwb2AA3ovzXRr0PJXeEYX69jyE+nYQ9FWaWCo8OZ2p7RyfWO+YpJckLWlmLyZH83I7Re0CXGxmc5uyZvZi2nxf0mm0DYvUpfSOKNS3gyBYEOjGYZQZ+Fj6lPS3vQWfPo+3gOaScWLCx5fmm+dZTemCFYIgCBZIuk9rbgqwlaQn8PmYFVWacZLmKrakMfNlmT9C+KwUtPUgsBguPtAupW8RLUhIOsLMftQgz7PAW/j8o1lJKSIIgpLTCdFw+crxHpcta+y/C9gv8/lZYOka+eaLYm5EOKJycQTQriNKbG5mr3R1ZYIg6D6siTGislG6rjmF6Gm7oqdN3Me5oqd//FOILwRBryeWgeg9hOhpQ9FTA66SdHcK4aydKSN6uu8XQ3whCIKeo3SOKBGip/X5RHJ62+AKEJs2MgiCoPfTjcoK3U5Zx4gqoqf98Qf4/zrpvD0hevpYNqOkDSgueoqZvZD+vizpYmB94Ka89kEQ9E5ijKj3EaKnNUjjTkMr23irq2EMfxAEJWDO7PypZJTOEYXoabuip4sDN0u6H7gT+Gu6D0EQBL2WED1tghA9bSNET+clRE87hxA9rc/MA3fK/c8z/vcXdais7qasY0RBJ1LkH3lIrqGxzuGWZ/5VyG6FxUcVsmtpaf5/eLFudA7vvvpSt5VVlDkftCu2XJcPXvtP0zZDV1i9UFlFnMqnTyvewXDDMV8rbAuUcgnwvJTeEYXo6TyE6GkQ9FX6cO9V6R1RiJ4GQbAg0MwyEGWj9I4oCIJggSBaREFPIOkY4G0z+2lP1yUIgp6lL88jCkcUBEFQBvpwsELp5hH1ZSTtmcRY75f0p6pjX5Y0Mx27MDNR93OSHkr7b0r7Vk9iqvel863YE9cTBEGQh3BEvQRJqwPfA7ZIIq0HVWW5yMzGp2OPAvum/UcBn0z7t0/7DgB+ZWZjcT2+52uUN1d9+7TTTuuCKwqCoDMJrbmgO9gCOL+yjlASac0eXyMtRzEc19m7Mu2/BZgu6TzaxFVvA46UtAzuwJ6oLiy7pv1bb73Vd0dBg6CPYLOjay7oeaYDX0vLTByLi71iZgfgLallgbslLZrkj7YH3gUuryWBFARBuejLLaJwRL2H64DPVdY9kjSy6vhQ4MUkejp3PSJJy5vZHWZ2FPAfYFlJHwOeNrNf4+sqrdUtVxAEQVCAcES9BDN7GDgeuDGJlv68Ksv3gTvwrrh/ZPafmFZ+fQi4Fbgf2AV4KIm9roEvBhgEQZmxOflTB0gBUA9LmiNpXDv5Jkl6TNKTkg7L7B8j6Y60/1xJAxqVGWNEvQgzOx1fVK/WsZPwFV6r9+9UI/uUlIIg6CN0o0D1Q8BO+HI7NZHUCvwOX6D0eWCmpBlm9gi+NM8vzOwcSSfjgVXzPbuyRIsoCIKgDMyx/KkDmNmj1Qt21mB94Ekze9rMPgDOAXZI66ttAVyQ8p0O7Jin0EiR6iZgcnfYdLddXy2rDHWM+9H1CZgM3JVJRe7BDcC4Osd2Bk7NfP4i8FtgseSgKvuXBR5qVFa0iIJGTO4mm+6266tlFbXrq2UVtevuOnYqZjbVzMZl0tTscUnXpInw1WmHnqhvjBEFQRAsYJjZxA6e4gW8tVNhmbTvVWC4pH5mNiuzv12iRRQEQRA0y0xgxRQhNwDYDZhh3h93Pd51B7AXPoWkXcIRBY2Y2jhLp9h0t11fLauoXV8tq6hdd9ex1yDpM5KeBzYC/irpyrR/KUmXA6TWztdwhZdHgfPMp6AAfBf4lqQngUWBPzYsMw0oBUEQBEGPEC2iIAiCoEcJRxQEQRD0KOGIgiAIgh4lHFEQZFDV2htB8xS9hx2wa/o5liRqmskfv4suJBxR0K3kEUBM+ZTZXryJ88/3m27yQTWmibKU/rZmtnOXJWmCpC2bqFu1fZf//zbzwJY0UFKrNRkBJWlhSf0L2H1E0lAzm5P3XqRw45FmlnutBEmq1E3Sso3yB80TjmgBRNLSktZq9i1P0kqSfi3pcEmTCpS7Dr52UqN82X/8A4FDaiyLURMzlx6WtIWkjSV9JO+DStLKwDRJ/fPkNzOTtB1wKvBTScs181AENgCWSmW3+11kHN1KktZK5TeUWc7YrSVpeUkfzWGzkqTJqYzZOe/dasA04EJJO0papJFNxu4s4ExJn5eUa5K9pCWAG4GTJY3Ic98lrYSHG0/IU0aFzG/xYODUvL/FID/hiBYw0sP2SeAwYMO8zkjSqsC5wBvAO8D3JK3XZPGvAjtL2rq9TJl//H2BvfFlz1+TNDhnXSfjYot74QsDjsnpIN4EBgKzUv5GzmFV3LHeBLwPXJqnLEmjJC0EvAeMh7Zrrkdyep/GJwd+XdJtkpZscD1ZZzkdOBA4qr1WmKTlgWuAY+UrApPjelYC/gxcDlwK7AOs2qhuyQmdDpwPXAgchK9AnIeXgceBD4DfSVq8PcecfvdnAseY2UX18rVjvze+vMoX029xUUkLN3ueoDbhiBYgUjfLJFyc8HH8H2uDHA/chYBvA783X4DvJHxdpHYfhJWHV2ph9DOzf+LrLK2YqU82f7Y7rh+wKXACMFvSQcCfJR1bo5xhme0t8TWYPm5mk4GLgcskfazeA1XSJ9N5RwP/A9bNtsoy+UZJWjNtrwMchy/vfpqZHQGcB1wsaYV6D8X00P4lvhDiisBGkraVNKw9xyJfF+ZHwCfxB/5awLmNWjip9XQEsA3wb1w1eV9J29QxGY+38NYDNpV0PMx1RvN106X7+XngAjM7y8z+iC9VPzkdb++3tStwZrI7D3cq30n3Y5V2rqkV6I8vBPlX/Lf8A0mrSlq/jtmewJLmqxcj6XhJv5G0Z3qhqC6jut6D8cmqG0k6FLgWOFzS0u1cX5CTcEQLEKlf/GLgcOCnwGz8YbBRe10iZvYO/pC9JH3+AF+GfNta+eWryFYeXhviXTZfSW+l9wD7SFos20+fffBLWgOwVN6J+ANgIXyG9iqSRmTsVgK+KGlwajEdhj9sR0hqMbPj8a6fmyWNzjqIjFNaFv9f2BmXsD8J+IukEyRtn7mmnYH/pXv1HDAEGCdpVLreHwKX4bPRB9d6CJvZ4/hD+lvAFcDKwO7AX4CrJC1T5+H9NK5wvHK6xlHAS8DVkpar9T1ULhNvCa2JO4wvAa8Bh6qGwKWZnQNMNbN/AV8GNpH0o3Rsdnopyeafg69bM1UJfD2bhdJxq+XA0rGjzexXklrks/dfSrYTgJ0kDahzD2eb2fu4lMxA4MfpOv8OrAvzj5+Z2ZH4opNXSboMGIQrAqwLbCepX6Wsqt/iTpKGprptBnwDeApvCS+JO8SgozQrDR6p/AloSX+H4g/6X+KD9JsBE6vyqs45dgd+m7bHARum7f7ADsDm+D/5QcB+wNfxf/xtgUeAg/GHh6rOezDe979k+rwKMCRt74C3xIZm8o8GRgKrp7yL4t08RwOLZvIdAixfVdaYGtd1GP72PDFd4yqZY4OBxXEnXinrUuAYYLFMvhVqnHdCOt+nM/sWxbvBhgKtwKjq+44/7JbN7P8+8L20/Tl8Rd6xNeyWS/UdnLmu7dP2t3Bnu3aj7xp3fDel+7cG3jIe3J5dsjk7bW+Urrt/g9/kapntLfCxnIUb2HwRb+0tmX5T1wF/AkZW5WvNbJ+BO9psWVcBC9U4/zfS/f1Y+jwicz8/heutLdPT/899IfV4BSL10Bff5oyGpX/mS/Axkh0a2FUedFumh+Jq6Z91C5KDANYB7sP78VfM2G6Mj9vcDvy1xrl3TccWSZ+Xq/yj487sQWDNbD3S9lDgF7hDXQH4CO4gjs4+3KvK+lp6wJ6AjxtU9n8JH0toqb7mtL0K3r04BVg+lXUxvirlYnXK2g5vCVa0uY7PHDsdmFBdTvq8I+6UZ+AvDIvj4pJnAUfiTnm+9WLwbrg78a7DGcDC6V48ia+8+TjpxSHnb2Uo8AQwC9guR/610gN/C+AxYKscNtl7vA7uHD7SwGYRvLX9b+DQ9PmnZBxzJm/WGQ3ObK+XvpNRVfnHAXdkfovj8a7UAbgDvK/yW4zU8dTjFYjUg19+mzPaDu+f/3T6XLMVVGW7BfBf4AHg03hXzE34wliVN9Rbgc/UsB2Y8n6pav+OeOtiX+Ao/I3zNNyBbUdqnVQ9tAalv0sDP0gP7IqDuAnvhmypKucLeDfO4rjTuRc4KR0bnx6i1Tbr4d1bI/GWzPGZshbHx21WrHGtS+NOflm8a+wufHD+l+n4FGo7k7VSHYfgTufO9BAcgwdw/IkaTgEPErgfd8jfxcdrKvfoaHx55+2b/J2sB7wIbNvo94G3cpcHnk0P60lNljUp2TWsI+5gjwL2zOwb0l7dapR1L7A9bS9Ylb+fwFcZ3Q/4Tfot/yP9BtYBluvp/9++lHq8ApG66Iut80ZfI99AvEX0uYpd9T9lHbux+Fr1W2T2VbrIJqbPm+JjJvumz8uRusfwPvZD0vYS6aGyLj4gf3V6SKyLtz7WqVOHb+Nvsz9O9RmJO4gpwEr4OMqyNa53p3Tsq8l+Hdxp/TrlWabKZhO8dTcdHytbL5V1XHpIrUCdrif8LX1F/A37vnQPdgCeAX5cx2YY7swPxZ3OrbR1D1V3L87t3gQ+mu7D/njX4p0Zu/Epb2v17yPHb+nLeCurhRrdqXV+U7eQWtd5ykrnXQofb2rGblDVOXJdV/pefoe//MxzDzN5jk95Nkuff0EvWYG1r6Uer0CkLvhSfSB2/fR3LD5fpT2nMqCg3Vrpb9Z5TQLmADulzzumh/2v8W6mNdJD4Pdp+zv4m+el+JjHokC/ZLsD/sY6Jn3OtoRWwseCJuHjUJemh/1IvIvuOKqcQ9VDaxE8HL3S9fcnvIttVLYsfFzgS/hYR//0UP4r7owWxZ3eKjXuTaUFtWz6vBXww7S9Dd6FNL6G3VZ419mmeLfaTNKYEz4ucSvupKrf7jcHLgI+DvwT+D/axjM2S9e2RPo8EH/TH5C+5yNy/KZy2WTvW5GySONCBexUwGZgVZ2/ho8z/QTvQs6+zO2CB1LMN/4XqeOpxysQqQu+VO8OOhAfS/i/Wg/KTN5K95zy2lX9g1bsNwc2TdsT8W67z6bPG+JjIdtk7AbhTurq9PkGPCpOeDffZ/BxlTUq9cvYboUPvh+cPo/AI9FmpLJGMH+ff7b1VKnn9fi41JfwrrVqm23xFtA9tL2lD0sPu+vxVka/Gven0oI6LdlvgjvO/8Pfsv9NpiWZsVsLbw1WAj+OwrsO98MH/B8mdY9V2a0L/CpTx11wh7Vnuo/3UTX2h7e03scDSFZq5/dReSlQEzZzf1NNltXaUbtmbPBWeKWsPfGu0I/gLxo3Aj/EXz4+jo9drtHT/9t9NfV4BSJ10RfrD653gT+QBlxr5Kn8Aw/Do9UGNWm3SLLbHh/M3jyTZ0t8AusX0ufKP3zWiX0B2COd42+0tcwWx7uZlqxR9j74m+nleGvhY5lr+AbeyhlUZZNtPX0Dbz2tgUeynYe/BY+tshmPR7RthTuD0zNlDQcOoHaLproFtV+q6wp4oMMe2fuUseuHO5D/khxs2j8ZH/v6HbB1rXuJd18+ke5NS3rIbomHhP8K+FTFLmM7GLg7fUeVqMQBmXJbMt/zCNypjihgM7hgWUXtctUx/SampHs0GG8BLYFHd16Dtz6vJgWykInAjNQFz6uerkCkTvwy5201DAS2xruovk/bQP+w9NCrPMSG42+CGxW02wZ3CBun/evjXWr98VbSu3jff62WwyR8LOEK2t68v4OPxfSrcU1b42+qle6bk/HAgsq40yLA8Koy2ms9jU/7qh3XGDxq6/TMvlNSvVZKn1trXE+9FtSX8RbfJrW+r3RfK9t7pQfgrlV5+9ewy4axH5PKXonaTj/rhIZn9n8LnyNTua4Vqu758HQvti9gs2XBsoraNVPHLfEuzD+SgkzSec6n7bd3GT6HLZxQF6cer0CkTv5C/WF4SvpHXAyPovolHkH1ZfzB/ZGUdzj+9rdJB+wGpnwnpjyXpAdp5cFf3d21D/7WWem2OwPvrtoRb0ncD6xe47p2Ar6Jz6afnNn/e/ytttacoPZaTwfhraeFqmwWx53w15Pt7pljZ+DdloNrlNWoBbU/mRYUbU5he1we5yzauuR2T/dx9+r8mc/b4GNrR2TsfpLOs3p1/qrfx7X4eFLlIX1Yuq+740509bR/BP6AP7SAzSYFyypql6uOeIuoYvMj3IEflY4NwsPNd8UnL18DLN7T/9MLQurxCkTqhC+x7aG2Mj734Tt4t8OleGtkFTwE+HZSODU+DjMDH8guYrd7+uddKD0UDyONe+Bdbn/G3/SzYwyfTf/oB6UHwzfxbpFDcFmZaZkHTOWa+qe/1+OthV3wPvxdMtf/C2CpqnvSdOsp7W9N1/d7vHvmElJEYTo+3zgBxVtQn8LDuVfBgyXepG1O0V74mNYSNey2wJ3kCG5/MQAAGYdJREFUuPSwvBzYOR37Ff5WP99k0JT/WnzM45e4I660ZPfHu7a2qdz3dE37F7DZvGBZRe2K1PHnwDl46/1E3CGNSPf2Bmp010bqwmdYT1cgUid9kT4mcSUuygjeqjkyPVRHp33Z7o5lSFpnBez2w8ckzsUHwjelrctuY7xVsw3zdoXsiD/cK91hK+ATVL+dyTOoxnVVWhXH0jbWsTv+5rt3nXtRpPW0NLBy2v4oPka2Af52fD1VXWUZu6ItqBZ8bGzt9DC8Nn1+h7ZgiqVq2A3G3+DXwFtf9+IvAZcCO6Y8K9ewWwZ/Ocg6y++lh3GlvOoosjUK2CxasKyidkXquEf67X00fd4Aj2I8Gn8Ba6WG0kKkLnx+9XQFInXSF+n/QA8A52X2Vea6XINHCM0TzVTEDh/QvZ627qAD8DGe1fGIo9OommiJt5oOxKO+vkRbUMLyeCTZ96vrlT6vgbegfo47tzvwyLJN8If3GXjLZu5YS/rbbOtp4bT/Onz8aA28BbVZOr4HNQIT0rGmW1BV5Y7C38DXTvtuAt6mRmstYzcEf3v/K2n8grbJv/MFeKTji+HdrreRWk9p/w/wQI4RnWHT3XYFbQ4ghXZnfjPj8dbkIWSCGyJ1T+rxCkQq+MW1PXzXw9/olkwP5btJ81XS8ZFkJkEWtcvsbwXOJr1tpn3HA+em7VEpZecVTU3bB+MDwBtlHgBjSK2eGmW14M5qFTzseg7e3XUVLs2zcFX+pltPGdtBeBj0uXiL8GV8QmjNMQKabEFRZ6JlKvdkXFV7c9whrpfj+x+FhyhvgrcuryPj9DL3/xOpTpvijm9/vNvwM5m8KxS16W67omVV3btt8CjNlTP7dsDDtWvKNEXq2tTjFYjUgS/PB7pn4l0K1+POZSl83OHnnWGX+ccfAQxL2z/D3xwrEyQ3B36Ttkfj/fQVpYbtgJ9kzvddvAWxKTUi6dqp82C8ZbQo7gyr1Q+aaj21U84wvGX3fdyBbZi9D2k7dwuKeYVM53NG+Djasekh+jyZ1mSOuu6DtzLvI00grjq+Ne6sdsed+GdxB/plPKBi586w6W67omVl7BfBW0w/xgMj9iCjQhGp+1OPVyBSwS/OuySuTg/OA0jrA6VjS+J94LXGCpq2wx3X3XhrYW/cKV2AtwJ+lmwqys4j8EiuX+Jvnp8HvlF1viPxAfX5xoTqXKvwh/9MqkKgM3maaj3lLPdIMkrNVccatqBwJ3MRcEq2ntnrSn/745F1tRQkGqlWf5Ske5Y5n2hTjlgDH8S/P/M9DyctDVF1j5uy6W67omXVuW9L4r//y/EW/lo9/T+9IKcer0Ckgl+cj9X8EfgKcDNtcyEm4SHVNfu5m7XDu30uxLuONsbfwPfFZVQ+hUfAfYL5J0seRFo4Dp/lv0Patyveaqo7BtLONR8KLJ0jX7utpxz2levYDQ/kmC/YIJO33RYU7hyvBH6WsanljLLOpzJJc1X8bb3mxOIc1/EdPCLsNtqiBfdt70FdxKa77YqWVedcA+r9r0TqvtTjFYiU84tqe2ANy+z7LT5prxLyPAHvplmlqB0eBbYt3sIYjbeYsuu3rIm3gA6rLiNtH4hHrVVUF67EB+O/ibeefk470isN7kHDrjxytJ7y3m+8WzG3rAt1WlD4ONi1ZLo9q5xRxfEMIQVS4GHE1wL/wsPhR9Y4b8VuMG2tqaVpW47jEOAFYNX0eS1cFX2zqt9GbpvutitaVqRypR6vQKQmvixfbqHSlTAGHxw/Ee9qq0Sl1VoaILcd3nJZkbZ1WA7GFRTWoy3AYG18PGaFqgfqN/A5R2PT50XwiKYpterVxfcqV+upk8qq2YJKD8/l0vZo0vyVjF0rbc5kOD5eNxYP5ngYn1NzFB6KvAfzKilk7W7GFRU+ja/kOg04Lh3/A664cCY+d2v7qro3bdPddkXLilSe1OMViJTzi/KH0vX4BNQ/pH/KjfBuoa/gYzcTUl51xA4f5zkJ2Ct9PhQPMFiXed/el6DNOS2Ed+FV1KYHZM51JD5OU3etmC64X7kDITqpvHlaUPi8qZvxeUu/xYNBRuPRWidV2Q7DWz+bpM8HAH/MHN8Lf+P/Ar5IXVZm6ar03a6Khx9PxJe1OAM4IeUbW9lf9T03bdPddkXLilSu1OMViJTjS/JF1c5h3kHvH+GD8Rt0hl3VA6MVn7T6O2C3tO/b6WG5Hm3ziabiDqiimP0AbS2ryoNkxXS85tyRvpjwOVU34s76W3igR6WF+bF0bLX0eRG8FfmJjP0G6YG7bmbf+em7rLQ2R6TvYzO8O/V54Px0rB8+NjWdjEOr+n6bsuluu6JlRSpnaiEoA7Pwh/zakrYDMLMjgFeAgyUN66idmZmkiZK+jMujnIq/0U+QtIuZ/QzvypM5/8bHfdbF1QTewceAPi3p4+l8X8C7A4eZ2eudfE96FZKU+fghPsn38/gaS58zs/9KGmtmTwOfNLNHks0WePdnq6QDJe2LS/28AWwpaSdJlXWW/oc7NtJ5jzazG83sJTyMfF1J25vZLLwr60fAAElrZutoZrPz2nS3XdGygpLT054w0vyJttbEOLzrYT18AP6r+HyVT2XyrtRRu/R5TeApvBvkz6RgBDzK7XSSdA3zL6H9RTx6aUe8G+VA/GExFe9OWq2n72c3fm8bpvswGl+I7i7aohI/iYd3L53J3482lfJn0727Ex+M3x1vhV6IvxCMxSV9Tsp8z5vgGml74F2tm+Erv85d0pu28aqmbbrbrmhZkcqferwCkep8MT5p72l8bOU5fOGuyoP+TGoskNasXeYffylcDmdS+rw+8zqj3amKHgNWo03Da1t8cucOeJfKargTbCpsuoyJzPLU+FjanzLb5+AvAZWJp5UH6RjaJgcPwp3/funz4vhk2aMz5x+GO7h7aJMCmoRP6jwWX678MjzqcQtcY69W0ErTNt1tV7SsSOVOPV6BSFVfiD94huKaZRWZmg3xbrFd8FDdb1B7gmBuu4wT+jQ+RvEE3rXWHw/dHo8PtFd04NYhTUylLdLuKtqWc9gW16b7InXkbPpSYt5owbXS/d8GOD6zf0c86u1XwFaZ72ki8Dpty2l/Jz10K7pxS+CtqWUz5/r/9s4/2qqyzOOfBxAUSbj8kiYVxgWKhAtXmg7kjyHLUFAmqlVaOc5oTWouhowsxx81DJBLVi2Ymmo0mzJNIAfFVRI140TqOOQUpA41uFAbnDSVjB8jJvKdP573cDbnXi7n7HPuPvee83zWeta9+5z9Pe8+e59z3v2+7/NjEfsHd95IOSP6CPyGo5RK6QN0XXyvZk3RurxthfVta/oBhB3gwrin1YcpjzpmAuvS/wfMSFCLLtPZnIBnT/gXPF1Kv/SDeSo+JWT4tFCp1PZy3GPrMjw1zSXp9d6N38G+oVHnoTca7gRyOd5pH4Z39nfiCUt34+s4s/CF9S49BfE7/y34aGcSPpX5nnReJ+DTcaO6OYalpEX8tP1mfDqwI/NYZRLZmjVF6/K2Fda3LZwVegGlBVozG2dmE9PDv8IdAcal7eeAl8xskKTd9eiSZjAejT5Z0iZJq/FO5XL8ztMkrccDKo+T9EN8auhsPO3My3KHhp8AU83sY5JW4ck+dzTy/PRCduPZIkbgndEsPM/ZtfhIZwp+A/AlPJVMJyStAT6O58R7Gh/JzsDXhG4HvijphazGzE41s/PTtV4IbDezhenp/ngi1EoHlJo1edsq+hiD1sHimvYOzGwWsAxflN2FL9Z+BJ9uA79rXpB+7OvSmdlkSY+b2XjcRftJ4CpJe83sIrwzep+k58xsAl7iYCswFk8PNA/4lqRl6fUuw6enrpf0+4adlF6ImQ2Qe29hZvfhU5RLJL2YHrsReETSD8xsjNy7sLvXm4kHF58qaaeZTQZelbQ540EmMzsDP/cbca+8bWl7ESD82lwr6V4zs1o16VgK0+VtK2hRmj0kCxN4VPwKyoF5N+Ojk1H41MRM4JT0nOXVUb7x2ATclf6fgAe5LqW86D6m4viWANuBy9P2ufiU3tzMPrnyofVFw3PrzcQDTR/A3dhLqXmW4qMZ6KIq6wFe71x85NopjU96fipee6h0nSfha0ql6zGBzolPa9YUrcvbVljrWUzNNRFzOnAPoePw9QYkzcc7k7+R9ISk70l6ND2nWnVm1l+S8Gkk8KmjiWZ2m6TN+N3nkfhUEngm6Sxfxb2/Pmpm75d0P55G/6IUK4Sk7Y0+P72JzDToNDxDxUW4y7vhU5kfNLNheODpCvBYmGpeO53PS/Hr0hUT8c6qNP26BZ/Sm5T0myU9k/5XHZqidXnbClqNZveE7WiU7whL6XJOxAM/P0GK78EX/j/P/nedNenwTqlUM+gkPJ3Pm9L2AOCXwK1p+3gOkgofT2HzCzwmZja+PtSp7HarGu688QDlDNvj8Yzi30jnYgHJSaSez0bmOo+knCppLj6FOjVtz8STyQ7Nq6n4TPW4Lm9bYa1vTT+AdrX0o343XihuPH5XeAde5+cafPqsq6SRVenw+JTr8QDIUbjX2yrco640jfRmvG7Pl2o47hl49u1HSNm728XSOXydcpnpQ9K5XownOD1oZdUa2rog/SD/mJTqB69CuhOfvroHmF2vpmhd3rbCWtuafgDtaHgA6Rp8uuszuMfURDwP2d34ms3MenV4FdSF+AhpMB4UeEfqjIbgI6olpBiXGo5/NN24Frey4SPBJ4EL0/ZZuDdhzfWVunjt0ohhCJ4i6CQ8MHYN8Pb03BXAf1NOSNs/jyZvW0UeY7OvdVhxNoCgEDJeQpPxpJZrJH3ZzEYAO/DO4nrg03i26uPNbCPwbB6dpK2S1pnZa3hg5Y14oTrwIMFZeKG7i9J+JqmqeXhJlWtIbYPc42svcIeZvQcfUX5W0ssNeG2Z2TvwgNatkjYAG8xsNzDfzAbgzii7gAVmtknS+jwagCJ1edsK2oRm94TtZHgMzpP4OsMGysXMOvC8YmvS9jm4g0BHPbpMu1OBm5INwUc000hrHWG5ruUF6VrMT9v71kByvFZplPFWfMH+70lpbjL7zMcDjoen7T/HR8I1a/K2VfQxhrWPNf0A2sVwj6i1pOqp+AL3LZTdUzuAcZn9D6tH10X7U3HvuGUUVDCu1S11/FuBOQ14rVPTTcR7M9frTuCGzD7j6tUUrcvbVlh7WbhvF4CZ9cfXY07D87+BB532Axab2ThJv5P0dCaI8ZU6dJ2uq6R/x6u07qYc7BrUgaS1eELTDQ14ubH4mt6J6bqvx0cQJ5vZgrTPMw3QFK3L21bQTjS7J2xVozzdMpiyq+qleADf7LTdH0/ncmK9uvT4VA7iPkwbFajrzZa5ztlM3Kfj3mSzSVm98SnUk/JqitblbSusva3pB9CKlvkyzsZdph/CAyCn4PV97iVNVTRIV8qIsIpMzaHK1868fstnx+4Lhgdz/hT3ZCyVOzgLXzN5H12sOeXRFK3L21ZY+1rTD6CVLPsFw3OvbcBdVecAt+IuqiPwrNVrcKeBfnl1GU2pfMA/4UlHOx0TZRfaobjLdpdZocMK+6wcAzyOj2LfiLvUbwCOwrNwrwOOrFdTtC5vW2HtbU0/gFYxPLh0PuXcbu8E7sk8/zY8EPRkPNj0TfXoMs9PxNPKfB5fBL6NcuqZAWmf0t9heDDh6c0+X+1u6cZiecVjfwdckf7v5FCSR1O0Lm9bYe1t4azQAMxsEr5msxcvAwDurrrTzM4ys4GSHsJHM8dI2i3p2by6iuZ34vFBv8QTZ16CZ1NYDnzfzA6VtCflQfsuniH7wR45EcEByeaqM7N3Ay8D483spsxu2/FaRwC/yaPJ21bRxxgE+9HsnrCvG54s9D/xwNDs44Pxkgw3AZ/EY4GewtP916MrTbWdgldNnZLRjsQDAw/HPeNKWY0H4R3Tnzb7fLWz4UHEvwDekbb/CF9L+QZwYXru7Ho1RevythUWVrKmH0BfN3xOvJQ4tD/JKyhtH4Z7vH0BuIuMI0FeXXp+Jl4HZx6eXmZG5rmfUVEyAhhOpux0WFM+Jx3A6swNRXa69GY888W59WqK1uVtKywsa5Hipw7StMQYYJqZDZe0LaXK2ZtieQYCT0j6upkNkRc+sySvWSdJZnY8PhV3PnBG2vcrZnaNpBV4B7UNyqnzJW0rPRY0jf/Dr9WwtL03/R0gL98BlFNB1aEpWpe3rSDYR6wR1YGc9XjqnavN7IjUWQyStBePvD8ndS67MpqadJlOaFB6nRm4F9In8EDXLwO3mNl04GOSthR7JoLuSDcfe/BYmolmdmy66fgT4B/M7KjSvqUf6zyaonV52wqCSmJE1BjuA84D5pnZEkm7zOxk4Abg6tS51KVLC8F/ia8XrcTXge5Mo6WtePZtk7Srp95kkI/0I/y6mf0Y+BBwppn9Gh/VzpO0tRGaonV52wqCSixuVOonjVzOw9duzgD+FS+5cJ2ke/LqMiOhYXiM0HI8aemVwGbgeeBp4HI80PXnMQXSO0ij21e7ePyPgXHJNkl6JDvirVWTt62ijzEIuiM6ojqp/LKZ2bvwUtt7JD12oC9jtTozOw14CzBS0oK073vxekTPA/8M/FbS6p58n0H1mNlbgaskXVzFvqXrXLMmb1tFH2MQHIyYmquCtDbzWpr/3u8LVvllk/SDSh2gGnX7YjNwF9gngdFm9iDwoKTvmtkheB2iVZJeii9+c6k4/1uAQWY2FNgp6fWKTmCAPLar9ANv1Wq6aK9HdXmPsaSJz2RQDdERHYS04Poj4DtmtkbSf2Se61e5/mNm/dMXNJcO9n2JTwM+h0+5PWaeqXgOsNfMHpb0HTNbJ+mlkqZnzkBQDemanYkn+3wFd8EfKen3pedh33XeY2YdwMVm9vNaNZKWmtmZaTqsx3V5jzE+k0G1REd0cF5JdjRwt5ktBR6XdH+pM8ncNZY6oaF46p08uuPlHnVDgel4yp/HcJft6/DCYf2AB9Q5y0LQJNKIYSye8PNZ3LNxrJl9HQ9Svg/4VfqxHopnuViYR5O3rSKPsdHnN2htoiM6ODvwGiob8ZxZZwNzzWwO/oXbLo8DGpD5Mv4Qzx+XRzcPvNaNeSnqRWb2nKQ706job/G1pKAXke7+b09GmjodDbyIJ/t8QOVUSyvx8uI/SfK8mqJ1tWqCoDrUC6Jqe7vh5Y434rE7x+Aeaw8C38KdBd6Q9huKp7o/vR5dRdvn4amALmn2eQir6rNSyo7xUeBTFc8NBh4BzqxXU7Qub1thYdVY0w+gtxvl8glX4aORJ4C56bET8Kk0gCPw9Dqn16M7wDFcAPwXnsOrf7PPSVhVn5vpeC2eocAh6bHhwPhGaorW5W0rLKw7C/ftKjGzGXgczxckfS49ts/pwMzGAqMl/bQRui7aHyXphUa/r6BnSIv2YyRt6klN0bq8bQVBd0RHVANmdi2eNftTygT1HcxNNa8uCIKgHYhccxmSV9G+v5nHS+fpUTzb8JDu9NXqohMKgiCIERGwnxv1aEm/NS9I94cu9usHnCDpiXp0QRAEQZm274gynclM4OPABrxkwj8qBe2l/bLrOoPwc7c7r66o9xcEQdDbafupudQJvQVYjCcPPRp4G57ePktp2m4YHsTXkVfXM+8kCIKgb9KWHZGZHWtmF2QeOhL4Ch7vMwFPYb/LzCaaWb9M5oMpeA2hRZJ+U4NuGLAiowuCIAgSbdkR4YGm3zazP0vbvwauAG4Fzpf0VJpyuxo4PHUmHcBtwHGUq1FWq1sJLJD0bwW9vyAIgj5D26X4SaOUH5nZfOCLycFghZndiwfpnWxmO/A0PDdI2pGki/C0PRNq1C0EFivSngRBEHRJWzormNks4EKgP57R+v14ipJ3ARfjyR1XSlqdTUyKJ36sWaeUVTsIgiDoTFt1RCnOZySwBviMPLHoucA3gSslrTSzgfh5eTXTmeTWRaxQEARB97TV1FzqFF4wr6/yYhqt3G9mC4HlZrZH0qqK/evVBUEQBN3Q8s4KmWwJWbfpXcBHKL//9fhoZ1u9uiAIgqA22mJqLk2jfRpYBzyFT6mtBHYDz+OlFv5C0sPZ6bS8uiAIgqB6Wn5qzszOApYAH8arm87Hs13PSZmxjwZWS3oY9it9nEsXBEEQ1EZLdkQZZ4GjgFF4JcljgGnAXOA6MztU0mcboQuCIAjy05JrRKkzOQefVnsU2IpXlrxS0lrgf4DpZnZCI3RBEARBflqyIzKzSXiMz12Snsbf5yHAcDObhud7u6yyuFdeXRAEQZCflpuaM7NjgWXA4cCgNN223cy+B/w1Hg90k6TNjdAFQRAE9dESXnOZtZ0pwAeAHcBUPOvBrZKeT/uNxGfgXiq5Z+fRhWNCEARB42iJEVHqTM4HPgkcCmzBY3zOBvaY2e2S/lfSi1kNQF5dEARB0BhaYo3IzI4ErgH+StJpwEN4Pri1wGzg0pSCpyG6IAiCoHG0REcE/AF/LyPS9tfwGkGn4HWA1nZVwrsOXRAEQdAgWqIjkvQ74G7g7WY2WdJreAaEw4FJwM8aqQuCIAgaR0t0RInlwEDg5pSMdBlwIzAG71QarQuCIAgaQEt4zZUwsyPwLAhTgO8Dg4FbgHeWPOAaqQuCIAjqp6U6oixmNh1YjDsibOxpXRAEQZCPVu6I3ggMlPRMEbogCIIgHy3bEQVBEAR9g1ZyVgiCIAj6INERBUEQBE0lOqIgCIKgqURHFARBEDSV6IiCIAiCpvL/cxIJ2tOFOiQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbhmQ61-FA1-"
      },
      "source": [
        "###TESTING DATA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CZkiilRFA2C"
      },
      "source": [
        "NULL VALUES CHECK UP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwJt4YiIFA2E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ef956d6-bb0d-4f95-cb4b-99e7d50b1ea9"
      },
      "source": [
        "test.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6l6jBfNFA2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "012916e6-8352-4e57-a9a0-9225c7aa8b6a"
      },
      "source": [
        "test.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0UflS-nFA2g"
      },
      "source": [
        "#test.fillna(0, inplace = True)\n",
        "#test.isnull().values.any()\n",
        "#test.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMQf9YHPFA2-"
      },
      "source": [
        "TESTING CORRELATION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKg5FKzHFA2_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "1bbbd514-09b5-48cc-e7c5-f1e3590a37d2"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "cmap = cm.get_cmap('jet', 30)\n",
        "cax = ax1.imshow(test.corr(), interpolation=\"none\", cmap=cmap)\n",
        "ax1.grid(True)\n",
        "plt.title('Glass Quality Attributes Correlation')\n",
        "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCyXgoGqFA3B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "2b8d8e61-f18d-411c-aef0-879339d09aed"
      },
      "source": [
        "corr = test.corr()\n",
        "ax = sns.heatmap(corr, vmin =- 1, vmax = 1, center = 0, \n",
        "                  cmap = sns.diverging_palette(20, 220, n=200),\n",
        "                  square = True)\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right'\n",
        ");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgiZAKP9yUjf"
      },
      "source": [
        "#DATA SPLITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf_EXK5qJ6nk"
      },
      "source": [
        "TRAINING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmcNojwbxT3E"
      },
      "source": [
        "Y = train['class'].values\n",
        "X = train.drop('class', axis=1).values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zdJM5bICdao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb51c3e6-6fe4-44de-ef70-2aea83b864ba"
      },
      "source": [
        "print(\"X_train.shape: \",X_train.shape, \"X_test.shape: \",X_test.shape,\"Y_train.shape: \", Y_train.shape, \"Y_test.shape: \",Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (1086, 15) X_test.shape:  (272, 15) Y_train.shape:  (1086,) Y_test.shape:  (272,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgwJmTVl8z6P"
      },
      "source": [
        "TESTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZOgMAkN80UL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b513bc02-a3ee-46a3-d86c-f48aec18a4ce"
      },
      "source": [
        "XTest = test\n",
        "XTest.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(583, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzn-YoUq772k"
      },
      "source": [
        "#MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJ-h-LSye6I"
      },
      "source": [
        "###ENSEMBLE MODELLING USING CROSS VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5KMllIxxT5o"
      },
      "source": [
        "models_list = []\n",
        "models_list.append(('KNN', KNeighborsClassifier()))\n",
        "models_list.append(('SVM', SVC(C=1.7, kernel='rbf'))) \n",
        "models_list.append(('NB', GaussianNB()))\n",
        "models_list.append(('DT', DecisionTreeClassifier(criterion='entropy')))\n",
        "models_list.append(('BAG', BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)))\n",
        "models_list.append(('ET', ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2)))\n",
        "models_list.append(('RF', RandomForestClassifier(criterion='entropy')))\n",
        "models_list.append(('GB', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)))\n",
        "models_list.append(('GB', XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)))\n",
        "models_list.append(('ADB', AdaBoostClassifier()))\n",
        "\n",
        "EVALUATION = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wS-9xSgxTxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "f063f189-94cd-46a6-d39e-4d34650199bf"
      },
      "source": [
        "num_folds = 10\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "for name, model in models_list:\n",
        "    kfold = KFold(n_splits=num_folds)\n",
        "    start = time.time()\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    end = time.time()\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN: 0.581057 (0.027827) (run time: 0.094559)\n",
            "SVM: 0.645336 (0.045423) (run time: 0.497910)\n",
            "NB: 0.570982 (0.042587) (run time: 0.015431)\n",
            "DT: 0.834191 (0.040427) (run time: 0.126720)\n",
            "BAG: 0.804723 (0.034026) (run time: 0.291694)\n",
            "ET: 0.842482 (0.024167) (run time: 0.165897)\n",
            "RF: 0.850832 (0.028180) (run time: 3.300597)\n",
            "GB: 0.838787 (0.036336) (run time: 1.257054)\n",
            "GB: 0.848046 (0.037192) (run time: 1.048408)\n",
            "ADB: 0.834183 (0.037163) (run time: 1.299385)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNeiWBJyxTvd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "e9c7a061-b63b-4211-fff7-dfd00d810e08"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Performance Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZF_bu1byp_7"
      },
      "source": [
        "PIPELINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUhaIHb2xTth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "67d2798d-5b47-4f5d-c04b-fa9f60359d16"
      },
      "source": [
        "# Standardize the dataset\n",
        "pipelines = []\n",
        "\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
        "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(C=1.7, kernel='rbf'))])))\n",
        "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
        "pipelines.append(('ScaledDT', Pipeline([('Scaler', StandardScaler()),('DT', DecisionTreeClassifier(criterion='entropy'))])))\n",
        "pipelines.append(('ScaledBAG', Pipeline([('Scaler', StandardScaler()),('BAG', BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5))])))\n",
        "pipelines.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2))])))\n",
        "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier(criterion='entropy'))])))\n",
        "pipelines.append(('ScaledGB', Pipeline([('Scaler', StandardScaler()),('GB', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))])))\n",
        "pipelines.append(('ScaledXGB', Pipeline([('Scaler', StandardScaler()),('GB', XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75))])))\n",
        "pipelines.append(('ScaledADB', Pipeline([('Scaler', StandardScaler()),('ADB', AdaBoostClassifier())])))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    kfold = KFold(n_splits=num_folds, random_state=123)\n",
        "    for name, model in pipelines:\n",
        "        start = time.time()\n",
        "        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "        end = time.time()\n",
        "        results.append(cv_results)\n",
        "        names.append(name)\n",
        "        print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ScaledKNN: 0.833299 (0.033522) (run time: 0.112038)\n",
            "ScaledSVC: 0.840664 (0.033138) (run time: 0.313874)\n",
            "ScaledNB: 0.802999 (0.032634) (run time: 0.030741)\n",
            "ScaledDT: 0.825858 (0.048574) (run time: 0.137969)\n",
            "ScaledBAG: 0.765962 (0.056165) (run time: 0.289177)\n",
            "ScaledET: 0.846194 (0.025991) (run time: 0.175164)\n",
            "ScaledRF: 0.839772 (0.030584) (run time: 3.255133)\n",
            "ScaledGB: 0.838787 (0.036336) (run time: 1.247128)\n",
            "ScaledXGB: 0.851750 (0.034525) (run time: 0.971629)\n",
            "ScaledADB: 0.834183 (0.037163) (run time: 1.264963)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzzZxMIOyHAZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "19284459-f1e2-48d1-f199-dbad72e7d0d8"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Performance Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhNKDyAO1MiO"
      },
      "source": [
        "###SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nctkC0t6yHHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "69f4e8f6-2c37-4aae-c34b-80a4442fb36b"
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
        "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "param_grid = dict(C=c_values, kernel=kernel_values)\n",
        "model = SVC()\n",
        "kfold = KFold(n_splits=num_folds)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.845268 using {'C': 2.0, 'kernel': 'rbf'}\n",
            "0.802022 (0.041028) with: {'C': 0.1, 'kernel': 'linear'}\n",
            "0.800212 (0.033079) with: {'C': 0.1, 'kernel': 'poly'}\n",
            "0.816743 (0.028841) with: {'C': 0.1, 'kernel': 'rbf'}\n",
            "0.805641 (0.046796) with: {'C': 0.1, 'kernel': 'sigmoid'}\n",
            "0.813082 (0.036791) with: {'C': 0.3, 'kernel': 'linear'}\n",
            "0.801181 (0.033791) with: {'C': 0.3, 'kernel': 'poly'}\n",
            "0.821339 (0.029322) with: {'C': 0.3, 'kernel': 'rbf'}\n",
            "0.785398 (0.047908) with: {'C': 0.3, 'kernel': 'sigmoid'}\n",
            "0.820421 (0.030779) with: {'C': 0.5, 'kernel': 'linear'}\n",
            "0.804850 (0.033512) with: {'C': 0.5, 'kernel': 'poly'}\n",
            "0.830530 (0.035900) with: {'C': 0.5, 'kernel': 'rbf'}\n",
            "0.771534 (0.041971) with: {'C': 0.5, 'kernel': 'sigmoid'}\n",
            "0.825926 (0.031760) with: {'C': 0.7, 'kernel': 'linear'}\n",
            "0.809446 (0.033375) with: {'C': 0.7, 'kernel': 'poly'}\n",
            "0.831448 (0.036116) with: {'C': 0.7, 'kernel': 'rbf'}\n",
            "0.772511 (0.042738) with: {'C': 0.7, 'kernel': 'sigmoid'}\n",
            "0.826860 (0.034092) with: {'C': 0.9, 'kernel': 'linear'}\n",
            "0.814942 (0.033644) with: {'C': 0.9, 'kernel': 'poly'}\n",
            "0.838812 (0.031936) with: {'C': 0.9, 'kernel': 'rbf'}\n",
            "0.766030 (0.043924) with: {'C': 0.9, 'kernel': 'sigmoid'}\n",
            "0.828695 (0.034956) with: {'C': 1.0, 'kernel': 'linear'}\n",
            "0.817712 (0.033682) with: {'C': 1.0, 'kernel': 'poly'}\n",
            "0.839738 (0.030804) with: {'C': 1.0, 'kernel': 'rbf'}\n",
            "0.766947 (0.045268) with: {'C': 1.0, 'kernel': 'sigmoid'}\n",
            "0.828695 (0.032973) with: {'C': 1.3, 'kernel': 'linear'}\n",
            "0.832441 (0.030044) with: {'C': 1.3, 'kernel': 'poly'}\n",
            "0.838829 (0.030775) with: {'C': 1.3, 'kernel': 'rbf'}\n",
            "0.761383 (0.052344) with: {'C': 1.3, 'kernel': 'sigmoid'}\n",
            "0.831448 (0.032990) with: {'C': 1.5, 'kernel': 'linear'}\n",
            "0.836120 (0.024823) with: {'C': 1.5, 'kernel': 'poly'}\n",
            "0.839747 (0.031600) with: {'C': 1.5, 'kernel': 'rbf'}\n",
            "0.744835 (0.053116) with: {'C': 1.5, 'kernel': 'sigmoid'}\n",
            "0.829604 (0.032540) with: {'C': 1.7, 'kernel': 'linear'}\n",
            "0.832441 (0.026120) with: {'C': 1.7, 'kernel': 'poly'}\n",
            "0.840664 (0.033138) with: {'C': 1.7, 'kernel': 'rbf'}\n",
            "0.749431 (0.051701) with: {'C': 1.7, 'kernel': 'sigmoid'}\n",
            "0.830522 (0.032546) with: {'C': 2.0, 'kernel': 'linear'}\n",
            "0.832441 (0.026084) with: {'C': 2.0, 'kernel': 'poly'}\n",
            "0.845268 (0.033561) with: {'C': 2.0, 'kernel': 'rbf'}\n",
            "0.751291 (0.048601) with: {'C': 2.0, 'kernel': 'sigmoid'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebA8q_w4yHM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4fa4bc5-588c-4242-e3f5-09920a76b62d"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "SVM = SVC(C=1.7, kernel='rbf')\n",
        "start = time.time()\n",
        "SVM.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.035850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijCBx7I_yHFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a097442b-95c6-4350-fa0f-8a31ce30148d"
      },
      "source": [
        "# estimate accuracy on validation dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "SVMPred = SVM.predict(X_test_scaled)\n",
        "\n",
        "print(confusion_matrix(Y_test, SVMPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[160  22]\n",
            " [ 18  72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7-k4ARyd1N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "28a66a37-b78a-4fc2-b695-cbfb43c787f8"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, SVMPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, SVMPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.88      0.89       182\n",
            "           2       0.77      0.80      0.78        90\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.83      0.84      0.84       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZAKR83_N9e1"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "SVMPred = SVM.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['SVM'] = list(SVMPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THL-enXHzZMF"
      },
      "source": [
        "###LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnSM8M9FzZMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8da8084-ee05-483b-91c4-2338f1b028c0"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "LSVC = LinearSVC()\n",
        "start = time.time()\n",
        "LSVC.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.053529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDhUCfOzZMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cc1ecad7-3193-4620-948f-4831395b23d7"
      },
      "source": [
        "# estimate accuracy on test dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "LSVCPred = LSVC.predict(X_test_scaled)\n",
        "LSVC.fit(X_test_scaled, Y_test)\n",
        "print(confusion_matrix(Y_test, LSVCPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[142  40]\n",
            " [ 15  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dl-8V6zZMw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ae12060c-bcd3-4c54-d68b-60960da24126"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, LSVCPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, LSVCPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 79.78%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.78      0.84       182\n",
            "           2       0.65      0.83      0.73        90\n",
            "\n",
            "    accuracy                           0.80       272\n",
            "   macro avg       0.78      0.81      0.78       272\n",
            "weighted avg       0.82      0.80      0.80       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_-BN_1RNpYu"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "LSVCPred = LSVC.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['LinearSVC'] = list(LSVCPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0KZ5Tra1lFL"
      },
      "source": [
        "###CALIBERATED CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-QFdLyi1lFR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94b4ae7e-d95d-4d08-8b88-179c2a34e4e6"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "CCCV = CalibratedClassifierCV(base_estimator = LSVC)\n",
        "start = time.time()\n",
        "CCCV.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.208222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEE_IaTe1lFb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7a33a3a5-f87e-4ce9-a59a-4d23952ca5ab"
      },
      "source": [
        "# estimate accuracy on test dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "CCCVPred = CCCV.predict(X_test_scaled)\n",
        "CCCV.fit(X_test_scaled, Y_test)\n",
        "print(confusion_matrix(Y_test, CCCVPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[145  37]\n",
            " [ 15  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rr2TLdp1lFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "53f7c768-9c35-442e-a3b1-f51aec9f20d9"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, CCCVPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, CCCVPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 80.88%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.80      0.85       182\n",
            "           2       0.67      0.83      0.74        90\n",
            "\n",
            "    accuracy                           0.81       272\n",
            "   macro avg       0.79      0.82      0.80       272\n",
            "weighted avg       0.83      0.81      0.81       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpbbf8LSNDvb"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "CCCVPred = CCCV.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['CaliberatedVC'] = list(CCCVPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni0WiBNW1VdB"
      },
      "source": [
        "###NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_DOUaaZ1ZJ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbab73a6-b7d6-470e-f258-48dedd8cea92"
      },
      "source": [
        "NB = GaussianNB()\n",
        "NB.fit(X_train,Y_train)\n",
        "NBPred = NB.predict(X_test)\n",
        "NB.score(X_test, Y_test)\n",
        "NB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNO56LS1Y93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6ee79af0-70bc-4326-d61a-bc2791ff4849"
      },
      "source": [
        "cm = confusion_matrix(Y_test, NBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 70 112]\n",
            " [ 17  73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1PpE1bYQtLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "b60b152d-e9bb-4f7e-d3ce-b75631aefa18"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, NBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, NBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 52.57%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.80      0.38      0.52       182\n",
            "           2       0.39      0.81      0.53        90\n",
            "\n",
            "    accuracy                           0.53       272\n",
            "   macro avg       0.60      0.60      0.53       272\n",
            "weighted avg       0.67      0.53      0.52       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jachJLptJ5lo"
      },
      "source": [
        "NBPred = NB.predict(XTest)\n",
        "\n",
        "EVALUATION['NB'] = list(NBPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7qiblRq15S8"
      },
      "source": [
        "###DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_9z888W2DQh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e634b8be-ce8b-45de-9139-0606f6b9c9ea"
      },
      "source": [
        "DT = DecisionTreeClassifier(criterion='entropy')\n",
        "DT.fit(X_train,Y_train)\n",
        "DTPred = DT.predict(X_test)\n",
        "DT.score(X_test, Y_test)\n",
        "DT.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F96rdu8v2LNT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4749caf4-b1ae-49b8-a592-f3d67030e229"
      },
      "source": [
        "cm = confusion_matrix(Y_test, DTPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[158  24]\n",
            " [ 17  73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjgcubEkQqoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "acc37749-d63d-4211-e323-81143755617a"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, DTPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, DTPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.93%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.87      0.89       182\n",
            "           2       0.75      0.81      0.78        90\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.83      0.84      0.83       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yu4CTTqJ_c7"
      },
      "source": [
        "DTPred = DT.predict(XTest)\n",
        "\n",
        "EVALUATION['DT'] = list(DTPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcVKYY7V715"
      },
      "source": [
        "###DECISON TREE BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvTIZqkeV6SU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7f2d64e-eb30-4642-9b58-5780a35c8b03"
      },
      "source": [
        "Bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
        "Bagging.fit(X_train, Y_train)\n",
        "BPred = Bagging.predict(X_test)\n",
        "Bagging.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8272058823529411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHA989gFc0cY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c2fc1bdb-1d38-4245-e0f9-bf459a95ea08"
      },
      "source": [
        "cm = confusion_matrix(Y_test, BPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[166  16]\n",
            " [ 31  59]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJoehLjQmXF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "df5df42a-b203-48f7-d845-0fbc138f631c"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, BPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, BPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.72%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.84      0.91      0.88       182\n",
            "           2       0.79      0.66      0.72        90\n",
            "\n",
            "    accuracy                           0.83       272\n",
            "   macro avg       0.81      0.78      0.80       272\n",
            "weighted avg       0.82      0.83      0.82       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZA9w42rKDO6"
      },
      "source": [
        "BPred = Bagging.predict(XTest)\n",
        "\n",
        "EVALUATION['Bagging'] = list(BPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltwo4bpdfUJB"
      },
      "source": [
        "###EXTRA TREE CLASSFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOxJXgyfYKV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "070b518c-479a-4fc8-83c8-7abd94aa8e4c"
      },
      "source": [
        "ET = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
        "ET.fit(X_train, Y_train)\n",
        "ET.score(X_train, Y_train)\n",
        "ETPred = ET.predict(X_test)\n",
        "ET.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8455882352941176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65zKOlAHfX-y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "06c79cc5-db08-46e9-b27d-2ca412690ad8"
      },
      "source": [
        "cm = confusion_matrix(Y_test, ETPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[161  21]\n",
            " [ 21  69]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFlomRnPQiGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "779ba4b6-3359-4ca0-a5ff-5fe24356c7c7"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, ETPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, ETPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.56%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.88      0.88       182\n",
            "           2       0.77      0.77      0.77        90\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.83      0.83      0.83       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLRS_VI8KFrH"
      },
      "source": [
        "ETPred = ET.predict(XTest)\n",
        "\n",
        "EVALUATION['ExtraTree'] = list(ETPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzr01v1ThHp3"
      },
      "source": [
        "###GRADIANT DESCENT CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBjwDnwdhZFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "23896953-7e26-4c93-ab19-94c72e6b40bf"
      },
      "source": [
        "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "GB.fit(X_train, Y_train)\n",
        "GBPred = GB.predict(X_test)\n",
        "#GB.score(X_test, Y_test)\n",
        "GB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=0, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7X4yjnWhY_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be0cb979-b7f2-41c1-8460-f65aa4984313"
      },
      "source": [
        "cm = confusion_matrix(Y_test, GBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[164  18]\n",
            " [ 17  73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97yLVNwHQLMN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "757e1895-3e47-4555-d7f9-001ad589a20d"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, GBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, GBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.13%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.90      0.90       182\n",
            "           2       0.80      0.81      0.81        90\n",
            "\n",
            "    accuracy                           0.87       272\n",
            "   macro avg       0.85      0.86      0.86       272\n",
            "weighted avg       0.87      0.87      0.87       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1rjByoJzMa"
      },
      "source": [
        "GBPred = GB.predict(XTest)\n",
        "\n",
        "EVALUATION['GB'] = list(GBPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZyd_Hfz2oLo"
      },
      "source": [
        "###RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiR0t-PM2NMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c52bf700-df6c-4501-fc17-3181319f1014"
      },
      "source": [
        "RF = RandomForestClassifier(criterion='entropy', random_state=0)\n",
        "RF.fit(X_train,Y_train)\n",
        "RFPred = RF.predict(X_test)\n",
        "RF.score(X_test, Y_test)\n",
        "RF.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfNHsez_2THS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d2d8958e-cccb-41a1-b591-17dee8d21bf0"
      },
      "source": [
        "cm = confusion_matrix(Y_test, RFPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[161  21]\n",
            " [ 13  77]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fn_dacEQTpE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d4edacec-4d6c-4f7b-f4ee-39ccbf5cef19"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, RFPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, RFPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.50%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.88      0.90       182\n",
            "           2       0.79      0.86      0.82        90\n",
            "\n",
            "    accuracy                           0.88       272\n",
            "   macro avg       0.86      0.87      0.86       272\n",
            "weighted avg       0.88      0.88      0.88       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWx56HeKMKN"
      },
      "source": [
        "RFPred = RF.predict(XTest)\n",
        "\n",
        "EVALUATION['RF'] = list(RFPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNRXrNXEH6wv"
      },
      "source": [
        "###XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4V9tFc9H5fv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "c2461ff1-58a4-4f36-f009-7780ead7eb4d"
      },
      "source": [
        "XGB = XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)\n",
        "XGB.fit(X_train, Y_train, verbose=False)\n",
        "XGBPred = XGB.predict(X_test)\n",
        "XGB.fit(X_test, Y_test, verbose=False)\n",
        "#XGB.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.08, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=0.75, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itRViZGOWts9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f0e19a6e-f78b-4732-8be6-d1581dd640f0"
      },
      "source": [
        "cm = confusion_matrix(Y_test, XGBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[161  21]\n",
            " [ 16  74]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsnTJsQ5O5js",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "e2d709b1-dd0f-40ff-fecf-e747a8920220"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, XGBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, XGBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.40%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.88      0.90       182\n",
            "           2       0.78      0.82      0.80        90\n",
            "\n",
            "    accuracy                           0.86       272\n",
            "   macro avg       0.84      0.85      0.85       272\n",
            "weighted avg       0.87      0.86      0.86       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFVUvOyaIjkr"
      },
      "source": [
        "#XGBPred = XGB.predict(XTest)\n",
        "#EVALUATION['XGB'] = list(XGBPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2NV_zuNP3vE"
      },
      "source": [
        "###ADABOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awv3ur1UP6kr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6e0347b-a46e-4f33-85df-3dd774555343"
      },
      "source": [
        "ADAB = AdaBoostClassifier()\n",
        "ADAB.fit(X_train, Y_train)\n",
        "ADABPred = ADAB.predict(X_test)\n",
        "ADAB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
              "                   n_estimators=50, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzJNvUimbL_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3756eb5a-831d-42f9-9c2f-f041dcad5cba"
      },
      "source": [
        "cm = confusion_matrix(Y_test, ADABPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[164  18]\n",
            " [ 16  74]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-FsxAUWP6gK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "04ebf409-df11-4cc5-d45a-1c8d30a2a68f"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, ADABPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, ADABPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.50%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.90      0.91       182\n",
            "           2       0.80      0.82      0.81        90\n",
            "\n",
            "    accuracy                           0.88       272\n",
            "   macro avg       0.86      0.86      0.86       272\n",
            "weighted avg       0.88      0.88      0.88       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve606tulIS_O"
      },
      "source": [
        "ADABPred = ADAB.predict(XTest)\n",
        "\n",
        "EVALUATION['ADB'] = list(ADABPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWm6WYhm4Fgj"
      },
      "source": [
        "###ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoYzMitY7kdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "f48a9f4d-df44-4962-d08e-b674f8b12cf8"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units = 32, activation = 'relu', input_shape=(15,)))\n",
        "model.add(Dense(units = 256, activation = 'relu'))\n",
        "#model.add(Dense(units = 1024, activation = 'relu'))\n",
        "\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               8448      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 9,217\n",
            "Trainable params: 9,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsKy91V7wKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e0edc43-a3ba-4848-8958-bc1a642edd8d"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size = 10, nb_epoch = 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1086/1086 [==============================] - 0s 448us/step - loss: -2705082.6246 - accuracy: 0.6455\n",
            "Epoch 2/1000\n",
            "1086/1086 [==============================] - 0s 154us/step - loss: -27675694.6512 - accuracy: 0.6492\n",
            "Epoch 3/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -119901821.7965 - accuracy: 0.6492\n",
            "Epoch 4/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -331097594.8425 - accuracy: 0.6492\n",
            "Epoch 5/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -720520122.5935 - accuracy: 0.6492\n",
            "Epoch 6/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1338730084.6409 - accuracy: 0.6492\n",
            "Epoch 7/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -2208006608.6041 - accuracy: 0.6492\n",
            "Epoch 8/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -3344715347.7716 - accuracy: 0.6492\n",
            "Epoch 9/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -4781666061.5285 - accuracy: 0.6492\n",
            "Epoch 10/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -6564547880.8766 - accuracy: 0.6492\n",
            "Epoch 11/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -8708710112.1473 - accuracy: 0.6492\n",
            "Epoch 12/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -11252272346.4015 - accuracy: 0.6492\n",
            "Epoch 13/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -14218376422.7182 - accuracy: 0.6492\n",
            "Epoch 14/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -17618022101.8932 - accuracy: 0.6492\n",
            "Epoch 15/1000\n",
            "1086/1086 [==============================] - 0s 149us/step - loss: -21445845740.4346 - accuracy: 0.6492\n",
            "Epoch 16/1000\n",
            "1086/1086 [==============================] - 0s 159us/step - loss: -25697546841.3996 - accuracy: 0.6492\n",
            "Epoch 17/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -30313011308.6114 - accuracy: 0.6492\n",
            "Epoch 18/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -35392742777.3996 - accuracy: 0.6492\n",
            "Epoch 19/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -40924410628.7146 - accuracy: 0.6492\n",
            "Epoch 20/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -47024323356.7588 - accuracy: 0.6492\n",
            "Epoch 21/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -53612718598.6004 - accuracy: 0.6492\n",
            "Epoch 22/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -60706361861.1860 - accuracy: 0.6492\n",
            "Epoch 23/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -68330271520.0589 - accuracy: 0.6492\n",
            "Epoch 24/1000\n",
            "1086/1086 [==============================] - 0s 142us/step - loss: -76394015026.9171 - accuracy: 0.6492\n",
            "Epoch 25/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -85033932948.1547 - accuracy: 0.6492\n",
            "Epoch 26/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -94269141746.3278 - accuracy: 0.6492\n",
            "Epoch 27/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -104011867656.9576 - accuracy: 0.6492\n",
            "Epoch 28/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -114307948832.5304 - accuracy: 0.6492\n",
            "Epoch 29/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -125122194242.0037 - accuracy: 0.6492\n",
            "Epoch 30/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -136547923072.2357 - accuracy: 0.6492\n",
            "Epoch 31/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -148635718696.0737 - accuracy: 0.6492\n",
            "Epoch 32/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -161323922067.0939 - accuracy: 0.6492\n",
            "Epoch 33/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -174541332802.4162 - accuracy: 0.6492\n",
            "Epoch 34/1000\n",
            "1086/1086 [==============================] - 0s 149us/step - loss: -188287901987.5948 - accuracy: 0.6492\n",
            "Epoch 35/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -202503186783.1160 - accuracy: 0.6492\n",
            "Epoch 36/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -217144571626.7845 - accuracy: 0.6492\n",
            "Epoch 37/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -232335957225.8416 - accuracy: 0.6492\n",
            "Epoch 38/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -248234405243.0497 - accuracy: 0.6492\n",
            "Epoch 39/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -264892319260.7588 - accuracy: 0.6492\n",
            "Epoch 40/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -282164093716.2725 - accuracy: 0.6492\n",
            "Epoch 41/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -300059675174.6593 - accuracy: 0.6492\n",
            "Epoch 42/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -318670466817.4144 - accuracy: 0.6492\n",
            "Epoch 43/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -337784767019.1381 - accuracy: 0.6492\n",
            "Epoch 44/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -357641149931.2560 - accuracy: 0.6492\n",
            "Epoch 45/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -378061115618.2983 - accuracy: 0.6492\n",
            "Epoch 46/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -399051388492.3757 - accuracy: 0.6492\n",
            "Epoch 47/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -420978459359.4696 - accuracy: 0.6492\n",
            "Epoch 48/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -443618728064.2358 - accuracy: 0.6492\n",
            "Epoch 49/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -466748633250.6520 - accuracy: 0.6492\n",
            "Epoch 50/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -490652819978.3720 - accuracy: 0.6492\n",
            "Epoch 51/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -515253731222.3941 - accuracy: 0.6492\n",
            "Epoch 52/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -540534452131.5949 - accuracy: 0.6492\n",
            "Epoch 53/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -566621352109.4954 - accuracy: 0.6492\n",
            "Epoch 54/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -593501075339.0792 - accuracy: 0.6492\n",
            "Epoch 55/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -620837950286.7329 - accuracy: 0.6492\n",
            "Epoch 56/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -649124920804.6555 - accuracy: 0.6492\n",
            "Epoch 57/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -678153356833.0018 - accuracy: 0.6492\n",
            "Epoch 58/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -707963804164.7145 - accuracy: 0.6492\n",
            "Epoch 59/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -738289563257.6353 - accuracy: 0.6492\n",
            "Epoch 60/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -769670335169.2965 - accuracy: 0.6492\n",
            "Epoch 61/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -801775757636.3610 - accuracy: 0.6492\n",
            "Epoch 62/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -834557088373.8636 - accuracy: 0.6492\n",
            "Epoch 63/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -868198819981.4365 - accuracy: 0.6492\n",
            "Epoch 64/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -902691707210.0184 - accuracy: 0.6492\n",
            "Epoch 65/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -937763022133.2744 - accuracy: 0.6492\n",
            "Epoch 66/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -973620182947.5948 - accuracy: 0.6492\n",
            "Epoch 67/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1010257643317.2744 - accuracy: 0.6492\n",
            "Epoch 68/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1047550487421.8784 - accuracy: 0.6492\n",
            "Epoch 69/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1085425057874.9760 - accuracy: 0.6492\n",
            "Epoch 70/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1123854020485.8931 - accuracy: 0.6492\n",
            "Epoch 71/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -1163228236288.9431 - accuracy: 0.6492\n",
            "Epoch 72/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1203145117943.0422 - accuracy: 0.6492\n",
            "Epoch 73/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1243823988362.6079 - accuracy: 0.6492\n",
            "Epoch 74/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1285229390631.1306 - accuracy: 0.6492\n",
            "Epoch 75/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -1327573156024.8103 - accuracy: 0.6492\n",
            "Epoch 76/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1370723053228.5525 - accuracy: 0.6492\n",
            "Epoch 77/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1414763299909.7754 - accuracy: 0.6492\n",
            "Epoch 78/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1459924806284.4937 - accuracy: 0.6492\n",
            "Epoch 79/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1506082557059.0645 - accuracy: 0.6492\n",
            "Epoch 80/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1552839081290.0183 - accuracy: 0.6492\n",
            "Epoch 81/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1600884916271.1453 - accuracy: 0.6492\n",
            "Epoch 82/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1649555459202.1216 - accuracy: 0.6492\n",
            "Epoch 83/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1699207638217.7827 - accuracy: 0.6492\n",
            "Epoch 84/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1750301478483.9189 - accuracy: 0.6492\n",
            "Epoch 85/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1802405785999.7937 - accuracy: 0.6492\n",
            "Epoch 86/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1855579798382.7920 - accuracy: 0.6492\n",
            "Epoch 87/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1909404565941.5100 - accuracy: 0.6492\n",
            "Epoch 88/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -1964215640426.0774 - accuracy: 0.6492\n",
            "Epoch 89/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -2019598055752.1326 - accuracy: 0.6492\n",
            "Epoch 90/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -2076067104247.5137 - accuracy: 0.6492\n",
            "Epoch 91/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -2133611345116.6411 - accuracy: 0.6492\n",
            "Epoch 92/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -2191873723629.6130 - accuracy: 0.6492\n",
            "Epoch 93/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -2251262138198.2759 - accuracy: 0.6492\n",
            "Epoch 94/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -2311341224350.8804 - accuracy: 0.6492\n",
            "Epoch 95/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -2372269184567.6318 - accuracy: 0.6492\n",
            "Epoch 96/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -2434993022104.7515 - accuracy: 0.6492\n",
            "Epoch 97/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -2498359493980.8770 - accuracy: 0.6492\n",
            "Epoch 98/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -2562720206191.7349 - accuracy: 0.6492\n",
            "Epoch 99/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -2628088327226.4604 - accuracy: 0.6492\n",
            "Epoch 100/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -2694327961166.2612 - accuracy: 0.6492\n",
            "Epoch 101/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -2761997908748.7295 - accuracy: 0.6492\n",
            "Epoch 102/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -2830289134261.9814 - accuracy: 0.6492\n",
            "Epoch 103/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -2900149227844.3608 - accuracy: 0.6492\n",
            "Epoch 104/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -2970537755574.4526 - accuracy: 0.6492\n",
            "Epoch 105/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3042095593015.6318 - accuracy: 0.6492\n",
            "Epoch 106/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -3114497824628.4492 - accuracy: 0.6492\n",
            "Epoch 107/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3187888115840.2358 - accuracy: 0.6492\n",
            "Epoch 108/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -3263009413252.0073 - accuracy: 0.6492\n",
            "Epoch 109/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3338692295723.3735 - accuracy: 0.6492\n",
            "Epoch 110/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3415211059711.0571 - accuracy: 0.6492\n",
            "Epoch 111/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3493034353522.5635 - accuracy: 0.6492\n",
            "Epoch 112/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -3571318254522.2246 - accuracy: 0.6492\n",
            "Epoch 113/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -3651989789226.4307 - accuracy: 0.6492\n",
            "Epoch 114/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -3733164259354.4014 - accuracy: 0.6492\n",
            "Epoch 115/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -3815321634065.4438 - accuracy: 0.6492\n",
            "Epoch 116/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -3898465811589.8936 - accuracy: 0.6492\n",
            "Epoch 117/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -3982096411495.2485 - accuracy: 0.6492\n",
            "Epoch 118/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -4068274727624.8398 - accuracy: 0.6492\n",
            "Epoch 119/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -4154860121682.0332 - accuracy: 0.6492\n",
            "Epoch 120/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -4242097749872.6777 - accuracy: 0.6492\n",
            "Epoch 121/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -4331376007550.8213 - accuracy: 0.6492\n",
            "Epoch 122/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -4420883465806.2617 - accuracy: 0.6492\n",
            "Epoch 123/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -4512550345077.3916 - accuracy: 0.6492\n",
            "Epoch 124/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -4604606379175.8379 - accuracy: 0.6492\n",
            "Epoch 125/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -4698420684592.5596 - accuracy: 0.6492\n",
            "Epoch 126/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -4793329285116.2275 - accuracy: 0.6492\n",
            "Epoch 127/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -4890023785345.6504 - accuracy: 0.6492\n",
            "Epoch 128/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -4987117129343.2930 - accuracy: 0.6492\n",
            "Epoch 129/1000\n",
            "1086/1086 [==============================] - 0s 152us/step - loss: -5085042823205.7158 - accuracy: 0.6492\n",
            "Epoch 130/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -5184546223913.0166 - accuracy: 0.6492\n",
            "Epoch 131/1000\n",
            "1086/1086 [==============================] - 0s 146us/step - loss: -5285241999759.7939 - accuracy: 0.6492\n",
            "Epoch 132/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -5387891459767.8672 - accuracy: 0.6492\n",
            "Epoch 133/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -5490852842290.4463 - accuracy: 0.6492\n",
            "Epoch 134/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -5596185780284.3457 - accuracy: 0.6492\n",
            "Epoch 135/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -5702145429217.3555 - accuracy: 0.6492\n",
            "Epoch 136/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -5810265448674.2988 - accuracy: 0.6492\n",
            "Epoch 137/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -5919165956488.2510 - accuracy: 0.6492\n",
            "Epoch 138/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -6028924588786.3281 - accuracy: 0.6492\n",
            "Epoch 139/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -6139566894342.1289 - accuracy: 0.6492\n",
            "Epoch 140/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -6251732310751.4697 - accuracy: 0.6492\n",
            "Epoch 141/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -6364814512286.4082 - accuracy: 0.6492\n",
            "Epoch 142/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -6479927861730.7705 - accuracy: 0.6492\n",
            "Epoch 143/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -6597019325449.4297 - accuracy: 0.6492\n",
            "Epoch 144/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -6714627922987.3740 - accuracy: 0.6492\n",
            "Epoch 145/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -6834544762502.8359 - accuracy: 0.6492\n",
            "Epoch 146/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -6955873692300.4932 - accuracy: 0.6492\n",
            "Epoch 147/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -7078319978133.9229 - accuracy: 0.6492\n",
            "Epoch 148/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -7200861833521.5029 - accuracy: 0.6492\n",
            "Epoch 149/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -7325676716725.9814 - accuracy: 0.6492\n",
            "Epoch 150/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -7451894436722.5635 - accuracy: 0.6492\n",
            "Epoch 151/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -7580032213912.2803 - accuracy: 0.6492\n",
            "Epoch 152/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -7710627784330.6084 - accuracy: 0.6492\n",
            "Epoch 153/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -7842265385476.7139 - accuracy: 0.6492\n",
            "Epoch 154/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -7976151357296.6777 - accuracy: 0.6492\n",
            "Epoch 155/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -8110953577681.3262 - accuracy: 0.6492\n",
            "Epoch 156/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -8249034317024.4121 - accuracy: 0.6492\n",
            "Epoch 157/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -8386645139403.1982 - accuracy: 0.6492\n",
            "Epoch 158/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -8525709811365.0088 - accuracy: 0.6492\n",
            "Epoch 159/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -8665313201836.5527 - accuracy: 0.6492\n",
            "Epoch 160/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -8806566781938.7988 - accuracy: 0.6492\n",
            "Epoch 161/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -8950135362569.4277 - accuracy: 0.6492\n",
            "Epoch 162/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -9094478197030.1895 - accuracy: 0.6492\n",
            "Epoch 163/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -9241438157610.9023 - accuracy: 0.6492\n",
            "Epoch 164/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -9388656215373.7891 - accuracy: 0.6492\n",
            "Epoch 165/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -9537657203521.5332 - accuracy: 0.6492\n",
            "Epoch 166/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -9687940279392.1777 - accuracy: 0.6492\n",
            "Epoch 167/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -9838629852109.0820 - accuracy: 0.6492\n",
            "Epoch 168/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -9989899560616.7793 - accuracy: 0.6492\n",
            "Epoch 169/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -10143413045161.2520 - accuracy: 0.6492\n",
            "Epoch 170/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -10298179906188.4922 - accuracy: 0.6492\n",
            "Epoch 171/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -10454551165648.3828 - accuracy: 0.6492\n",
            "Epoch 172/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -10611754539245.6133 - accuracy: 0.6492\n",
            "Epoch 173/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -10770099158828.7871 - accuracy: 0.6492\n",
            "Epoch 174/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -10931059421297.1484 - accuracy: 0.6492\n",
            "Epoch 175/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -11092971414394.1055 - accuracy: 0.6492\n",
            "Epoch 176/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -11256120104247.1602 - accuracy: 0.6492\n",
            "Epoch 177/1000\n",
            "1086/1086 [==============================] - 0s 113us/step - loss: -11422284573384.8398 - accuracy: 0.6492\n",
            "Epoch 178/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -11589203505657.3984 - accuracy: 0.6492\n",
            "Epoch 179/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -11758245875719.5449 - accuracy: 0.6492\n",
            "Epoch 180/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -11928550744531.6836 - accuracy: 0.6492\n",
            "Epoch 181/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -12100359681983.8828 - accuracy: 0.6492\n",
            "Epoch 182/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -12273435228880.3828 - accuracy: 0.6492\n",
            "Epoch 183/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -12447294815350.8066 - accuracy: 0.6492\n",
            "Epoch 184/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -12623057618276.4199 - accuracy: 0.6492\n",
            "Epoch 185/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -12800115225141.7461 - accuracy: 0.6492\n",
            "Epoch 186/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -12977105053443.3008 - accuracy: 0.6492\n",
            "Epoch 187/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -13156539138289.3848 - accuracy: 0.6492\n",
            "Epoch 188/1000\n",
            "1086/1086 [==============================] - 0s 148us/step - loss: -13336935454278.7188 - accuracy: 0.6492\n",
            "Epoch 189/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -13519414925262.9688 - accuracy: 0.6492\n",
            "Epoch 190/1000\n",
            "1086/1086 [==============================] - 0s 150us/step - loss: -13703087606608.6191 - accuracy: 0.6492\n",
            "Epoch 191/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -13888311045563.1680 - accuracy: 0.6492\n",
            "Epoch 192/1000\n",
            "1086/1086 [==============================] - 0s 147us/step - loss: -14076104900970.0762 - accuracy: 0.6492\n",
            "Epoch 193/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -14266343014731.9043 - accuracy: 0.6492\n",
            "Epoch 194/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -14458846179033.8105 - accuracy: 0.6492\n",
            "Epoch 195/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -14652109420781.6133 - accuracy: 0.6492\n",
            "Epoch 196/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -14847208664778.7246 - accuracy: 0.6492\n",
            "Epoch 197/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -15044141793461.0391 - accuracy: 0.6492\n",
            "Epoch 198/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -15242920169539.8906 - accuracy: 0.6492\n",
            "Epoch 199/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -15443608576437.5117 - accuracy: 0.6492\n",
            "Epoch 200/1000\n",
            "1086/1086 [==============================] - 0s 152us/step - loss: -15646244337539.5371 - accuracy: 0.6492\n",
            "Epoch 201/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -15849282290901.0977 - accuracy: 0.6492\n",
            "Epoch 202/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -16055431082157.4941 - accuracy: 0.6492\n",
            "Epoch 203/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -16263952742305.7090 - accuracy: 0.6492\n",
            "Epoch 204/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -16474996625445.7168 - accuracy: 0.6492\n",
            "Epoch 205/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -16686133235146.2520 - accuracy: 0.6492\n",
            "Epoch 206/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -16900001593581.6113 - accuracy: 0.6492\n",
            "Epoch 207/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -17113530224813.4941 - accuracy: 0.6492\n",
            "Epoch 208/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -17330342120363.1387 - accuracy: 0.6492\n",
            "Epoch 209/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -17548960942777.7539 - accuracy: 0.6492\n",
            "Epoch 210/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -17767199884501.0977 - accuracy: 0.6492\n",
            "Epoch 211/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -17988596508630.5117 - accuracy: 0.6492\n",
            "Epoch 212/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -18210177320058.5781 - accuracy: 0.6492\n",
            "Epoch 213/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -18434531009975.3945 - accuracy: 0.6492\n",
            "Epoch 214/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -18662365512615.3672 - accuracy: 0.6492\n",
            "Epoch 215/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -18891591902134.4531 - accuracy: 0.6492\n",
            "Epoch 216/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -19124238296684.4375 - accuracy: 0.6492\n",
            "Epoch 217/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -19356248651877.8359 - accuracy: 0.6492\n",
            "Epoch 218/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -19589895782309.4805 - accuracy: 0.6492\n",
            "Epoch 219/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -19828350051013.0703 - accuracy: 0.6492\n",
            "Epoch 220/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -20068953663325.8203 - accuracy: 0.6492\n",
            "Epoch 221/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -20309843131480.6328 - accuracy: 0.6492\n",
            "Epoch 222/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -20553006787061.6250 - accuracy: 0.6492\n",
            "Epoch 223/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -20798873880474.1641 - accuracy: 0.6492\n",
            "Epoch 224/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -21045273914117.1875 - accuracy: 0.6492\n",
            "Epoch 225/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -21296556301506.2383 - accuracy: 0.6492\n",
            "Epoch 226/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -21547274372218.5781 - accuracy: 0.6492\n",
            "Epoch 227/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -21796263469614.2031 - accuracy: 0.6492\n",
            "Epoch 228/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -22049996546063.0898 - accuracy: 0.6492\n",
            "Epoch 229/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -22302915435472.8516 - accuracy: 0.6492\n",
            "Epoch 230/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -22558767211627.4922 - accuracy: 0.6492\n",
            "Epoch 231/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -22816964453755.0508 - accuracy: 0.6492\n",
            "Epoch 232/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -23077686730857.6055 - accuracy: 0.6492\n",
            "Epoch 233/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -23339670558821.8359 - accuracy: 0.6492\n",
            "Epoch 234/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -23602886298116.7148 - accuracy: 0.6492\n",
            "Epoch 235/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -23871462821141.2148 - accuracy: 0.6492\n",
            "Epoch 236/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -24140200309271.5703 - accuracy: 0.6492\n",
            "Epoch 237/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -24412438047644.0508 - accuracy: 0.6492\n",
            "Epoch 238/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -24682813092013.4961 - accuracy: 0.6492\n",
            "Epoch 239/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -24956332314933.2734 - accuracy: 0.6492\n",
            "Epoch 240/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -25230540868130.8867 - accuracy: 0.6492\n",
            "Epoch 241/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -25508741487146.4336 - accuracy: 0.6492\n",
            "Epoch 242/1000\n",
            "1086/1086 [==============================] - 0s 153us/step - loss: -25787360728013.0859 - accuracy: 0.6492\n",
            "Epoch 243/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -26067003971233.2383 - accuracy: 0.6492\n",
            "Epoch 244/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -26351275295715.7148 - accuracy: 0.6492\n",
            "Epoch 245/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -26639075074244.1250 - accuracy: 0.6492\n",
            "Epoch 246/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -26925660009466.3438 - accuracy: 0.6492\n",
            "Epoch 247/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -27214265402460.4062 - accuracy: 0.6492\n",
            "Epoch 248/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -27504987212624.6172 - accuracy: 0.6492\n",
            "Epoch 249/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -27796950899815.7227 - accuracy: 0.6492\n",
            "Epoch 250/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -28093237350618.7539 - accuracy: 0.6492\n",
            "Epoch 251/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -28389870323625.2539 - accuracy: 0.6492\n",
            "Epoch 252/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -28691347045494.8047 - accuracy: 0.6492\n",
            "Epoch 253/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -28991212266030.2031 - accuracy: 0.6492\n",
            "Epoch 254/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -29294701188558.0273 - accuracy: 0.6492\n",
            "Epoch 255/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -29603197791041.5312 - accuracy: 0.6492\n",
            "Epoch 256/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -29911821168843.6719 - accuracy: 0.6492\n",
            "Epoch 257/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -30222530982755.4766 - accuracy: 0.6492\n",
            "Epoch 258/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -30531055433452.6719 - accuracy: 0.6492\n",
            "Epoch 259/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -30844929690659.8320 - accuracy: 0.6492\n",
            "Epoch 260/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -31159161312657.6797 - accuracy: 0.6492\n",
            "Epoch 261/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -31476579870678.5117 - accuracy: 0.6492\n",
            "Epoch 262/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -31794946616099.3594 - accuracy: 0.6492\n",
            "Epoch 263/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -32114259275195.1680 - accuracy: 0.6492\n",
            "Epoch 264/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -32438117811533.7930 - accuracy: 0.6492\n",
            "Epoch 265/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -32764733781589.8047 - accuracy: 0.6492\n",
            "Epoch 266/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -33094335974435.8320 - accuracy: 0.6492\n",
            "Epoch 267/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -33425956251018.1406 - accuracy: 0.6492\n",
            "Epoch 268/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -33761332429041.3828 - accuracy: 0.6492\n",
            "Epoch 269/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -34098702771790.2656 - accuracy: 0.6492\n",
            "Epoch 270/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -34438476179897.2812 - accuracy: 0.6492\n",
            "Epoch 271/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -34779678184276.3867 - accuracy: 0.6492\n",
            "Epoch 272/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -35122129169881.3398 - accuracy: 0.6492\n",
            "Epoch 273/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -35466908638598.3672 - accuracy: 0.6492\n",
            "Epoch 274/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -35812046338527.0000 - accuracy: 0.6492\n",
            "Epoch 275/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -36161621374217.8984 - accuracy: 0.6492\n",
            "Epoch 276/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -36515090086155.7891 - accuracy: 0.6492\n",
            "Epoch 277/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -36873227316244.7500 - accuracy: 0.6492\n",
            "Epoch 278/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -37235652024904.6016 - accuracy: 0.6492\n",
            "Epoch 279/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -37601965145327.5000 - accuracy: 0.6492\n",
            "Epoch 280/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -37966240230601.7812 - accuracy: 0.6492\n",
            "Epoch 281/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -38330233831333.4766 - accuracy: 0.6492\n",
            "Epoch 282/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -38696788462476.9688 - accuracy: 0.6492\n",
            "Epoch 283/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -39062954170711.2188 - accuracy: 0.6492\n",
            "Epoch 284/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -39430752437189.5391 - accuracy: 0.6492\n",
            "Epoch 285/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -39802557552983.2188 - accuracy: 0.6492\n",
            "Epoch 286/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -40171556114560.2344 - accuracy: 0.6492\n",
            "Epoch 287/1000\n",
            "1086/1086 [==============================] - 0s 144us/step - loss: -40548710357124.0078 - accuracy: 0.6492\n",
            "Epoch 288/1000\n",
            "1086/1086 [==============================] - 0s 150us/step - loss: -40925654522169.0469 - accuracy: 0.6492\n",
            "Epoch 289/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -41306320994666.0781 - accuracy: 0.6492\n",
            "Epoch 290/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -41685892684360.6016 - accuracy: 0.6492\n",
            "Epoch 291/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -42069065607186.8594 - accuracy: 0.6492\n",
            "Epoch 292/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -42454732864992.8828 - accuracy: 0.6492\n",
            "Epoch 293/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -42843593158256.2031 - accuracy: 0.6492\n",
            "Epoch 294/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -43233191419225.1094 - accuracy: 0.6492\n",
            "Epoch 295/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -43626158571067.4062 - accuracy: 0.6492\n",
            "Epoch 296/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -44020190578631.4297 - accuracy: 0.6492\n",
            "Epoch 297/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -44418125691057.2656 - accuracy: 0.6492\n",
            "Epoch 298/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -44813453468447.5859 - accuracy: 0.6492\n",
            "Epoch 299/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -45217524577636.4141 - accuracy: 0.6492\n",
            "Epoch 300/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -45621112012920.6953 - accuracy: 0.6492\n",
            "Epoch 301/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -46031127750493.8203 - accuracy: 0.6492\n",
            "Epoch 302/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -46439669097157.0703 - accuracy: 0.6492\n",
            "Epoch 303/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -46855616261412.2969 - accuracy: 0.6492\n",
            "Epoch 304/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -47268409006483.5703 - accuracy: 0.6492\n",
            "Epoch 305/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -47684347813631.5312 - accuracy: 0.6492\n",
            "Epoch 306/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -48104028853106.5625 - accuracy: 0.6492\n",
            "Epoch 307/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -48528871036999.6641 - accuracy: 0.6492\n",
            "Epoch 308/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -48952103257964.9062 - accuracy: 0.6492\n",
            "Epoch 309/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -49377326651373.1406 - accuracy: 0.6492\n",
            "Epoch 310/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -49805665447113.7812 - accuracy: 0.6492\n",
            "Epoch 311/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -50231949831188.7500 - accuracy: 0.6492\n",
            "Epoch 312/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -50666076045191.3047 - accuracy: 0.6492\n",
            "Epoch 313/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -51103037412331.2500 - accuracy: 0.6492\n",
            "Epoch 314/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -51542046569707.7266 - accuracy: 0.6492\n",
            "Epoch 315/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -51986985199152.0859 - accuracy: 0.6492\n",
            "Epoch 316/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -52430718105234.1562 - accuracy: 0.6492\n",
            "Epoch 317/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -52875784173353.0156 - accuracy: 0.6492\n",
            "Epoch 318/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -53321451625394.6797 - accuracy: 0.6492\n",
            "Epoch 319/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -53766502564473.6328 - accuracy: 0.6492\n",
            "Epoch 320/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -54215680374025.8984 - accuracy: 0.6492\n",
            "Epoch 321/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -54667090908440.9922 - accuracy: 0.6492\n",
            "Epoch 322/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -55123715816868.5312 - accuracy: 0.6492\n",
            "Epoch 323/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -55582517796698.0547 - accuracy: 0.6492\n",
            "Epoch 324/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -56044600307976.0156 - accuracy: 0.6492\n",
            "Epoch 325/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -56511109090762.2578 - accuracy: 0.6492\n",
            "Epoch 326/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -56979048452495.7969 - accuracy: 0.6492\n",
            "Epoch 327/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -57453198910782.7031 - accuracy: 0.6492\n",
            "Epoch 328/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -57926025972439.9297 - accuracy: 0.6492\n",
            "Epoch 329/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -58400766664658.7422 - accuracy: 0.6492\n",
            "Epoch 330/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -58879781340041.1953 - accuracy: 0.6492\n",
            "Epoch 331/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -59357853495378.9766 - accuracy: 0.6492\n",
            "Epoch 332/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -59841276275038.7656 - accuracy: 0.6492\n",
            "Epoch 333/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -60327168215979.1328 - accuracy: 0.6492\n",
            "Epoch 334/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -60816545997871.1406 - accuracy: 0.6492\n",
            "Epoch 335/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -61311993303138.0625 - accuracy: 0.6492\n",
            "Epoch 336/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -61808345585735.6641 - accuracy: 0.6492\n",
            "Epoch 337/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -62302287491667.9219 - accuracy: 0.6492\n",
            "Epoch 338/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -62799493461167.3828 - accuracy: 0.6492\n",
            "Epoch 339/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -63300405919442.2734 - accuracy: 0.6492\n",
            "Epoch 340/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -63807814850239.4141 - accuracy: 0.6492\n",
            "Epoch 341/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -64315882654876.5234 - accuracy: 0.6492\n",
            "Epoch 342/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -64830979829933.4922 - accuracy: 0.6492\n",
            "Epoch 343/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -65350006818849.9375 - accuracy: 0.6492\n",
            "Epoch 344/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -65862005177915.4062 - accuracy: 0.6492\n",
            "Epoch 345/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -66386772035346.3906 - accuracy: 0.6492\n",
            "Epoch 346/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -66911990566406.5938 - accuracy: 0.6492\n",
            "Epoch 347/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -67432232477320.7188 - accuracy: 0.6492\n",
            "Epoch 348/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -67960176572170.8438 - accuracy: 0.6492\n",
            "Epoch 349/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -68484422869550.1953 - accuracy: 0.6492\n",
            "Epoch 350/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -69007692621148.8828 - accuracy: 0.6492\n",
            "Epoch 351/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -69538923379710.1094 - accuracy: 0.6492\n",
            "Epoch 352/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -70067350436982.8125 - accuracy: 0.6492\n",
            "Epoch 353/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -70603386101231.9688 - accuracy: 0.6492\n",
            "Epoch 354/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -71137870713261.9688 - accuracy: 0.6492\n",
            "Epoch 355/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -71673576981747.2656 - accuracy: 0.6492\n",
            "Epoch 356/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -72214077060663.6406 - accuracy: 0.6492\n",
            "Epoch 357/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -72759606246186.9062 - accuracy: 0.6492\n",
            "Epoch 358/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -73311478937837.6094 - accuracy: 0.6492\n",
            "Epoch 359/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -73861740616747.3750 - accuracy: 0.6492\n",
            "Epoch 360/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -74418718664390.9531 - accuracy: 0.6492\n",
            "Epoch 361/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -74980740597220.6562 - accuracy: 0.6492\n",
            "Epoch 362/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -75546627632601.3438 - accuracy: 0.6492\n",
            "Epoch 363/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -76113006416814.9219 - accuracy: 0.6492\n",
            "Epoch 364/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -76683316263451.3438 - accuracy: 0.6492\n",
            "Epoch 365/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -77255731347154.2656 - accuracy: 0.6492\n",
            "Epoch 366/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -77829488142988.5000 - accuracy: 0.6492\n",
            "Epoch 367/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -78402848414039.2188 - accuracy: 0.6492\n",
            "Epoch 368/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -78980574366816.1719 - accuracy: 0.6492\n",
            "Epoch 369/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -79560883117974.3906 - accuracy: 0.6492\n",
            "Epoch 370/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -80144726299476.3906 - accuracy: 0.6492\n",
            "Epoch 371/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -80731047913219.2969 - accuracy: 0.6492\n",
            "Epoch 372/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -81328152445169.3906 - accuracy: 0.6492\n",
            "Epoch 373/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -81920653140707.2344 - accuracy: 0.6492\n",
            "Epoch 374/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -82523902939807.3594 - accuracy: 0.6492\n",
            "Epoch 375/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -83122574620125.1094 - accuracy: 0.6492\n",
            "Epoch 376/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -83725521698906.5156 - accuracy: 0.6492\n",
            "Epoch 377/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -84337756229428.3438 - accuracy: 0.6492\n",
            "Epoch 378/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -84949137161395.1562 - accuracy: 0.6492\n",
            "Epoch 379/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -85564820149528.9844 - accuracy: 0.6492\n",
            "Epoch 380/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -86181404385234.7344 - accuracy: 0.6492\n",
            "Epoch 381/1000\n",
            "1086/1086 [==============================] - 0s 155us/step - loss: -86800521008378.8125 - accuracy: 0.6492\n",
            "Epoch 382/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -87421800491026.8594 - accuracy: 0.6492\n",
            "Epoch 383/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -88046047947163.1094 - accuracy: 0.6492\n",
            "Epoch 384/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -88676273175037.1719 - accuracy: 0.6492\n",
            "Epoch 385/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -89307531358049.5938 - accuracy: 0.6492\n",
            "Epoch 386/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -89944892446972.7031 - accuracy: 0.6492\n",
            "Epoch 387/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -90580642813812.4531 - accuracy: 0.6492\n",
            "Epoch 388/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -91227071812979.5000 - accuracy: 0.6492\n",
            "Epoch 389/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -91867350872871.1406 - accuracy: 0.6492\n",
            "Epoch 390/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -92512392074460.6406 - accuracy: 0.6492\n",
            "Epoch 391/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -93161423038211.2969 - accuracy: 0.6492\n",
            "Epoch 392/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -93809465396108.9688 - accuracy: 0.6492\n",
            "Epoch 393/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -94461619012428.8438 - accuracy: 0.6492\n",
            "Epoch 394/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -95117169375318.7500 - accuracy: 0.6492\n",
            "Epoch 395/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -95778866089659.6406 - accuracy: 0.6492\n",
            "Epoch 396/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -96439075109765.4219 - accuracy: 0.6492\n",
            "Epoch 397/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -97105111218413.6094 - accuracy: 0.6492\n",
            "Epoch 398/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -97771696523931.5781 - accuracy: 0.6492\n",
            "Epoch 399/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -98445534986300.3594 - accuracy: 0.6492\n",
            "Epoch 400/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -99123382247046.8438 - accuracy: 0.6492\n",
            "Epoch 401/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -99799928385739.6562 - accuracy: 0.6492\n",
            "Epoch 402/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -100482970445028.1875 - accuracy: 0.6492\n",
            "Epoch 403/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -101165673098803.8594 - accuracy: 0.6492\n",
            "Epoch 404/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -101851428253386.7188 - accuracy: 0.6492\n",
            "Epoch 405/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -102538847192005.5312 - accuracy: 0.6492\n",
            "Epoch 406/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -103229950036876.9688 - accuracy: 0.6492\n",
            "Epoch 407/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -103925115484216.5781 - accuracy: 0.6492\n",
            "Epoch 408/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -104626852533559.1719 - accuracy: 0.6492\n",
            "Epoch 409/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -105329266906996.4531 - accuracy: 0.6492\n",
            "Epoch 410/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -106034703389916.6406 - accuracy: 0.6492\n",
            "Epoch 411/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -106745630569194.7812 - accuracy: 0.6492\n",
            "Epoch 412/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -107462332788034.4688 - accuracy: 0.6492\n",
            "Epoch 413/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -108179033278558.2812 - accuracy: 0.6492\n",
            "Epoch 414/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -108904996650977.8281 - accuracy: 0.6492\n",
            "Epoch 415/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -109626352847907.8281 - accuracy: 0.6492\n",
            "Epoch 416/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -110354274107482.5156 - accuracy: 0.6492\n",
            "Epoch 417/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -111083252494962.0938 - accuracy: 0.6492\n",
            "Epoch 418/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -111811525057549.2031 - accuracy: 0.6492\n",
            "Epoch 419/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -112541684369110.0312 - accuracy: 0.6492\n",
            "Epoch 420/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -113278495364045.0781 - accuracy: 0.6492\n",
            "Epoch 421/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -114015919400345.2188 - accuracy: 0.6492\n",
            "Epoch 422/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -114756845866834.5000 - accuracy: 0.6492\n",
            "Epoch 423/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -115502014634297.0469 - accuracy: 0.6492\n",
            "Epoch 424/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -116246701228665.6406 - accuracy: 0.6492\n",
            "Epoch 425/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -116996713260048.9688 - accuracy: 0.6492\n",
            "Epoch 426/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -117744898888321.1719 - accuracy: 0.6492\n",
            "Epoch 427/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -118506314754579.7969 - accuracy: 0.6492\n",
            "Epoch 428/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -119259820717231.3906 - accuracy: 0.6492\n",
            "Epoch 429/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -120020306788653.7344 - accuracy: 0.6492\n",
            "Epoch 430/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -120789738754840.0469 - accuracy: 0.6492\n",
            "Epoch 431/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -121558394003078.8438 - accuracy: 0.6492\n",
            "Epoch 432/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -122327628164720.2031 - accuracy: 0.6492\n",
            "Epoch 433/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -123099350811076.5938 - accuracy: 0.6492\n",
            "Epoch 434/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -123882540826503.3125 - accuracy: 0.6492\n",
            "Epoch 435/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -124663724211439.5000 - accuracy: 0.6492\n",
            "Epoch 436/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -125449455135312.1406 - accuracy: 0.6492\n",
            "Epoch 437/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -126235504545558.1562 - accuracy: 0.6492\n",
            "Epoch 438/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -127021028140803.2969 - accuracy: 0.6492\n",
            "Epoch 439/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -127814452554410.6719 - accuracy: 0.6492\n",
            "Epoch 440/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -128606643297112.1562 - accuracy: 0.6492\n",
            "Epoch 441/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -129406128877560.4531 - accuracy: 0.6492\n",
            "Epoch 442/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -130204889132384.6406 - accuracy: 0.6492\n",
            "Epoch 443/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -131014014538942.4688 - accuracy: 0.6492\n",
            "Epoch 444/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -131823235740817.2031 - accuracy: 0.6492\n",
            "Epoch 445/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -132643912996603.7500 - accuracy: 0.6492\n",
            "Epoch 446/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -133463030870702.4531 - accuracy: 0.6492\n",
            "Epoch 447/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -134288126132978.3125 - accuracy: 0.6492\n",
            "Epoch 448/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -135113698817893.3594 - accuracy: 0.6492\n",
            "Epoch 449/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -135944546962045.4219 - accuracy: 0.6492\n",
            "Epoch 450/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -136776431550433.8125 - accuracy: 0.6492\n",
            "Epoch 451/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -137607389647096.9375 - accuracy: 0.6492\n",
            "Epoch 452/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -138439013735557.8906 - accuracy: 0.6492\n",
            "Epoch 453/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -139280781166988.0156 - accuracy: 0.6492\n",
            "Epoch 454/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -140129525899641.1719 - accuracy: 0.6492\n",
            "Epoch 455/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -140979301074977.9375 - accuracy: 0.6492\n",
            "Epoch 456/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -141828660999011.5000 - accuracy: 0.6492\n",
            "Epoch 457/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -142681044602012.5000 - accuracy: 0.6492\n",
            "Epoch 458/1000\n",
            "1086/1086 [==============================] - 0s 148us/step - loss: -143534942463656.7812 - accuracy: 0.6492\n",
            "Epoch 459/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -144391445366576.5625 - accuracy: 0.6492\n",
            "Epoch 460/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -145255110340962.5312 - accuracy: 0.6492\n",
            "Epoch 461/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -146114099720757.7500 - accuracy: 0.6492\n",
            "Epoch 462/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -146986516532380.5000 - accuracy: 0.6492\n",
            "Epoch 463/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -147858118374454.6875 - accuracy: 0.6492\n",
            "Epoch 464/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -148740774255599.0312 - accuracy: 0.6492\n",
            "Epoch 465/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -149623913519839.4688 - accuracy: 0.6492\n",
            "Epoch 466/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -150507785135241.6562 - accuracy: 0.6492\n",
            "Epoch 467/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -151391248662678.8438 - accuracy: 0.6492\n",
            "Epoch 468/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -152279418522659.8438 - accuracy: 0.6492\n",
            "Epoch 469/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -153171975215236.0000 - accuracy: 0.6492\n",
            "Epoch 470/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -154067297091071.0625 - accuracy: 0.6492\n",
            "Epoch 471/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -154960206672920.5312 - accuracy: 0.6492\n",
            "Epoch 472/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -155864420110916.8438 - accuracy: 0.6492\n",
            "Epoch 473/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -156768175315649.2812 - accuracy: 0.6492\n",
            "Epoch 474/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -157676248041958.5312 - accuracy: 0.6492\n",
            "Epoch 475/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -158583144242581.4375 - accuracy: 0.6492\n",
            "Epoch 476/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -159499433904192.1250 - accuracy: 0.6492\n",
            "Epoch 477/1000\n",
            "1086/1086 [==============================] - 0s 149us/step - loss: -160415173071730.5625 - accuracy: 0.6492\n",
            "Epoch 478/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -161338066883804.6250 - accuracy: 0.6492\n",
            "Epoch 479/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -162263049722018.1562 - accuracy: 0.6492\n",
            "Epoch 480/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -163190546316738.7188 - accuracy: 0.6492\n",
            "Epoch 481/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -164111693543842.6250 - accuracy: 0.6492\n",
            "Epoch 482/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -165046921875976.5000 - accuracy: 0.6492\n",
            "Epoch 483/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -165978831181443.0625 - accuracy: 0.6492\n",
            "Epoch 484/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -166919880341198.5000 - accuracy: 0.6492\n",
            "Epoch 485/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -167855999787204.1250 - accuracy: 0.6492\n",
            "Epoch 486/1000\n",
            "1086/1086 [==============================] - 0s 113us/step - loss: -168795197741791.4688 - accuracy: 0.6492\n",
            "Epoch 487/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -169742542550644.0000 - accuracy: 0.6492\n",
            "Epoch 488/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -170693706247701.6875 - accuracy: 0.6492\n",
            "Epoch 489/1000\n",
            "1086/1086 [==============================] - 0s 112us/step - loss: -171648927159516.6250 - accuracy: 0.6492\n",
            "Epoch 490/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -172606199246991.3125 - accuracy: 0.6492\n",
            "Epoch 491/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -173572929655938.1250 - accuracy: 0.6492\n",
            "Epoch 492/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -174540709928673.3438 - accuracy: 0.6492\n",
            "Epoch 493/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -175504728110466.5938 - accuracy: 0.6492\n",
            "Epoch 494/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -176471789299640.3438 - accuracy: 0.6492\n",
            "Epoch 495/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -177448487680479.0000 - accuracy: 0.6492\n",
            "Epoch 496/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -178432423664532.5000 - accuracy: 0.6492\n",
            "Epoch 497/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -179416953082738.5625 - accuracy: 0.6492\n",
            "Epoch 498/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -180417697992077.9062 - accuracy: 0.6492\n",
            "Epoch 499/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -181412175948881.0938 - accuracy: 0.6492\n",
            "Epoch 500/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -182406308774038.8438 - accuracy: 0.6492\n",
            "Epoch 501/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -183410282804067.5000 - accuracy: 0.6492\n",
            "Epoch 502/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -184416782802989.2500 - accuracy: 0.6492\n",
            "Epoch 503/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -185427604981492.2188 - accuracy: 0.6492\n",
            "Epoch 504/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -186452285903187.4688 - accuracy: 0.6492\n",
            "Epoch 505/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -187474157120110.3438 - accuracy: 0.6492\n",
            "Epoch 506/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -188504435261319.2812 - accuracy: 0.6492\n",
            "Epoch 507/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -189533240776986.8750 - accuracy: 0.6492\n",
            "Epoch 508/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -190562785252693.3438 - accuracy: 0.6492\n",
            "Epoch 509/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -191594455361404.0000 - accuracy: 0.6492\n",
            "Epoch 510/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -192630173800700.6875 - accuracy: 0.6492\n",
            "Epoch 511/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -193668203457918.8125 - accuracy: 0.6492\n",
            "Epoch 512/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -194711256952824.4688 - accuracy: 0.6492\n",
            "Epoch 513/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -195748578275712.7188 - accuracy: 0.6492\n",
            "Epoch 514/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -196790895682697.6562 - accuracy: 0.6492\n",
            "Epoch 515/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -197849762885624.4688 - accuracy: 0.6492\n",
            "Epoch 516/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -198903960001277.6562 - accuracy: 0.6492\n",
            "Epoch 517/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -199963342052968.6875 - accuracy: 0.6492\n",
            "Epoch 518/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -201027675918886.6562 - accuracy: 0.6492\n",
            "Epoch 519/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -202095785979262.8125 - accuracy: 0.6492\n",
            "Epoch 520/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -203172731377705.5000 - accuracy: 0.6492\n",
            "Epoch 521/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -204252885970996.8125 - accuracy: 0.6492\n",
            "Epoch 522/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -205342298782144.8125 - accuracy: 0.6492\n",
            "Epoch 523/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -206437148357022.8750 - accuracy: 0.6492\n",
            "Epoch 524/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -207533408046960.6875 - accuracy: 0.6492\n",
            "Epoch 525/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -208643937531055.3750 - accuracy: 0.6492\n",
            "Epoch 526/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -209729996860497.0938 - accuracy: 0.6492\n",
            "Epoch 527/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -210813275049207.0312 - accuracy: 0.6492\n",
            "Epoch 528/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -211909783405419.0312 - accuracy: 0.6492\n",
            "Epoch 529/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -213009986515720.9688 - accuracy: 0.6492\n",
            "Epoch 530/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -214106476853274.4062 - accuracy: 0.6492\n",
            "Epoch 531/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -215210301210412.7812 - accuracy: 0.6492\n",
            "Epoch 532/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -216312293407892.9688 - accuracy: 0.6492\n",
            "Epoch 533/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -217420190372662.2188 - accuracy: 0.6492\n",
            "Epoch 534/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -218534519067506.5625 - accuracy: 0.6492\n",
            "Epoch 535/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -219656277359010.6250 - accuracy: 0.6492\n",
            "Epoch 536/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -220777161971391.4062 - accuracy: 0.6492\n",
            "Epoch 537/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -221897759484435.8125 - accuracy: 0.6492\n",
            "Epoch 538/1000\n",
            "1086/1086 [==============================] - 0s 113us/step - loss: -223034187849984.4688 - accuracy: 0.6492\n",
            "Epoch 539/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -224168419243153.1875 - accuracy: 0.6492\n",
            "Epoch 540/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -225313876251174.6562 - accuracy: 0.6492\n",
            "Epoch 541/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -226461620708429.3125 - accuracy: 0.6492\n",
            "Epoch 542/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -227606637902544.3750 - accuracy: 0.6492\n",
            "Epoch 543/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -228763575725740.5312 - accuracy: 0.6492\n",
            "Epoch 544/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -229931889897555.0000 - accuracy: 0.6492\n",
            "Epoch 545/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -231087991137659.0625 - accuracy: 0.6492\n",
            "Epoch 546/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -232252578392294.0625 - accuracy: 0.6492\n",
            "Epoch 547/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -233416440286756.7812 - accuracy: 0.6492\n",
            "Epoch 548/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -234592206061085.2188 - accuracy: 0.6492\n",
            "Epoch 549/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -235774796170543.6250 - accuracy: 0.6492\n",
            "Epoch 550/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -236965051149453.4375 - accuracy: 0.6492\n",
            "Epoch 551/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -238166893269965.0625 - accuracy: 0.6492\n",
            "Epoch 552/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -239368600970468.1875 - accuracy: 0.6492\n",
            "Epoch 553/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -240572578779596.1250 - accuracy: 0.6492\n",
            "Epoch 554/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -241775545713183.1250 - accuracy: 0.6492\n",
            "Epoch 555/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -242987250147071.5312 - accuracy: 0.6492\n",
            "Epoch 556/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -244197776450788.1875 - accuracy: 0.6492\n",
            "Epoch 557/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -245418326152714.3750 - accuracy: 0.6492\n",
            "Epoch 558/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -246645177140557.8125 - accuracy: 0.6492\n",
            "Epoch 559/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -247885100034066.8750 - accuracy: 0.6492\n",
            "Epoch 560/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -249119730514911.9375 - accuracy: 0.6492\n",
            "Epoch 561/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -250364707013935.6250 - accuracy: 0.6492\n",
            "Epoch 562/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -251604046120239.6250 - accuracy: 0.6492\n",
            "Epoch 563/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -252854240324508.0312 - accuracy: 0.6492\n",
            "Epoch 564/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -254116297382864.8438 - accuracy: 0.6492\n",
            "Epoch 565/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -255384772961990.9375 - accuracy: 0.6492\n",
            "Epoch 566/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -256654144430064.9062 - accuracy: 0.6492\n",
            "Epoch 567/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -257928658072117.7500 - accuracy: 0.6492\n",
            "Epoch 568/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -259202752071593.2500 - accuracy: 0.6492\n",
            "Epoch 569/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -260477583092062.7812 - accuracy: 0.6492\n",
            "Epoch 570/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -261763580643844.7188 - accuracy: 0.6492\n",
            "Epoch 571/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -263062791724226.2188 - accuracy: 0.6492\n",
            "Epoch 572/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -264350858178733.4688 - accuracy: 0.6492\n",
            "Epoch 573/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -265650037995667.0938 - accuracy: 0.6492\n",
            "Epoch 574/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -266948033341971.8125 - accuracy: 0.6492\n",
            "Epoch 575/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -268251026092145.1875 - accuracy: 0.6492\n",
            "Epoch 576/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -269546778822599.4375 - accuracy: 0.6492\n",
            "Epoch 577/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -270860306404150.2188 - accuracy: 0.6492\n",
            "Epoch 578/1000\n",
            "1086/1086 [==============================] - 0s 146us/step - loss: -272163259535578.7812 - accuracy: 0.6492\n",
            "Epoch 579/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -273471965282877.3125 - accuracy: 0.6492\n",
            "Epoch 580/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -274785709705793.0625 - accuracy: 0.6492\n",
            "Epoch 581/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -276102650793267.4062 - accuracy: 0.6492\n",
            "Epoch 582/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -277427878452931.1562 - accuracy: 0.6492\n",
            "Epoch 583/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -278752126014111.3438 - accuracy: 0.6492\n",
            "Epoch 584/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -280087340686228.5000 - accuracy: 0.6492\n",
            "Epoch 585/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -281423558266978.0625 - accuracy: 0.6492\n",
            "Epoch 586/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -282764963028162.2500 - accuracy: 0.6492\n",
            "Epoch 587/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -284102887340631.6875 - accuracy: 0.6492\n",
            "Epoch 588/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -285446118659413.3125 - accuracy: 0.6492\n",
            "Epoch 589/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -286793122933026.3750 - accuracy: 0.6492\n",
            "Epoch 590/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -288151041207105.5000 - accuracy: 0.6492\n",
            "Epoch 591/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -289508497400004.1250 - accuracy: 0.6492\n",
            "Epoch 592/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -290856676897032.0000 - accuracy: 0.6492\n",
            "Epoch 593/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -292221956883216.5000 - accuracy: 0.6492\n",
            "Epoch 594/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -293583049260693.9375 - accuracy: 0.6492\n",
            "Epoch 595/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -294960821988221.8750 - accuracy: 0.6492\n",
            "Epoch 596/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -296325347213191.3125 - accuracy: 0.6492\n",
            "Epoch 597/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -297707853796934.7500 - accuracy: 0.6492\n",
            "Epoch 598/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -299094269369658.9375 - accuracy: 0.6492\n",
            "Epoch 599/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -300479549260470.0000 - accuracy: 0.6492\n",
            "Epoch 600/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -301864246667795.8125 - accuracy: 0.6492\n",
            "Epoch 601/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -303262359512292.1875 - accuracy: 0.6492\n",
            "Epoch 602/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -304652190191804.6250 - accuracy: 0.6492\n",
            "Epoch 603/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -306063994756591.9375 - accuracy: 0.6492\n",
            "Epoch 604/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -307488125241498.6250 - accuracy: 0.6492\n",
            "Epoch 605/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -308901762822377.8750 - accuracy: 0.6492\n",
            "Epoch 606/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -310325810751718.1250 - accuracy: 0.6492\n",
            "Epoch 607/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -311742937478892.6875 - accuracy: 0.6492\n",
            "Epoch 608/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -313166517199738.0625 - accuracy: 0.6492\n",
            "Epoch 609/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -314591068831774.1875 - accuracy: 0.6492\n",
            "Epoch 610/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -316026630896095.0000 - accuracy: 0.6492\n",
            "Epoch 611/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -317466721235871.8125 - accuracy: 0.6492\n",
            "Epoch 612/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -318905612219054.4375 - accuracy: 0.6492\n",
            "Epoch 613/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -320343640481505.3750 - accuracy: 0.6492\n",
            "Epoch 614/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -321779637918733.1875 - accuracy: 0.6492\n",
            "Epoch 615/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -323217469485802.8125 - accuracy: 0.6492\n",
            "Epoch 616/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -324671791996520.6875 - accuracy: 0.6492\n",
            "Epoch 617/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -326125485464830.5625 - accuracy: 0.6492\n",
            "Epoch 618/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -327596006853245.4375 - accuracy: 0.6492\n",
            "Epoch 619/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -329076001274093.6250 - accuracy: 0.6492\n",
            "Epoch 620/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -330555250429688.0000 - accuracy: 0.6492\n",
            "Epoch 621/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -332040946559779.3125 - accuracy: 0.6492\n",
            "Epoch 622/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -333543242777632.0625 - accuracy: 0.6492\n",
            "Epoch 623/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -335034956978790.8125 - accuracy: 0.6492\n",
            "Epoch 624/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -336537801928364.5625 - accuracy: 0.6492\n",
            "Epoch 625/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -338050076832339.9375 - accuracy: 0.6492\n",
            "Epoch 626/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -339554464304194.0000 - accuracy: 0.6492\n",
            "Epoch 627/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -341074466023261.8125 - accuracy: 0.6492\n",
            "Epoch 628/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -342583560479398.9375 - accuracy: 0.6492\n",
            "Epoch 629/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -344111433338072.8750 - accuracy: 0.6492\n",
            "Epoch 630/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -345652685206280.9375 - accuracy: 0.6492\n",
            "Epoch 631/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -347194109213301.8750 - accuracy: 0.6492\n",
            "Epoch 632/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -348732157216922.6250 - accuracy: 0.6492\n",
            "Epoch 633/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -350287039440070.0625 - accuracy: 0.6492\n",
            "Epoch 634/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -351834523146794.4375 - accuracy: 0.6492\n",
            "Epoch 635/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -353387258203714.9375 - accuracy: 0.6492\n",
            "Epoch 636/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -354956186754604.3125 - accuracy: 0.6492\n",
            "Epoch 637/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -356532314566165.6875 - accuracy: 0.6492\n",
            "Epoch 638/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -358110142909717.1875 - accuracy: 0.6492\n",
            "Epoch 639/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -359684530713684.8750 - accuracy: 0.6492\n",
            "Epoch 640/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -361257389857986.2500 - accuracy: 0.6492\n",
            "Epoch 641/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -362847821563183.5625 - accuracy: 0.6492\n",
            "Epoch 642/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -364430057495052.2500 - accuracy: 0.6492\n",
            "Epoch 643/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -366006249493754.8125 - accuracy: 0.6492\n",
            "Epoch 644/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -367586234794828.8125 - accuracy: 0.6492\n",
            "Epoch 645/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -369163876932621.1875 - accuracy: 0.6492\n",
            "Epoch 646/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -370749218691928.1250 - accuracy: 0.6492\n",
            "Epoch 647/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -372342459758045.1250 - accuracy: 0.6492\n",
            "Epoch 648/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -373941217250581.1875 - accuracy: 0.6492\n",
            "Epoch 649/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -375546193108494.1250 - accuracy: 0.6492\n",
            "Epoch 650/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -377158867552501.1875 - accuracy: 0.6492\n",
            "Epoch 651/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -378784233505643.0000 - accuracy: 0.6492\n",
            "Epoch 652/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -380407759163490.0625 - accuracy: 0.6492\n",
            "Epoch 653/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -382037223272199.0625 - accuracy: 0.6492\n",
            "Epoch 654/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -383668847099873.8125 - accuracy: 0.6492\n",
            "Epoch 655/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -385308503251438.0625 - accuracy: 0.6492\n",
            "Epoch 656/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -386953252614709.7500 - accuracy: 0.6492\n",
            "Epoch 657/1000\n",
            "1086/1086 [==============================] - 0s 113us/step - loss: -388606578448123.7500 - accuracy: 0.6492\n",
            "Epoch 658/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -390271364400371.3125 - accuracy: 0.6492\n",
            "Epoch 659/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -391932538010229.8750 - accuracy: 0.6492\n",
            "Epoch 660/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -393595668050003.0000 - accuracy: 0.6492\n",
            "Epoch 661/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -395259864918336.5625 - accuracy: 0.6492\n",
            "Epoch 662/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -396935222124287.5000 - accuracy: 0.6492\n",
            "Epoch 663/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -398609200873773.6875 - accuracy: 0.6492\n",
            "Epoch 664/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -400277469458786.5000 - accuracy: 0.6492\n",
            "Epoch 665/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -401956776076274.8125 - accuracy: 0.6492\n",
            "Epoch 666/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -403645310813819.5625 - accuracy: 0.6492\n",
            "Epoch 667/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -405334345142068.3125 - accuracy: 0.6492\n",
            "Epoch 668/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -407030170171116.6875 - accuracy: 0.6492\n",
            "Epoch 669/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -408732220404700.1875 - accuracy: 0.6492\n",
            "Epoch 670/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -410449296026554.1875 - accuracy: 0.6492\n",
            "Epoch 671/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -412153820653285.1250 - accuracy: 0.6492\n",
            "Epoch 672/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -413864436335376.5000 - accuracy: 0.6492\n",
            "Epoch 673/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -415583346207282.0000 - accuracy: 0.6492\n",
            "Epoch 674/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -417307783851300.2500 - accuracy: 0.6492\n",
            "Epoch 675/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -419040070664071.3125 - accuracy: 0.6492\n",
            "Epoch 676/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -420776454925634.4375 - accuracy: 0.6492\n",
            "Epoch 677/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -422517218519481.2500 - accuracy: 0.6492\n",
            "Epoch 678/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -424256835268087.5000 - accuracy: 0.6492\n",
            "Epoch 679/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -426012436266887.3125 - accuracy: 0.6492\n",
            "Epoch 680/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -427762583359767.0625 - accuracy: 0.6492\n",
            "Epoch 681/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -429524809349116.2500 - accuracy: 0.6492\n",
            "Epoch 682/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -431293709213801.6250 - accuracy: 0.6492\n",
            "Epoch 683/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -433070399698849.6875 - accuracy: 0.6492\n",
            "Epoch 684/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -434842802049725.5625 - accuracy: 0.6492\n",
            "Epoch 685/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -436630488645209.5625 - accuracy: 0.6492\n",
            "Epoch 686/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -438410665141446.0625 - accuracy: 0.6492\n",
            "Epoch 687/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -440205461486771.1875 - accuracy: 0.6492\n",
            "Epoch 688/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -442006004259340.2500 - accuracy: 0.6492\n",
            "Epoch 689/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -443804224868148.3125 - accuracy: 0.6492\n",
            "Epoch 690/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -445596971017112.2500 - accuracy: 0.6492\n",
            "Epoch 691/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -447402709085959.0625 - accuracy: 0.6492\n",
            "Epoch 692/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -449197814654994.8750 - accuracy: 0.6492\n",
            "Epoch 693/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -451003182538857.6250 - accuracy: 0.6492\n",
            "Epoch 694/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -452805489025914.0625 - accuracy: 0.6492\n",
            "Epoch 695/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -454605371365538.1875 - accuracy: 0.6492\n",
            "Epoch 696/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -456425152073282.9375 - accuracy: 0.6492\n",
            "Epoch 697/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -458234593638215.1875 - accuracy: 0.6492\n",
            "Epoch 698/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -460060637736620.5625 - accuracy: 0.6492\n",
            "Epoch 699/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -461901536404859.0625 - accuracy: 0.6492\n",
            "Epoch 700/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -463748593439119.7500 - accuracy: 0.6492\n",
            "Epoch 701/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -465602842779753.6250 - accuracy: 0.6492\n",
            "Epoch 702/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -467462087941978.0000 - accuracy: 0.6492\n",
            "Epoch 703/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -469303955008774.1250 - accuracy: 0.6492\n",
            "Epoch 704/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -471146145784305.8750 - accuracy: 0.6492\n",
            "Epoch 705/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -472991972044762.2500 - accuracy: 0.6492\n",
            "Epoch 706/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -474848775870535.6875 - accuracy: 0.6492\n",
            "Epoch 707/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -476696734408839.8125 - accuracy: 0.6492\n",
            "Epoch 708/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -478563297691591.4375 - accuracy: 0.6492\n",
            "Epoch 709/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -480428196784780.5000 - accuracy: 0.6492\n",
            "Epoch 710/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -482311619821236.1250 - accuracy: 0.6492\n",
            "Epoch 711/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -484190132592564.5625 - accuracy: 0.6492\n",
            "Epoch 712/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -486077140681398.0000 - accuracy: 0.6492\n",
            "Epoch 713/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -487985897902992.7500 - accuracy: 0.6492\n",
            "Epoch 714/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -489885238799139.3125 - accuracy: 0.6492\n",
            "Epoch 715/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -491776370452679.9375 - accuracy: 0.6492\n",
            "Epoch 716/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -493699879386140.3125 - accuracy: 0.6492\n",
            "Epoch 717/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -495611730409981.1875 - accuracy: 0.6492\n",
            "Epoch 718/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -497539658593063.1250 - accuracy: 0.6492\n",
            "Epoch 719/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -499465192553681.3750 - accuracy: 0.6492\n",
            "Epoch 720/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -501400624465380.6250 - accuracy: 0.6492\n",
            "Epoch 721/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -503341690789178.9375 - accuracy: 0.6492\n",
            "Epoch 722/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -505281877440749.6250 - accuracy: 0.6492\n",
            "Epoch 723/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -507237914948391.1250 - accuracy: 0.6492\n",
            "Epoch 724/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -509194254027842.0000 - accuracy: 0.6492\n",
            "Epoch 725/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -511161756801608.6250 - accuracy: 0.6492\n",
            "Epoch 726/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -513150119740176.5000 - accuracy: 0.6492\n",
            "Epoch 727/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -515107190487669.8750 - accuracy: 0.6492\n",
            "Epoch 728/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -517082087154131.6875 - accuracy: 0.6492\n",
            "Epoch 729/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -519071398658591.1250 - accuracy: 0.6492\n",
            "Epoch 730/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -521064289309705.4375 - accuracy: 0.6492\n",
            "Epoch 731/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -523070738984107.6250 - accuracy: 0.6492\n",
            "Epoch 732/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -525086777491848.2500 - accuracy: 0.6492\n",
            "Epoch 733/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -527107849017144.0625 - accuracy: 0.6492\n",
            "Epoch 734/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -529133314514476.3125 - accuracy: 0.6492\n",
            "Epoch 735/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -531161037026268.1250 - accuracy: 0.6492\n",
            "Epoch 736/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -533205085230089.3750 - accuracy: 0.6492\n",
            "Epoch 737/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -535248605169818.6250 - accuracy: 0.6492\n",
            "Epoch 738/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -537309603926204.6250 - accuracy: 0.6492\n",
            "Epoch 739/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -539369852861920.9375 - accuracy: 0.6492\n",
            "Epoch 740/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -541428558359421.8750 - accuracy: 0.6492\n",
            "Epoch 741/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -543491510408201.3750 - accuracy: 0.6492\n",
            "Epoch 742/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -545556531635247.1250 - accuracy: 0.6492\n",
            "Epoch 743/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -547625555315747.8750 - accuracy: 0.6492\n",
            "Epoch 744/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -549701542290026.5625 - accuracy: 0.6492\n",
            "Epoch 745/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -551785470920164.6875 - accuracy: 0.6492\n",
            "Epoch 746/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -553880715662767.8125 - accuracy: 0.6492\n",
            "Epoch 747/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -555971781217858.9375 - accuracy: 0.6492\n",
            "Epoch 748/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -558059223473135.0000 - accuracy: 0.6492\n",
            "Epoch 749/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -560168086650125.6875 - accuracy: 0.6492\n",
            "Epoch 750/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -562274176236904.1875 - accuracy: 0.6492\n",
            "Epoch 751/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -564398188233367.7500 - accuracy: 0.6492\n",
            "Epoch 752/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -566532349456798.8750 - accuracy: 0.6492\n",
            "Epoch 753/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -568667722758773.8750 - accuracy: 0.6492\n",
            "Epoch 754/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -570807698297503.2500 - accuracy: 0.6492\n",
            "Epoch 755/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -572964038515785.5000 - accuracy: 0.6492\n",
            "Epoch 756/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -575123650963203.2500 - accuracy: 0.6492\n",
            "Epoch 757/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -577275772345055.3750 - accuracy: 0.6492\n",
            "Epoch 758/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -579423328609357.3750 - accuracy: 0.6492\n",
            "Epoch 759/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -581576758185358.0000 - accuracy: 0.6492\n",
            "Epoch 760/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -583747799137911.7500 - accuracy: 0.6492\n",
            "Epoch 761/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -585937929598067.0000 - accuracy: 0.6492\n",
            "Epoch 762/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -588111233715567.7500 - accuracy: 0.6492\n",
            "Epoch 763/1000\n",
            "1086/1086 [==============================] - 0s 142us/step - loss: -590296389792973.6250 - accuracy: 0.6492\n",
            "Epoch 764/1000\n",
            "1086/1086 [==============================] - 0s 146us/step - loss: -592479828418756.1250 - accuracy: 0.6492\n",
            "Epoch 765/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -594664959899040.8750 - accuracy: 0.6492\n",
            "Epoch 766/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -596867916725246.1250 - accuracy: 0.6492\n",
            "Epoch 767/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -599055601378984.7500 - accuracy: 0.6492\n",
            "Epoch 768/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -601246967532851.3750 - accuracy: 0.6492\n",
            "Epoch 769/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -603429251199481.3750 - accuracy: 0.6492\n",
            "Epoch 770/1000\n",
            "1086/1086 [==============================] - 0s 143us/step - loss: -605622834684130.3750 - accuracy: 0.6492\n",
            "Epoch 771/1000\n",
            "1086/1086 [==============================] - 0s 147us/step - loss: -607831290757512.2500 - accuracy: 0.6492\n",
            "Epoch 772/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -610044011839427.6250 - accuracy: 0.6492\n",
            "Epoch 773/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -612257575426572.2500 - accuracy: 0.6492\n",
            "Epoch 774/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -614477691300583.0000 - accuracy: 0.6492\n",
            "Epoch 775/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -616687308743334.8750 - accuracy: 0.6492\n",
            "Epoch 776/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -618915405824603.3750 - accuracy: 0.6492\n",
            "Epoch 777/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -621165610699419.5000 - accuracy: 0.6492\n",
            "Epoch 778/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -623412630472013.8750 - accuracy: 0.6492\n",
            "Epoch 779/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -625649772541375.0000 - accuracy: 0.6492\n",
            "Epoch 780/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -627899452984325.6250 - accuracy: 0.6492\n",
            "Epoch 781/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -630154800689928.8750 - accuracy: 0.6492\n",
            "Epoch 782/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -632412234161889.3750 - accuracy: 0.6492\n",
            "Epoch 783/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -634683351935900.0000 - accuracy: 0.6492\n",
            "Epoch 784/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -636964578955814.6250 - accuracy: 0.6492\n",
            "Epoch 785/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -639243262503051.6250 - accuracy: 0.6492\n",
            "Epoch 786/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -641526247274281.0000 - accuracy: 0.6492\n",
            "Epoch 787/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -643818968222363.5000 - accuracy: 0.6492\n",
            "Epoch 788/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -646119270160576.3750 - accuracy: 0.6492\n",
            "Epoch 789/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -648418101466121.3750 - accuracy: 0.6492\n",
            "Epoch 790/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -650740401184783.1250 - accuracy: 0.6492\n",
            "Epoch 791/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -653075767852030.1250 - accuracy: 0.6492\n",
            "Epoch 792/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -655428102859977.7500 - accuracy: 0.6492\n",
            "Epoch 793/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -657790649982417.8750 - accuracy: 0.6492\n",
            "Epoch 794/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -660139632289297.8750 - accuracy: 0.6492\n",
            "Epoch 795/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -662492711323712.1250 - accuracy: 0.6492\n",
            "Epoch 796/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -664833830891180.5000 - accuracy: 0.6492\n",
            "Epoch 797/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -667184080903271.7500 - accuracy: 0.6492\n",
            "Epoch 798/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -669563448598301.6250 - accuracy: 0.6492\n",
            "Epoch 799/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -671934302590021.7500 - accuracy: 0.6492\n",
            "Epoch 800/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -674304917377031.5000 - accuracy: 0.6492\n",
            "Epoch 801/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -676683618204760.6250 - accuracy: 0.6492\n",
            "Epoch 802/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -679073406661596.1250 - accuracy: 0.6492\n",
            "Epoch 803/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -681452795514495.2500 - accuracy: 0.6492\n",
            "Epoch 804/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -683819850968205.5000 - accuracy: 0.6492\n",
            "Epoch 805/1000\n",
            "1086/1086 [==============================] - 0s 144us/step - loss: -686216337224517.2500 - accuracy: 0.6492\n",
            "Epoch 806/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -688595829408498.2500 - accuracy: 0.6492\n",
            "Epoch 807/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -691029042454392.2500 - accuracy: 0.6492\n",
            "Epoch 808/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -693448308629398.3750 - accuracy: 0.6492\n",
            "Epoch 809/1000\n",
            "1086/1086 [==============================] - 0s 146us/step - loss: -695882199620228.8750 - accuracy: 0.6492\n",
            "Epoch 810/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -698303616076051.3750 - accuracy: 0.6492\n",
            "Epoch 811/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -700730169444868.7500 - accuracy: 0.6492\n",
            "Epoch 812/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -703176376231170.3750 - accuracy: 0.6492\n",
            "Epoch 813/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -705608030297523.6250 - accuracy: 0.6492\n",
            "Epoch 814/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -708060805075193.0000 - accuracy: 0.6492\n",
            "Epoch 815/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -710498318113050.8750 - accuracy: 0.6492\n",
            "Epoch 816/1000\n",
            "1086/1086 [==============================] - 0s 144us/step - loss: -712955106105838.1250 - accuracy: 0.6492\n",
            "Epoch 817/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -715396548434112.3750 - accuracy: 0.6492\n",
            "Epoch 818/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -717853986957270.5000 - accuracy: 0.6492\n",
            "Epoch 819/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -720308998863146.0000 - accuracy: 0.6492\n",
            "Epoch 820/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -722786746807803.2500 - accuracy: 0.6492\n",
            "Epoch 821/1000\n",
            "1086/1086 [==============================] - 0s 154us/step - loss: -725271521121198.8750 - accuracy: 0.6492\n",
            "Epoch 822/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -727746955301806.8750 - accuracy: 0.6492\n",
            "Epoch 823/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -730263612666301.1250 - accuracy: 0.6492\n",
            "Epoch 824/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -732762169591057.5000 - accuracy: 0.6492\n",
            "Epoch 825/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -735248859427834.3750 - accuracy: 0.6492\n",
            "Epoch 826/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -737757802714151.6250 - accuracy: 0.6492\n",
            "Epoch 827/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -740263173498174.7500 - accuracy: 0.6492\n",
            "Epoch 828/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -742789281913748.5000 - accuracy: 0.6492\n",
            "Epoch 829/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -745310322016950.0000 - accuracy: 0.6492\n",
            "Epoch 830/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -747847010041524.0000 - accuracy: 0.6492\n",
            "Epoch 831/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -750373766996043.5000 - accuracy: 0.6492\n",
            "Epoch 832/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -752904625010955.8750 - accuracy: 0.6492\n",
            "Epoch 833/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -755431854234546.6250 - accuracy: 0.6492\n",
            "Epoch 834/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -757970161466607.5000 - accuracy: 0.6492\n",
            "Epoch 835/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -760508739976673.0000 - accuracy: 0.6492\n",
            "Epoch 836/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -763049280452574.0000 - accuracy: 0.6492\n",
            "Epoch 837/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -765608986875915.3750 - accuracy: 0.6492\n",
            "Epoch 838/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -768176543646933.1250 - accuracy: 0.6492\n",
            "Epoch 839/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -770746276265290.0000 - accuracy: 0.6492\n",
            "Epoch 840/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -773335162947225.6250 - accuracy: 0.6492\n",
            "Epoch 841/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -775914525867157.0000 - accuracy: 0.6492\n",
            "Epoch 842/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -778521255593354.1250 - accuracy: 0.6492\n",
            "Epoch 843/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -781117377499897.8750 - accuracy: 0.6492\n",
            "Epoch 844/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -783731756358216.5000 - accuracy: 0.6492\n",
            "Epoch 845/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -786355777933017.7500 - accuracy: 0.6492\n",
            "Epoch 846/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -788958809829881.3750 - accuracy: 0.6492\n",
            "Epoch 847/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -791588637902972.5000 - accuracy: 0.6492\n",
            "Epoch 848/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -794210972931141.7500 - accuracy: 0.6492\n",
            "Epoch 849/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -796859752312221.0000 - accuracy: 0.6492\n",
            "Epoch 850/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -799518826728333.0000 - accuracy: 0.6492\n",
            "Epoch 851/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -802185538948856.0000 - accuracy: 0.6492\n",
            "Epoch 852/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -804850678961945.8750 - accuracy: 0.6492\n",
            "Epoch 853/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -807528505271222.5000 - accuracy: 0.6492\n",
            "Epoch 854/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -810215976346822.0000 - accuracy: 0.6492\n",
            "Epoch 855/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -812932536623873.3750 - accuracy: 0.6492\n",
            "Epoch 856/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -815663165416283.8750 - accuracy: 0.6492\n",
            "Epoch 857/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -818381298054289.2500 - accuracy: 0.6492\n",
            "Epoch 858/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -821087233457361.3750 - accuracy: 0.6492\n",
            "Epoch 859/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -823797595809267.7500 - accuracy: 0.6492\n",
            "Epoch 860/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -826513866927486.8750 - accuracy: 0.6492\n",
            "Epoch 861/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -829234213027840.0000 - accuracy: 0.6492\n",
            "Epoch 862/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -831963493649383.5000 - accuracy: 0.6492\n",
            "Epoch 863/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -834697207970331.2500 - accuracy: 0.6492\n",
            "Epoch 864/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -837439641349412.3750 - accuracy: 0.6492\n",
            "Epoch 865/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -840172067204116.7500 - accuracy: 0.6492\n",
            "Epoch 866/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -842938643475929.3750 - accuracy: 0.6492\n",
            "Epoch 867/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -845711249594650.8750 - accuracy: 0.6492\n",
            "Epoch 868/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -848465939293894.8750 - accuracy: 0.6492\n",
            "Epoch 869/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -851232493918309.8750 - accuracy: 0.6492\n",
            "Epoch 870/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -854013932546066.8750 - accuracy: 0.6492\n",
            "Epoch 871/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -856799530069555.7500 - accuracy: 0.6492\n",
            "Epoch 872/1000\n",
            "1086/1086 [==============================] - 0s 147us/step - loss: -859589903773688.5000 - accuracy: 0.6492\n",
            "Epoch 873/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -862375228045610.0000 - accuracy: 0.6492\n",
            "Epoch 874/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -865181296078327.5000 - accuracy: 0.6492\n",
            "Epoch 875/1000\n",
            "1086/1086 [==============================] - 0s 151us/step - loss: -867963167976529.1250 - accuracy: 0.6492\n",
            "Epoch 876/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -870758584867762.6250 - accuracy: 0.6492\n",
            "Epoch 877/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -873548594318221.0000 - accuracy: 0.6492\n",
            "Epoch 878/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -876361205789599.7500 - accuracy: 0.6492\n",
            "Epoch 879/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -879181861020719.1250 - accuracy: 0.6492\n",
            "Epoch 880/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -882028910993392.8750 - accuracy: 0.6492\n",
            "Epoch 881/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -884894859213791.8750 - accuracy: 0.6492\n",
            "Epoch 882/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -887751422159530.6250 - accuracy: 0.6492\n",
            "Epoch 883/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -890615447540654.8750 - accuracy: 0.6492\n",
            "Epoch 884/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -893464587081980.7500 - accuracy: 0.6492\n",
            "Epoch 885/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -896320372896279.5000 - accuracy: 0.6492\n",
            "Epoch 886/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -899196712963952.6250 - accuracy: 0.6492\n",
            "Epoch 887/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -902056144630174.8750 - accuracy: 0.6492\n",
            "Epoch 888/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -904946292986286.0000 - accuracy: 0.6492\n",
            "Epoch 889/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -907838799410230.7500 - accuracy: 0.6492\n",
            "Epoch 890/1000\n",
            "1086/1086 [==============================] - 0s 149us/step - loss: -910743198143980.2500 - accuracy: 0.6492\n",
            "Epoch 891/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -913660124703764.7500 - accuracy: 0.6492\n",
            "Epoch 892/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -916593142361320.0000 - accuracy: 0.6492\n",
            "Epoch 893/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -919525310784317.7500 - accuracy: 0.6492\n",
            "Epoch 894/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -922458311019676.5000 - accuracy: 0.6492\n",
            "Epoch 895/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -925385074815481.3750 - accuracy: 0.6492\n",
            "Epoch 896/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -928329475528292.8750 - accuracy: 0.6492\n",
            "Epoch 897/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -931289074326752.5000 - accuracy: 0.6492\n",
            "Epoch 898/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -934218236482358.2500 - accuracy: 0.6492\n",
            "Epoch 899/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -937182535683500.1250 - accuracy: 0.6492\n",
            "Epoch 900/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -940139393099251.7500 - accuracy: 0.6492\n",
            "Epoch 901/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -943111522747816.3750 - accuracy: 0.6492\n",
            "Epoch 902/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -946112599624890.7500 - accuracy: 0.6492\n",
            "Epoch 903/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -949129525985521.3750 - accuracy: 0.6492\n",
            "Epoch 904/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -952127557830237.2500 - accuracy: 0.6492\n",
            "Epoch 905/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -955137257665288.8750 - accuracy: 0.6492\n",
            "Epoch 906/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -958123464180164.6250 - accuracy: 0.6492\n",
            "Epoch 907/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -961143515391415.5000 - accuracy: 0.6492\n",
            "Epoch 908/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -964132912533813.2500 - accuracy: 0.6492\n",
            "Epoch 909/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -967160337441567.5000 - accuracy: 0.6492\n",
            "Epoch 910/1000\n",
            "1086/1086 [==============================] - 0s 147us/step - loss: -970179809215127.7500 - accuracy: 0.6492\n",
            "Epoch 911/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -973205685447580.0000 - accuracy: 0.6492\n",
            "Epoch 912/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -976223205879136.7500 - accuracy: 0.6492\n",
            "Epoch 913/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -979287642350790.0000 - accuracy: 0.6492\n",
            "Epoch 914/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -982325433166114.5000 - accuracy: 0.6492\n",
            "Epoch 915/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -985396833165327.1250 - accuracy: 0.6492\n",
            "Epoch 916/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -988422474234817.7500 - accuracy: 0.6492\n",
            "Epoch 917/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -991506026467094.1250 - accuracy: 0.6492\n",
            "Epoch 918/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -994596100732019.0000 - accuracy: 0.6492\n",
            "Epoch 919/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -997699884145224.5000 - accuracy: 0.6492\n",
            "Epoch 920/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1000750774451681.0000 - accuracy: 0.6492\n",
            "Epoch 921/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1003815666101872.1250 - accuracy: 0.6492\n",
            "Epoch 922/1000\n",
            "1086/1086 [==============================] - 0s 151us/step - loss: -1006849757187922.5000 - accuracy: 0.6492\n",
            "Epoch 923/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -1009924527777961.7500 - accuracy: 0.6492\n",
            "Epoch 924/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -1012999798755275.2500 - accuracy: 0.6492\n",
            "Epoch 925/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1016088536069385.8750 - accuracy: 0.6492\n",
            "Epoch 926/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1019169726809059.7500 - accuracy: 0.6492\n",
            "Epoch 927/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1022276781784213.0000 - accuracy: 0.6492\n",
            "Epoch 928/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1025385270016961.7500 - accuracy: 0.6492\n",
            "Epoch 929/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1028497391188748.7500 - accuracy: 0.6492\n",
            "Epoch 930/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1031626709812678.5000 - accuracy: 0.6492\n",
            "Epoch 931/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -1034760010321656.0000 - accuracy: 0.6492\n",
            "Epoch 932/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1037892510073888.1250 - accuracy: 0.6492\n",
            "Epoch 933/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1041056166605094.2500 - accuracy: 0.6492\n",
            "Epoch 934/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1044227550334229.2500 - accuracy: 0.6492\n",
            "Epoch 935/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1047407244646109.5000 - accuracy: 0.6492\n",
            "Epoch 936/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1050571561979096.8750 - accuracy: 0.6492\n",
            "Epoch 937/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1053766009052384.5000 - accuracy: 0.6492\n",
            "Epoch 938/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1056953780361740.2500 - accuracy: 0.6492\n",
            "Epoch 939/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -1060172438896519.2500 - accuracy: 0.6492\n",
            "Epoch 940/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1063352247823742.8750 - accuracy: 0.6492\n",
            "Epoch 941/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -1066555166142667.7500 - accuracy: 0.6492\n",
            "Epoch 942/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1069766291513389.2500 - accuracy: 0.6492\n",
            "Epoch 943/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1072971479017400.2500 - accuracy: 0.6492\n",
            "Epoch 944/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1076170743109794.2500 - accuracy: 0.6492\n",
            "Epoch 945/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -1079380723616079.7500 - accuracy: 0.6492\n",
            "Epoch 946/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -1082598193897969.8750 - accuracy: 0.6492\n",
            "Epoch 947/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -1085797878471270.6250 - accuracy: 0.6492\n",
            "Epoch 948/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1088995041186463.3750 - accuracy: 0.6492\n",
            "Epoch 949/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1092208435977862.7500 - accuracy: 0.6492\n",
            "Epoch 950/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1095437357549692.5000 - accuracy: 0.6492\n",
            "Epoch 951/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1098688386849895.6250 - accuracy: 0.6492\n",
            "Epoch 952/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1101936640901572.7500 - accuracy: 0.6492\n",
            "Epoch 953/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1105206855931191.1250 - accuracy: 0.6492\n",
            "Epoch 954/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1108475807268381.2500 - accuracy: 0.6492\n",
            "Epoch 955/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1111788057927453.7500 - accuracy: 0.6492\n",
            "Epoch 956/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1115067011913045.3750 - accuracy: 0.6492\n",
            "Epoch 957/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1118356027573370.6250 - accuracy: 0.6492\n",
            "Epoch 958/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1121655098803322.6250 - accuracy: 0.6492\n",
            "Epoch 959/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -1124939313980855.3750 - accuracy: 0.6492\n",
            "Epoch 960/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1128223160811308.7500 - accuracy: 0.6492\n",
            "Epoch 961/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1131543005491916.7500 - accuracy: 0.6492\n",
            "Epoch 962/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1134849707496186.0000 - accuracy: 0.6492\n",
            "Epoch 963/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1138185509225323.2500 - accuracy: 0.6492\n",
            "Epoch 964/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1141534492622667.0000 - accuracy: 0.6492\n",
            "Epoch 965/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1144890279570211.5000 - accuracy: 0.6492\n",
            "Epoch 966/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1148246399076627.2500 - accuracy: 0.6492\n",
            "Epoch 967/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1151599264726195.0000 - accuracy: 0.6492\n",
            "Epoch 968/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1154979338522631.5000 - accuracy: 0.6492\n",
            "Epoch 969/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1158348485960178.0000 - accuracy: 0.6492\n",
            "Epoch 970/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1161726718644473.0000 - accuracy: 0.6492\n",
            "Epoch 971/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1165105993887525.2500 - accuracy: 0.6492\n",
            "Epoch 972/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1168475638997007.0000 - accuracy: 0.6492\n",
            "Epoch 973/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1171869943786909.0000 - accuracy: 0.6492\n",
            "Epoch 974/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1175265993681488.2500 - accuracy: 0.6492\n",
            "Epoch 975/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1178689346390231.0000 - accuracy: 0.6492\n",
            "Epoch 976/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -1182133056682553.5000 - accuracy: 0.6492\n",
            "Epoch 977/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1185551609790524.2500 - accuracy: 0.6492\n",
            "Epoch 978/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -1188987480233482.2500 - accuracy: 0.6492\n",
            "Epoch 979/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1192424337063966.2500 - accuracy: 0.6492\n",
            "Epoch 980/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1195881848138139.2500 - accuracy: 0.6492\n",
            "Epoch 981/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1199328551719070.5000 - accuracy: 0.6492\n",
            "Epoch 982/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1202776234453705.0000 - accuracy: 0.6492\n",
            "Epoch 983/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1206248836531322.5000 - accuracy: 0.6492\n",
            "Epoch 984/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1209732743078929.0000 - accuracy: 0.6492\n",
            "Epoch 985/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1213208201925581.2500 - accuracy: 0.6492\n",
            "Epoch 986/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1216692696598803.2500 - accuracy: 0.6492\n",
            "Epoch 987/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1220178465243588.7500 - accuracy: 0.6492\n",
            "Epoch 988/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1223657303361026.7500 - accuracy: 0.6492\n",
            "Epoch 989/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -1227159087577144.5000 - accuracy: 0.6492\n",
            "Epoch 990/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1230643621470585.2500 - accuracy: 0.6492\n",
            "Epoch 991/1000\n",
            "1086/1086 [==============================] - 0s 115us/step - loss: -1234169417949651.7500 - accuracy: 0.6492\n",
            "Epoch 992/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1237701170473642.7500 - accuracy: 0.6492\n",
            "Epoch 993/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1241237552346740.0000 - accuracy: 0.6492\n",
            "Epoch 994/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -1244768095206036.0000 - accuracy: 0.6492\n",
            "Epoch 995/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1248314236560250.2500 - accuracy: 0.6492\n",
            "Epoch 996/1000\n",
            "1086/1086 [==============================] - 0s 114us/step - loss: -1251829942928476.2500 - accuracy: 0.6492\n",
            "Epoch 997/1000\n",
            "1086/1086 [==============================] - 0s 116us/step - loss: -1255372092176199.2500 - accuracy: 0.6492\n",
            "Epoch 998/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1258909849035612.0000 - accuracy: 0.6492\n",
            "Epoch 999/1000\n",
            "1086/1086 [==============================] - 0s 145us/step - loss: -1262464052962140.0000 - accuracy: 0.6492\n",
            "Epoch 1000/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1266010996932728.7500 - accuracy: 0.6492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff40f755e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIyBdITDe8zM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "5e88a376-b277-435c-96c7-28743ac2d49e"
      },
      "source": [
        "ANNPred = model.predict(X_test)\n",
        "\n",
        "ANN_Pred = []\n",
        "for i in ANN_Pred:\n",
        "  for k in i:\n",
        "    ANN_Pred.append(int(k))\n",
        "  \n",
        "ANNPred = np.array(ANN_Pred) \n",
        "\n",
        "cm = confusion_matrix(Y_test, ANNPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-a12d75daffe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mANNPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANN_Pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mANNPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [272, 0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni4o6y-a462j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "3d83d91d-5063-4896-8bc9-3f7a0b764680"
      },
      "source": [
        "print(\"Accuracy score %f\" % accuracy_score(Y_test, ANNPred))\n",
        "print(classification_report(Y_test, ANNPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-f064f7374f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy score %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mANNPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mANNPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [272, 0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZqDxRweOg5F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ac2b5b0-64cc-49be-a047-eafd6ce613f9"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_test, Y_test, batch_size = 10, nb_epoch = 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "272/272 [==============================] - 0s 477us/step - loss: -1363328660329171.0000 - accuracy: 0.6691\n",
            "Epoch 2/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1364295917718347.2500 - accuracy: 0.6691\n",
            "Epoch 3/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1365266155993208.5000 - accuracy: 0.6691\n",
            "Epoch 4/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1366234845644679.5000 - accuracy: 0.6691\n",
            "Epoch 5/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1367176756052931.7500 - accuracy: 0.6691\n",
            "Epoch 6/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1368146551612837.5000 - accuracy: 0.6691\n",
            "Epoch 7/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1369054576948404.7500 - accuracy: 0.6691\n",
            "Epoch 8/1000\n",
            "272/272 [==============================] - 0s 120us/step - loss: -1369952101103736.5000 - accuracy: 0.6691\n",
            "Epoch 9/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1370879943076201.2500 - accuracy: 0.6691\n",
            "Epoch 10/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1371883249567623.5000 - accuracy: 0.6691\n",
            "Epoch 11/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1372768361034932.7500 - accuracy: 0.6691\n",
            "Epoch 12/1000\n",
            "272/272 [==============================] - 0s 122us/step - loss: -1373787864895247.0000 - accuracy: 0.6691\n",
            "Epoch 13/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1374759159440805.5000 - accuracy: 0.6691\n",
            "Epoch 14/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1375693851960741.5000 - accuracy: 0.6691\n",
            "Epoch 15/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1376683982431653.5000 - accuracy: 0.6691\n",
            "Epoch 16/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1377582603150757.5000 - accuracy: 0.6691\n",
            "Epoch 17/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1378513547736244.7500 - accuracy: 0.6691\n",
            "Epoch 18/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1379471814974042.5000 - accuracy: 0.6691\n",
            "Epoch 19/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1380388668317214.2500 - accuracy: 0.6691\n",
            "Epoch 20/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1381348479290187.2500 - accuracy: 0.6691\n",
            "Epoch 21/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1382388211846927.0000 - accuracy: 0.6691\n",
            "Epoch 22/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1383315054033016.5000 - accuracy: 0.6691\n",
            "Epoch 23/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1384287155843313.0000 - accuracy: 0.6691\n",
            "Epoch 24/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1385248208006445.0000 - accuracy: 0.6691\n",
            "Epoch 25/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1386175318011181.0000 - accuracy: 0.6691\n",
            "Epoch 26/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1387130695466044.2500 - accuracy: 0.6691\n",
            "Epoch 27/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1388127823393009.0000 - accuracy: 0.6691\n",
            "Epoch 28/1000\n",
            "272/272 [==============================] - 0s 170us/step - loss: -1389151836438528.0000 - accuracy: 0.6691\n",
            "Epoch 29/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1390130534655397.5000 - accuracy: 0.6691\n",
            "Epoch 30/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1391089027799642.5000 - accuracy: 0.6691\n",
            "Epoch 31/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1391986470566972.2500 - accuracy: 0.6691\n",
            "Epoch 32/1000\n",
            "272/272 [==============================] - 0s 120us/step - loss: -1393007289989240.5000 - accuracy: 0.6691\n",
            "Epoch 33/1000\n",
            "272/272 [==============================] - 0s 121us/step - loss: -1393976707290774.7500 - accuracy: 0.6691\n",
            "Epoch 34/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1394931439624673.7500 - accuracy: 0.6691\n",
            "Epoch 35/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1396028988379859.0000 - accuracy: 0.6691\n",
            "Epoch 36/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1396942363395975.5000 - accuracy: 0.6691\n",
            "Epoch 37/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1397927469923388.2500 - accuracy: 0.6691\n",
            "Epoch 38/1000\n",
            "272/272 [==============================] - 0s 120us/step - loss: -1398902636659651.7500 - accuracy: 0.6691\n",
            "Epoch 39/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1399791205729219.7500 - accuracy: 0.6691\n",
            "Epoch 40/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1400801580316792.5000 - accuracy: 0.6691\n",
            "Epoch 41/1000\n",
            "272/272 [==============================] - 0s 120us/step - loss: -1401733980757534.2500 - accuracy: 0.6691\n",
            "Epoch 42/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1402677121207115.2500 - accuracy: 0.6691\n",
            "Epoch 43/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1403641276106631.5000 - accuracy: 0.6691\n",
            "Epoch 44/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1404652067071397.5000 - accuracy: 0.6691\n",
            "Epoch 45/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1405641468381063.5000 - accuracy: 0.6691\n",
            "Epoch 46/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1406550523942550.7500 - accuracy: 0.6691\n",
            "Epoch 47/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1407536075590475.2500 - accuracy: 0.6691\n",
            "Epoch 48/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1408380067281016.5000 - accuracy: 0.6691\n",
            "Epoch 49/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1409421350781891.7500 - accuracy: 0.6691\n",
            "Epoch 50/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1410337214716506.5000 - accuracy: 0.6691\n",
            "Epoch 51/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1411354971248278.7500 - accuracy: 0.6691\n",
            "Epoch 52/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1412359090339840.0000 - accuracy: 0.6691\n",
            "Epoch 53/1000\n",
            "272/272 [==============================] - 0s 119us/step - loss: -1413312199478091.2500 - accuracy: 0.6691\n",
            "Epoch 54/1000\n",
            "272/272 [==============================] - 0s 122us/step - loss: -1414336078460084.7500 - accuracy: 0.6691\n",
            "Epoch 55/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1415258562733477.5000 - accuracy: 0.6691\n",
            "Epoch 56/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1416224939473016.5000 - accuracy: 0.6691\n",
            "Epoch 57/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1417138368367435.2500 - accuracy: 0.6691\n",
            "Epoch 58/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1418108225793325.0000 - accuracy: 0.6691\n",
            "Epoch 59/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1419070097634484.7500 - accuracy: 0.6691\n",
            "Epoch 60/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1420037260651821.0000 - accuracy: 0.6691\n",
            "Epoch 61/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1420935781725364.7500 - accuracy: 0.6691\n",
            "Epoch 62/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1421944754937374.2500 - accuracy: 0.6691\n",
            "Epoch 63/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1422827125396179.0000 - accuracy: 0.6691\n",
            "Epoch 64/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1423810514818710.7500 - accuracy: 0.6691\n",
            "Epoch 65/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1424785156125876.7500 - accuracy: 0.6691\n",
            "Epoch 66/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1425737787267674.5000 - accuracy: 0.6691\n",
            "Epoch 67/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1426690433259158.7500 - accuracy: 0.6691\n",
            "Epoch 68/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1427628976628193.7500 - accuracy: 0.6691\n",
            "Epoch 69/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1428616169158776.5000 - accuracy: 0.6691\n",
            "Epoch 70/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1429579829685549.0000 - accuracy: 0.6691\n",
            "Epoch 71/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1430562657811516.2500 - accuracy: 0.6691\n",
            "Epoch 72/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1431541739036190.2500 - accuracy: 0.6691\n",
            "Epoch 73/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1432562249313581.0000 - accuracy: 0.6691\n",
            "Epoch 74/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1433470400940875.2500 - accuracy: 0.6691\n",
            "Epoch 75/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1434496346820367.0000 - accuracy: 0.6691\n",
            "Epoch 76/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1435413996387388.2500 - accuracy: 0.6691\n",
            "Epoch 77/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1436402446731143.5000 - accuracy: 0.6691\n",
            "Epoch 78/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1437300305659783.5000 - accuracy: 0.6691\n",
            "Epoch 79/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1438335904174803.0000 - accuracy: 0.6691\n",
            "Epoch 80/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1439334610455371.2500 - accuracy: 0.6691\n",
            "Epoch 81/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1440357677908931.7500 - accuracy: 0.6691\n",
            "Epoch 82/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1441309830907783.5000 - accuracy: 0.6691\n",
            "Epoch 83/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1442291210102181.5000 - accuracy: 0.6691\n",
            "Epoch 84/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1443250115090070.7500 - accuracy: 0.6691\n",
            "Epoch 85/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1444223112353430.7500 - accuracy: 0.6691\n",
            "Epoch 86/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1445160339358659.7500 - accuracy: 0.6691\n",
            "Epoch 87/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1446152734229443.7500 - accuracy: 0.6691\n",
            "Epoch 88/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1447124442145249.7500 - accuracy: 0.6691\n",
            "Epoch 89/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1448169467851956.7500 - accuracy: 0.6691\n",
            "Epoch 90/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1449095803822561.7500 - accuracy: 0.6691\n",
            "Epoch 91/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1450124395197620.7500 - accuracy: 0.6691\n",
            "Epoch 92/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1451114992130650.5000 - accuracy: 0.6691\n",
            "Epoch 93/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1452305196916254.2500 - accuracy: 0.6691\n",
            "Epoch 94/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1453307110235678.2500 - accuracy: 0.6691\n",
            "Epoch 95/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1454283009063032.5000 - accuracy: 0.6691\n",
            "Epoch 96/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1455290333805628.2500 - accuracy: 0.6691\n",
            "Epoch 97/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1456250679691143.5000 - accuracy: 0.6691\n",
            "Epoch 98/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1457204093352056.5000 - accuracy: 0.6691\n",
            "Epoch 99/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1458185011585505.7500 - accuracy: 0.6691\n",
            "Epoch 100/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1459096121739143.5000 - accuracy: 0.6691\n",
            "Epoch 101/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1460158908966309.5000 - accuracy: 0.6691\n",
            "Epoch 102/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1461127692773737.2500 - accuracy: 0.6691\n",
            "Epoch 103/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1462206485665430.7500 - accuracy: 0.6691\n",
            "Epoch 104/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1463115257109082.5000 - accuracy: 0.6691\n",
            "Epoch 105/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1464091103014189.0000 - accuracy: 0.6691\n",
            "Epoch 106/1000\n",
            "272/272 [==============================] - 0s 171us/step - loss: -1465031897922620.2500 - accuracy: 0.6691\n",
            "Epoch 107/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1466029002904275.0000 - accuracy: 0.6691\n",
            "Epoch 108/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1466968873523802.5000 - accuracy: 0.6691\n",
            "Epoch 109/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1467932817104414.2500 - accuracy: 0.6691\n",
            "Epoch 110/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1468938548165692.2500 - accuracy: 0.6691\n",
            "Epoch 111/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1469992160229496.5000 - accuracy: 0.6691\n",
            "Epoch 112/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1470992673206513.0000 - accuracy: 0.6691\n",
            "Epoch 113/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1471897359598531.7500 - accuracy: 0.6691\n",
            "Epoch 114/1000\n",
            "272/272 [==============================] - 0s 177us/step - loss: -1472913919489204.7500 - accuracy: 0.6691\n",
            "Epoch 115/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1473988886049972.7500 - accuracy: 0.6691\n",
            "Epoch 116/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1475044050345502.2500 - accuracy: 0.6691\n",
            "Epoch 117/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1476022469240229.5000 - accuracy: 0.6691\n",
            "Epoch 118/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1477093144985600.0000 - accuracy: 0.6691\n",
            "Epoch 119/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1478038731419889.0000 - accuracy: 0.6691\n",
            "Epoch 120/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1478990864114145.7500 - accuracy: 0.6691\n",
            "Epoch 121/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1479980714002311.5000 - accuracy: 0.6691\n",
            "Epoch 122/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1481092628850085.5000 - accuracy: 0.6691\n",
            "Epoch 123/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1482200675269692.2500 - accuracy: 0.6691\n",
            "Epoch 124/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1483109802072666.5000 - accuracy: 0.6691\n",
            "Epoch 125/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1484056782099155.0000 - accuracy: 0.6691\n",
            "Epoch 126/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1484992102729607.5000 - accuracy: 0.6691\n",
            "Epoch 127/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1485941023332954.5000 - accuracy: 0.6691\n",
            "Epoch 128/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1486861157778371.7500 - accuracy: 0.6691\n",
            "Epoch 129/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1487768068446810.5000 - accuracy: 0.6691\n",
            "Epoch 130/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1488725681342343.5000 - accuracy: 0.6691\n",
            "Epoch 131/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1489619954912075.2500 - accuracy: 0.6691\n",
            "Epoch 132/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1490582674403569.0000 - accuracy: 0.6691\n",
            "Epoch 133/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1491647361588766.2500 - accuracy: 0.6691\n",
            "Epoch 134/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1492648726163697.0000 - accuracy: 0.6691\n",
            "Epoch 135/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1493565830440357.5000 - accuracy: 0.6691\n",
            "Epoch 136/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1494433117580589.0000 - accuracy: 0.6691\n",
            "Epoch 137/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1495362024166099.0000 - accuracy: 0.6691\n",
            "Epoch 138/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1496249627481750.7500 - accuracy: 0.6691\n",
            "Epoch 139/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1497234551634040.5000 - accuracy: 0.6691\n",
            "Epoch 140/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1498213651208794.5000 - accuracy: 0.6691\n",
            "Epoch 141/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1499149019258398.2500 - accuracy: 0.6691\n",
            "Epoch 142/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1500075945800884.7500 - accuracy: 0.6691\n",
            "Epoch 143/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1500997461722051.7500 - accuracy: 0.6691\n",
            "Epoch 144/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1501881405461203.0000 - accuracy: 0.6691\n",
            "Epoch 145/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1502833555769223.5000 - accuracy: 0.6691\n",
            "Epoch 146/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1503754493393016.5000 - accuracy: 0.6691\n",
            "Epoch 147/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1504721083826176.0000 - accuracy: 0.6691\n",
            "Epoch 148/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1505657692819215.0000 - accuracy: 0.6691\n",
            "Epoch 149/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1506636147515873.7500 - accuracy: 0.6691\n",
            "Epoch 150/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1507599703065539.7500 - accuracy: 0.6691\n",
            "Epoch 151/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1508503735454539.2500 - accuracy: 0.6691\n",
            "Epoch 152/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1509533154711190.7500 - accuracy: 0.6691\n",
            "Epoch 153/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1510493551175077.5000 - accuracy: 0.6691\n",
            "Epoch 154/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1511420213044645.5000 - accuracy: 0.6691\n",
            "Epoch 155/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1512359217532687.0000 - accuracy: 0.6691\n",
            "Epoch 156/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1513303295532574.2500 - accuracy: 0.6691\n",
            "Epoch 157/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1514256921834676.7500 - accuracy: 0.6691\n",
            "Epoch 158/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1515157248525251.7500 - accuracy: 0.6691\n",
            "Epoch 159/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1516160930854068.7500 - accuracy: 0.6691\n",
            "Epoch 160/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1517111076284777.2500 - accuracy: 0.6691\n",
            "Epoch 161/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1518073251256922.5000 - accuracy: 0.6691\n",
            "Epoch 162/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1519037232308705.7500 - accuracy: 0.6691\n",
            "Epoch 163/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1519978414611998.2500 - accuracy: 0.6691\n",
            "Epoch 164/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1521031876074074.5000 - accuracy: 0.6691\n",
            "Epoch 165/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1522196036174787.7500 - accuracy: 0.6691\n",
            "Epoch 166/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -1523218880613195.2500 - accuracy: 0.6691\n",
            "Epoch 167/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1524170336270456.5000 - accuracy: 0.6691\n",
            "Epoch 168/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1525050012860416.0000 - accuracy: 0.6691\n",
            "Epoch 169/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1526032105780404.7500 - accuracy: 0.6691\n",
            "Epoch 170/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1527000935540133.5000 - accuracy: 0.6691\n",
            "Epoch 171/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1528016662738100.7500 - accuracy: 0.6691\n",
            "Epoch 172/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1528917857865487.0000 - accuracy: 0.6691\n",
            "Epoch 173/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1529901772192828.2500 - accuracy: 0.6691\n",
            "Epoch 174/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1530892119842334.2500 - accuracy: 0.6691\n",
            "Epoch 175/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1531789122331105.7500 - accuracy: 0.6691\n",
            "Epoch 176/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1532799747281619.0000 - accuracy: 0.6691\n",
            "Epoch 177/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1533781672148028.2500 - accuracy: 0.6691\n",
            "Epoch 178/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1534795251218552.5000 - accuracy: 0.6691\n",
            "Epoch 179/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1535878798184207.0000 - accuracy: 0.6691\n",
            "Epoch 180/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1536804577052551.5000 - accuracy: 0.6691\n",
            "Epoch 181/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1537801826922737.0000 - accuracy: 0.6691\n",
            "Epoch 182/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1538780419757718.7500 - accuracy: 0.6691\n",
            "Epoch 183/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1539766163264211.0000 - accuracy: 0.6691\n",
            "Epoch 184/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1540771797825656.5000 - accuracy: 0.6691\n",
            "Epoch 185/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1541788777485974.7500 - accuracy: 0.6691\n",
            "Epoch 186/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1542774943398972.2500 - accuracy: 0.6691\n",
            "Epoch 187/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1543774972566106.5000 - accuracy: 0.6691\n",
            "Epoch 188/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1544795696043670.7500 - accuracy: 0.6691\n",
            "Epoch 189/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1545774520768150.7500 - accuracy: 0.6691\n",
            "Epoch 190/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1546746544428694.7500 - accuracy: 0.6691\n",
            "Epoch 191/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1547722274281110.7500 - accuracy: 0.6691\n",
            "Epoch 192/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1548694483138680.5000 - accuracy: 0.6691\n",
            "Epoch 193/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1549668981376903.5000 - accuracy: 0.6691\n",
            "Epoch 194/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1550722524604777.2500 - accuracy: 0.6691\n",
            "Epoch 195/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1551708097655988.7500 - accuracy: 0.6691\n",
            "Epoch 196/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1552685391891275.2500 - accuracy: 0.6691\n",
            "Epoch 197/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1553686224915275.2500 - accuracy: 0.6691\n",
            "Epoch 198/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1554672370936169.2500 - accuracy: 0.6691\n",
            "Epoch 199/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1555619531194368.0000 - accuracy: 0.6691\n",
            "Epoch 200/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1556676840660510.2500 - accuracy: 0.6691\n",
            "Epoch 201/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1557651804959924.7500 - accuracy: 0.6691\n",
            "Epoch 202/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1558708541348442.5000 - accuracy: 0.6691\n",
            "Epoch 203/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1559680676034680.5000 - accuracy: 0.6691\n",
            "Epoch 204/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1560752790877364.7500 - accuracy: 0.6691\n",
            "Epoch 205/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1561733255104391.5000 - accuracy: 0.6691\n",
            "Epoch 206/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1562787736684423.5000 - accuracy: 0.6691\n",
            "Epoch 207/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1563802765284051.0000 - accuracy: 0.6691\n",
            "Epoch 208/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1564816713626804.7500 - accuracy: 0.6691\n",
            "Epoch 209/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1565768743263774.2500 - accuracy: 0.6691\n",
            "Epoch 210/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1566806859409769.2500 - accuracy: 0.6691\n",
            "Epoch 211/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1567790130960022.7500 - accuracy: 0.6691\n",
            "Epoch 212/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1568793096587143.5000 - accuracy: 0.6691\n",
            "Epoch 213/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1569772391594104.5000 - accuracy: 0.6691\n",
            "Epoch 214/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1570848042832595.0000 - accuracy: 0.6691\n",
            "Epoch 215/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1571823778991887.0000 - accuracy: 0.6691\n",
            "Epoch 216/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1572855784715444.7500 - accuracy: 0.6691\n",
            "Epoch 217/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1573868623024851.0000 - accuracy: 0.6691\n",
            "Epoch 218/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1574947765385336.5000 - accuracy: 0.6691\n",
            "Epoch 219/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1575975290543646.2500 - accuracy: 0.6691\n",
            "Epoch 220/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1576967654003411.0000 - accuracy: 0.6691\n",
            "Epoch 221/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1578000834856839.5000 - accuracy: 0.6691\n",
            "Epoch 222/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1579031415380570.5000 - accuracy: 0.6691\n",
            "Epoch 223/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1580063050617555.0000 - accuracy: 0.6691\n",
            "Epoch 224/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1581069391795862.7500 - accuracy: 0.6691\n",
            "Epoch 225/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1582166234982761.2500 - accuracy: 0.6691\n",
            "Epoch 226/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1583174754612404.7500 - accuracy: 0.6691\n",
            "Epoch 227/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1584209507616647.5000 - accuracy: 0.6691\n",
            "Epoch 228/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1585267607354428.2500 - accuracy: 0.6691\n",
            "Epoch 229/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1586247515134554.5000 - accuracy: 0.6691\n",
            "Epoch 230/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1587344887297204.7500 - accuracy: 0.6691\n",
            "Epoch 231/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1588392202654419.0000 - accuracy: 0.6691\n",
            "Epoch 232/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1589401367748126.2500 - accuracy: 0.6691\n",
            "Epoch 233/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1590425023599315.0000 - accuracy: 0.6691\n",
            "Epoch 234/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1591498220287156.7500 - accuracy: 0.6691\n",
            "Epoch 235/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -1592513009071284.7500 - accuracy: 0.6691\n",
            "Epoch 236/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1593585417846784.0000 - accuracy: 0.6691\n",
            "Epoch 237/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1594574418816301.0000 - accuracy: 0.6691\n",
            "Epoch 238/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1595604643811087.0000 - accuracy: 0.6691\n",
            "Epoch 239/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1596663991338887.5000 - accuracy: 0.6691\n",
            "Epoch 240/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1597604496285214.2500 - accuracy: 0.6691\n",
            "Epoch 241/1000\n",
            "272/272 [==============================] - 0s 221us/step - loss: -1598704130658063.0000 - accuracy: 0.6691\n",
            "Epoch 242/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1599735456534287.0000 - accuracy: 0.6691\n",
            "Epoch 243/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1600946640704211.0000 - accuracy: 0.6691\n",
            "Epoch 244/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1602043449719868.2500 - accuracy: 0.6691\n",
            "Epoch 245/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1603069176632019.0000 - accuracy: 0.6691\n",
            "Epoch 246/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1604080075692634.5000 - accuracy: 0.6691\n",
            "Epoch 247/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1605020791294795.2500 - accuracy: 0.6691\n",
            "Epoch 248/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1606059488860762.5000 - accuracy: 0.6691\n",
            "Epoch 249/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1607053956704617.2500 - accuracy: 0.6691\n",
            "Epoch 250/1000\n",
            "272/272 [==============================] - 0s 169us/step - loss: -1608152150889171.0000 - accuracy: 0.6691\n",
            "Epoch 251/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1609171603924389.5000 - accuracy: 0.6691\n",
            "Epoch 252/1000\n",
            "272/272 [==============================] - 0s 178us/step - loss: -1610195565065396.7500 - accuracy: 0.6691\n",
            "Epoch 253/1000\n",
            "272/272 [==============================] - 0s 174us/step - loss: -1611277128194650.5000 - accuracy: 0.6691\n",
            "Epoch 254/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1612280434377667.7500 - accuracy: 0.6691\n",
            "Epoch 255/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -1613313537659603.0000 - accuracy: 0.6691\n",
            "Epoch 256/1000\n",
            "272/272 [==============================] - 0s 170us/step - loss: -1614335347986191.0000 - accuracy: 0.6691\n",
            "Epoch 257/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1615477249840790.7500 - accuracy: 0.6691\n",
            "Epoch 258/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -1616510913825611.2500 - accuracy: 0.6691\n",
            "Epoch 259/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -1617553870691870.2500 - accuracy: 0.6691\n",
            "Epoch 260/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1618629145842387.0000 - accuracy: 0.6691\n",
            "Epoch 261/1000\n",
            "272/272 [==============================] - 0s 171us/step - loss: -1619641326324555.2500 - accuracy: 0.6691\n",
            "Epoch 262/1000\n",
            "272/272 [==============================] - 0s 170us/step - loss: -1620688708366577.0000 - accuracy: 0.6691\n",
            "Epoch 263/1000\n",
            "272/272 [==============================] - 0s 181us/step - loss: -1621780460840357.5000 - accuracy: 0.6691\n",
            "Epoch 264/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1622845732020705.7500 - accuracy: 0.6691\n",
            "Epoch 265/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1623936672341534.2500 - accuracy: 0.6691\n",
            "Epoch 266/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1624971320785016.5000 - accuracy: 0.6691\n",
            "Epoch 267/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1626061576385716.7500 - accuracy: 0.6691\n",
            "Epoch 268/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1627087160876574.2500 - accuracy: 0.6691\n",
            "Epoch 269/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1628087466297223.5000 - accuracy: 0.6691\n",
            "Epoch 270/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1629143869762861.0000 - accuracy: 0.6691\n",
            "Epoch 271/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1630168740481867.2500 - accuracy: 0.6691\n",
            "Epoch 272/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1631164493232248.5000 - accuracy: 0.6691\n",
            "Epoch 273/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1632263986355742.2500 - accuracy: 0.6691\n",
            "Epoch 274/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1633321465722036.7500 - accuracy: 0.6691\n",
            "Epoch 275/1000\n",
            "272/272 [==============================] - 0s 176us/step - loss: -1634357643297731.7500 - accuracy: 0.6691\n",
            "Epoch 276/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1635381868278964.7500 - accuracy: 0.6691\n",
            "Epoch 277/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1636398458886746.5000 - accuracy: 0.6691\n",
            "Epoch 278/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1637467878603836.2500 - accuracy: 0.6691\n",
            "Epoch 279/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1638503732636009.2500 - accuracy: 0.6691\n",
            "Epoch 280/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1639558576433513.2500 - accuracy: 0.6691\n",
            "Epoch 281/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1640662970477869.0000 - accuracy: 0.6691\n",
            "Epoch 282/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1641797277064734.2500 - accuracy: 0.6691\n",
            "Epoch 283/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1642894073759623.5000 - accuracy: 0.6691\n",
            "Epoch 284/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1643901575110174.2500 - accuracy: 0.6691\n",
            "Epoch 285/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1644979672533835.2500 - accuracy: 0.6691\n",
            "Epoch 286/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1646049239174927.0000 - accuracy: 0.6691\n",
            "Epoch 287/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1647089343050932.7500 - accuracy: 0.6691\n",
            "Epoch 288/1000\n",
            "272/272 [==============================] - 0s 176us/step - loss: -1648164501316065.7500 - accuracy: 0.6691\n",
            "Epoch 289/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1649223465866902.7500 - accuracy: 0.6691\n",
            "Epoch 290/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1650269062924167.5000 - accuracy: 0.6691\n",
            "Epoch 291/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1651388834212201.2500 - accuracy: 0.6691\n",
            "Epoch 292/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1652425828443557.5000 - accuracy: 0.6691\n",
            "Epoch 293/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1653478650004419.7500 - accuracy: 0.6691\n",
            "Epoch 294/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1654567523335228.2500 - accuracy: 0.6691\n",
            "Epoch 295/1000\n",
            "272/272 [==============================] - 0s 179us/step - loss: -1655570089639936.0000 - accuracy: 0.6691\n",
            "Epoch 296/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1656673767260160.0000 - accuracy: 0.6691\n",
            "Epoch 297/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1657689027687604.7500 - accuracy: 0.6691\n",
            "Epoch 298/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1658768539532348.2500 - accuracy: 0.6691\n",
            "Epoch 299/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1659799536448692.7500 - accuracy: 0.6691\n",
            "Epoch 300/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1660909737260212.7500 - accuracy: 0.6691\n",
            "Epoch 301/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1661958856607623.5000 - accuracy: 0.6691\n",
            "Epoch 302/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1663033569582622.2500 - accuracy: 0.6691\n",
            "Epoch 303/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1664067710160655.0000 - accuracy: 0.6691\n",
            "Epoch 304/1000\n",
            "272/272 [==============================] - 0s 177us/step - loss: -1665131304051169.7500 - accuracy: 0.6691\n",
            "Epoch 305/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1666243242599845.5000 - accuracy: 0.6691\n",
            "Epoch 306/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1667399218410676.7500 - accuracy: 0.6691\n",
            "Epoch 307/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1668479785230817.7500 - accuracy: 0.6691\n",
            "Epoch 308/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1669555016557025.7500 - accuracy: 0.6691\n",
            "Epoch 309/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1670646773533515.2500 - accuracy: 0.6691\n",
            "Epoch 310/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1671668433512809.2500 - accuracy: 0.6691\n",
            "Epoch 311/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1672746034466575.0000 - accuracy: 0.6691\n",
            "Epoch 312/1000\n",
            "272/272 [==============================] - 0s 197us/step - loss: -1673838117365157.5000 - accuracy: 0.6691\n",
            "Epoch 313/1000\n",
            "272/272 [==============================] - 0s 187us/step - loss: -1674955134845891.7500 - accuracy: 0.6691\n",
            "Epoch 314/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1676073348505118.2500 - accuracy: 0.6691\n",
            "Epoch 315/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1677107856747821.0000 - accuracy: 0.6691\n",
            "Epoch 316/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1678206102312598.7500 - accuracy: 0.6691\n",
            "Epoch 317/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1679252002223284.7500 - accuracy: 0.6691\n",
            "Epoch 318/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1680346555690285.0000 - accuracy: 0.6691\n",
            "Epoch 319/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1681410002379233.7500 - accuracy: 0.6691\n",
            "Epoch 320/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1682534795513615.0000 - accuracy: 0.6691\n",
            "Epoch 321/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1683626252967454.2500 - accuracy: 0.6691\n",
            "Epoch 322/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1684768375516461.0000 - accuracy: 0.6691\n",
            "Epoch 323/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1685789738502023.5000 - accuracy: 0.6691\n",
            "Epoch 324/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -1686835446800865.7500 - accuracy: 0.6691\n",
            "Epoch 325/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -1687918813473129.2500 - accuracy: 0.6691\n",
            "Epoch 326/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1688953803999111.5000 - accuracy: 0.6691\n",
            "Epoch 327/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1690008771933364.7500 - accuracy: 0.6691\n",
            "Epoch 328/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1691076274051674.5000 - accuracy: 0.6691\n",
            "Epoch 329/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1692164603664986.5000 - accuracy: 0.6691\n",
            "Epoch 330/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1693204079999096.5000 - accuracy: 0.6691\n",
            "Epoch 331/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1694291283688508.2500 - accuracy: 0.6691\n",
            "Epoch 332/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1695386819239454.2500 - accuracy: 0.6691\n",
            "Epoch 333/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1696458508691817.2500 - accuracy: 0.6691\n",
            "Epoch 334/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1697602666650202.5000 - accuracy: 0.6691\n",
            "Epoch 335/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1698721029083859.0000 - accuracy: 0.6691\n",
            "Epoch 336/1000\n",
            "272/272 [==============================] - 0s 185us/step - loss: -1699837902231190.7500 - accuracy: 0.6691\n",
            "Epoch 337/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1700885340217825.7500 - accuracy: 0.6691\n",
            "Epoch 338/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1701979297796819.0000 - accuracy: 0.6691\n",
            "Epoch 339/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1703025556347482.5000 - accuracy: 0.6691\n",
            "Epoch 340/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1704101283179821.0000 - accuracy: 0.6691\n",
            "Epoch 341/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1705181342273295.0000 - accuracy: 0.6691\n",
            "Epoch 342/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1706261678961844.7500 - accuracy: 0.6691\n",
            "Epoch 343/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1707401558483425.7500 - accuracy: 0.6691\n",
            "Epoch 344/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1708411485205684.7500 - accuracy: 0.6691\n",
            "Epoch 345/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1709546877438795.2500 - accuracy: 0.6691\n",
            "Epoch 346/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1710554447393972.7500 - accuracy: 0.6691\n",
            "Epoch 347/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1711660802830577.0000 - accuracy: 0.6691\n",
            "Epoch 348/1000\n",
            "272/272 [==============================] - 0s 175us/step - loss: -1712810554124649.2500 - accuracy: 0.6691\n",
            "Epoch 349/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1713854623483783.5000 - accuracy: 0.6691\n",
            "Epoch 350/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1715129077680971.2500 - accuracy: 0.6691\n",
            "Epoch 351/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1716485429410876.2500 - accuracy: 0.6691\n",
            "Epoch 352/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1717557832157063.5000 - accuracy: 0.6691\n",
            "Epoch 353/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1718659164421180.2500 - accuracy: 0.6691\n",
            "Epoch 354/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1719696381417110.7500 - accuracy: 0.6691\n",
            "Epoch 355/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1720719103507516.2500 - accuracy: 0.6691\n",
            "Epoch 356/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1721792945501364.7500 - accuracy: 0.6691\n",
            "Epoch 357/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1722779488115290.5000 - accuracy: 0.6691\n",
            "Epoch 358/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1723867456046983.5000 - accuracy: 0.6691\n",
            "Epoch 359/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1724936057304244.7500 - accuracy: 0.6691\n",
            "Epoch 360/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1725944546860574.2500 - accuracy: 0.6691\n",
            "Epoch 361/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1726973524127021.0000 - accuracy: 0.6691\n",
            "Epoch 362/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1727964866613007.0000 - accuracy: 0.6691\n",
            "Epoch 363/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1729040820932367.0000 - accuracy: 0.6691\n",
            "Epoch 364/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1730037114624602.5000 - accuracy: 0.6691\n",
            "Epoch 365/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1731090257296444.2500 - accuracy: 0.6691\n",
            "Epoch 366/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1732156484099614.2500 - accuracy: 0.6691\n",
            "Epoch 367/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1733152247798362.5000 - accuracy: 0.6691\n",
            "Epoch 368/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1734219175821312.0000 - accuracy: 0.6691\n",
            "Epoch 369/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1735321790247153.0000 - accuracy: 0.6691\n",
            "Epoch 370/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1736376119968707.7500 - accuracy: 0.6691\n",
            "Epoch 371/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1737406047969761.7500 - accuracy: 0.6691\n",
            "Epoch 372/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1738561787265024.0000 - accuracy: 0.6691\n",
            "Epoch 373/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1739600759665844.7500 - accuracy: 0.6691\n",
            "Epoch 374/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1740731096220611.7500 - accuracy: 0.6691\n",
            "Epoch 375/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1741790805630494.2500 - accuracy: 0.6691\n",
            "Epoch 376/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1742855432861816.5000 - accuracy: 0.6691\n",
            "Epoch 377/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1743875041024602.5000 - accuracy: 0.6691\n",
            "Epoch 378/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1744934832901903.0000 - accuracy: 0.6691\n",
            "Epoch 379/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1745970864663853.0000 - accuracy: 0.6691\n",
            "Epoch 380/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1747127902651331.7500 - accuracy: 0.6691\n",
            "Epoch 381/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1748212982665939.0000 - accuracy: 0.6691\n",
            "Epoch 382/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -1749287525663683.7500 - accuracy: 0.6691\n",
            "Epoch 383/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1750330148589327.0000 - accuracy: 0.6691\n",
            "Epoch 384/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1751414972905592.5000 - accuracy: 0.6691\n",
            "Epoch 385/1000\n",
            "272/272 [==============================] - 0s 188us/step - loss: -1752513415664339.0000 - accuracy: 0.6691\n",
            "Epoch 386/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1753648242938579.0000 - accuracy: 0.6691\n",
            "Epoch 387/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1754667662473336.5000 - accuracy: 0.6691\n",
            "Epoch 388/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1755718937878046.2500 - accuracy: 0.6691\n",
            "Epoch 389/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1756791984311115.2500 - accuracy: 0.6691\n",
            "Epoch 390/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1757892052177619.0000 - accuracy: 0.6691\n",
            "Epoch 391/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1758930231238174.2500 - accuracy: 0.6691\n",
            "Epoch 392/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1760027693640041.2500 - accuracy: 0.6691\n",
            "Epoch 393/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1761107127442974.2500 - accuracy: 0.6691\n",
            "Epoch 394/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1762276169065893.5000 - accuracy: 0.6691\n",
            "Epoch 395/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1763370243857950.2500 - accuracy: 0.6691\n",
            "Epoch 396/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1764476718400451.7500 - accuracy: 0.6691\n",
            "Epoch 397/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1765608211788980.7500 - accuracy: 0.6691\n",
            "Epoch 398/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1766693097315870.2500 - accuracy: 0.6691\n",
            "Epoch 399/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1767771794521148.2500 - accuracy: 0.6691\n",
            "Epoch 400/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1768891639706804.7500 - accuracy: 0.6691\n",
            "Epoch 401/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1769926573733044.7500 - accuracy: 0.6691\n",
            "Epoch 402/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1771025002736941.0000 - accuracy: 0.6691\n",
            "Epoch 403/1000\n",
            "272/272 [==============================] - 0s 173us/step - loss: -1772076572791507.0000 - accuracy: 0.6691\n",
            "Epoch 404/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1773219160430230.7500 - accuracy: 0.6691\n",
            "Epoch 405/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1774395440419539.0000 - accuracy: 0.6691\n",
            "Epoch 406/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1775450231236969.2500 - accuracy: 0.6691\n",
            "Epoch 407/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1776599182914740.7500 - accuracy: 0.6691\n",
            "Epoch 408/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1777749284494878.2500 - accuracy: 0.6691\n",
            "Epoch 409/1000\n",
            "272/272 [==============================] - 0s 192us/step - loss: -1778800328858202.5000 - accuracy: 0.6691\n",
            "Epoch 410/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1779923032859105.7500 - accuracy: 0.6691\n",
            "Epoch 411/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1781013463232753.0000 - accuracy: 0.6691\n",
            "Epoch 412/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1782140251282974.2500 - accuracy: 0.6691\n",
            "Epoch 413/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1783157739934419.0000 - accuracy: 0.6691\n",
            "Epoch 414/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1784357424525553.0000 - accuracy: 0.6691\n",
            "Epoch 415/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1785435800068577.7500 - accuracy: 0.6691\n",
            "Epoch 416/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -1786566902453910.7500 - accuracy: 0.6691\n",
            "Epoch 417/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1787646474052065.7500 - accuracy: 0.6691\n",
            "Epoch 418/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1788742969666861.0000 - accuracy: 0.6691\n",
            "Epoch 419/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1789874592269251.7500 - accuracy: 0.6691\n",
            "Epoch 420/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1790969025427576.5000 - accuracy: 0.6691\n",
            "Epoch 421/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1792104983737524.7500 - accuracy: 0.6691\n",
            "Epoch 422/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1793266043692453.5000 - accuracy: 0.6691\n",
            "Epoch 423/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1794461579962729.2500 - accuracy: 0.6691\n",
            "Epoch 424/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1795673826463503.0000 - accuracy: 0.6691\n",
            "Epoch 425/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1796811269156141.0000 - accuracy: 0.6691\n",
            "Epoch 426/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1797940570828077.0000 - accuracy: 0.6691\n",
            "Epoch 427/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1799189869563663.0000 - accuracy: 0.6691\n",
            "Epoch 428/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1800452085398588.2500 - accuracy: 0.6691\n",
            "Epoch 429/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1801576788324593.0000 - accuracy: 0.6691\n",
            "Epoch 430/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1802642167446588.2500 - accuracy: 0.6691\n",
            "Epoch 431/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -1803724252851501.0000 - accuracy: 0.6691\n",
            "Epoch 432/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -1804803974334343.5000 - accuracy: 0.6691\n",
            "Epoch 433/1000\n",
            "272/272 [==============================] - 0s 169us/step - loss: -1805896855677289.2500 - accuracy: 0.6691\n",
            "Epoch 434/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -1807018585350625.7500 - accuracy: 0.6691\n",
            "Epoch 435/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1808163426548796.2500 - accuracy: 0.6691\n",
            "Epoch 436/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -1809334306232922.5000 - accuracy: 0.6691\n",
            "Epoch 437/1000\n",
            "272/272 [==============================] - 0s 181us/step - loss: -1810442300223729.0000 - accuracy: 0.6691\n",
            "Epoch 438/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1811556469209208.5000 - accuracy: 0.6691\n",
            "Epoch 439/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1812718254786439.5000 - accuracy: 0.6691\n",
            "Epoch 440/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1813814259605985.7500 - accuracy: 0.6691\n",
            "Epoch 441/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1814921527704636.2500 - accuracy: 0.6691\n",
            "Epoch 442/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1816094064046321.0000 - accuracy: 0.6691\n",
            "Epoch 443/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1817192050612826.5000 - accuracy: 0.6691\n",
            "Epoch 444/1000\n",
            "272/272 [==============================] - 0s 176us/step - loss: -1818370968803930.5000 - accuracy: 0.6691\n",
            "Epoch 445/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1819415199127190.7500 - accuracy: 0.6691\n",
            "Epoch 446/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1820609363065916.2500 - accuracy: 0.6691\n",
            "Epoch 447/1000\n",
            "272/272 [==============================] - 0s 176us/step - loss: -1821626765919171.7500 - accuracy: 0.6691\n",
            "Epoch 448/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -1822903917763162.5000 - accuracy: 0.6691\n",
            "Epoch 449/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1824023311270851.7500 - accuracy: 0.6691\n",
            "Epoch 450/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1825112334779331.7500 - accuracy: 0.6691\n",
            "Epoch 451/1000\n",
            "272/272 [==============================] - 0s 165us/step - loss: -1826214768480015.0000 - accuracy: 0.6691\n",
            "Epoch 452/1000\n",
            "272/272 [==============================] - 0s 193us/step - loss: -1827374000581933.0000 - accuracy: 0.6691\n",
            "Epoch 453/1000\n",
            "272/272 [==============================] - 0s 183us/step - loss: -1828515996129882.5000 - accuracy: 0.6691\n",
            "Epoch 454/1000\n",
            "272/272 [==============================] - 0s 181us/step - loss: -1829713220977603.7500 - accuracy: 0.6691\n",
            "Epoch 455/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -1830882276262851.7500 - accuracy: 0.6691\n",
            "Epoch 456/1000\n",
            "272/272 [==============================] - 0s 169us/step - loss: -1831999387868702.2500 - accuracy: 0.6691\n",
            "Epoch 457/1000\n",
            "272/272 [==============================] - 0s 190us/step - loss: -1833185819785336.5000 - accuracy: 0.6691\n",
            "Epoch 458/1000\n",
            "272/272 [==============================] - 0s 185us/step - loss: -1834208169511755.2500 - accuracy: 0.6691\n",
            "Epoch 459/1000\n",
            "272/272 [==============================] - 0s 237us/step - loss: -1835428675984926.2500 - accuracy: 0.6691\n",
            "Epoch 460/1000\n",
            "272/272 [==============================] - 0s 198us/step - loss: -1836489175897750.7500 - accuracy: 0.6691\n",
            "Epoch 461/1000\n",
            "272/272 [==============================] - 0s 194us/step - loss: -1837615357315915.2500 - accuracy: 0.6691\n",
            "Epoch 462/1000\n",
            "272/272 [==============================] - 0s 185us/step - loss: -1838713190173515.2500 - accuracy: 0.6691\n",
            "Epoch 463/1000\n",
            "272/272 [==============================] - 0s 195us/step - loss: -1839809169056225.7500 - accuracy: 0.6691\n",
            "Epoch 464/1000\n",
            "272/272 [==============================] - 0s 188us/step - loss: -1840946794355290.5000 - accuracy: 0.6691\n",
            "Epoch 465/1000\n",
            "272/272 [==============================] - 0s 175us/step - loss: -1842077872931779.7500 - accuracy: 0.6691\n",
            "Epoch 466/1000\n",
            "272/272 [==============================] - 0s 190us/step - loss: -1843162788813402.5000 - accuracy: 0.6691\n",
            "Epoch 467/1000\n",
            "272/272 [==============================] - 0s 169us/step - loss: -1844353861233965.0000 - accuracy: 0.6691\n",
            "Epoch 468/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -1845396390466259.0000 - accuracy: 0.6691\n",
            "Epoch 469/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1846551820061515.2500 - accuracy: 0.6691\n",
            "Epoch 470/1000\n",
            "272/272 [==============================] - 0s 185us/step - loss: -1847714335224049.0000 - accuracy: 0.6691\n",
            "Epoch 471/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -1848772078584771.7500 - accuracy: 0.6691\n",
            "Epoch 472/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -1849918673495461.5000 - accuracy: 0.6691\n",
            "Epoch 473/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1851162930215032.5000 - accuracy: 0.6691\n",
            "Epoch 474/1000\n",
            "272/272 [==============================] - 0s 186us/step - loss: -1852326824440049.0000 - accuracy: 0.6691\n",
            "Epoch 475/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1853492924190479.0000 - accuracy: 0.6691\n",
            "Epoch 476/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1854719887085327.0000 - accuracy: 0.6691\n",
            "Epoch 477/1000\n",
            "272/272 [==============================] - 0s 179us/step - loss: -1855925341620103.5000 - accuracy: 0.6691\n",
            "Epoch 478/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1857063770405948.2500 - accuracy: 0.6691\n",
            "Epoch 479/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1858127600596148.7500 - accuracy: 0.6691\n",
            "Epoch 480/1000\n",
            "272/272 [==============================] - 0s 171us/step - loss: -1859320055163241.2500 - accuracy: 0.6691\n",
            "Epoch 481/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -1860425147312489.2500 - accuracy: 0.6691\n",
            "Epoch 482/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -1861608377217265.0000 - accuracy: 0.6691\n",
            "Epoch 483/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1862727288094720.0000 - accuracy: 0.6691\n",
            "Epoch 484/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1863905796192376.5000 - accuracy: 0.6691\n",
            "Epoch 485/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1864985232277504.0000 - accuracy: 0.6691\n",
            "Epoch 486/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1866131207140532.7500 - accuracy: 0.6691\n",
            "Epoch 487/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1867413659879303.5000 - accuracy: 0.6691\n",
            "Epoch 488/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1868553198116382.2500 - accuracy: 0.6691\n",
            "Epoch 489/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1869789440230339.7500 - accuracy: 0.6691\n",
            "Epoch 490/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1870930964227493.5000 - accuracy: 0.6691\n",
            "Epoch 491/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1872149753225697.7500 - accuracy: 0.6691\n",
            "Epoch 492/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1873344064944248.5000 - accuracy: 0.6691\n",
            "Epoch 493/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1874542224073185.7500 - accuracy: 0.6691\n",
            "Epoch 494/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1875703815594465.7500 - accuracy: 0.6691\n",
            "Epoch 495/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1876846466872500.7500 - accuracy: 0.6691\n",
            "Epoch 496/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1877982371859275.2500 - accuracy: 0.6691\n",
            "Epoch 497/1000\n",
            "272/272 [==============================] - 0s 165us/step - loss: -1879301492310016.0000 - accuracy: 0.6691\n",
            "Epoch 498/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1880452629035128.5000 - accuracy: 0.6691\n",
            "Epoch 499/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1881708121925029.5000 - accuracy: 0.6691\n",
            "Epoch 500/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1882911265953430.7500 - accuracy: 0.6691\n",
            "Epoch 501/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1883969408667407.0000 - accuracy: 0.6691\n",
            "Epoch 502/1000\n",
            "272/272 [==============================] - 0s 169us/step - loss: -1885160347178646.7500 - accuracy: 0.6691\n",
            "Epoch 503/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1886321037418014.2500 - accuracy: 0.6691\n",
            "Epoch 504/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1887399259375495.5000 - accuracy: 0.6691\n",
            "Epoch 505/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1888597061618507.2500 - accuracy: 0.6691\n",
            "Epoch 506/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1889712141454155.2500 - accuracy: 0.6691\n",
            "Epoch 507/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1890927180012122.5000 - accuracy: 0.6691\n",
            "Epoch 508/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1892061561371708.2500 - accuracy: 0.6691\n",
            "Epoch 509/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -1893110819416425.2500 - accuracy: 0.6691\n",
            "Epoch 510/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1894322287873807.0000 - accuracy: 0.6691\n",
            "Epoch 511/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1895471254971813.5000 - accuracy: 0.6691\n",
            "Epoch 512/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1896560069582366.2500 - accuracy: 0.6691\n",
            "Epoch 513/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1897796265682341.5000 - accuracy: 0.6691\n",
            "Epoch 514/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1898909410401822.2500 - accuracy: 0.6691\n",
            "Epoch 515/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1900116916105457.0000 - accuracy: 0.6691\n",
            "Epoch 516/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1901234081250364.2500 - accuracy: 0.6691\n",
            "Epoch 517/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1902438182289408.0000 - accuracy: 0.6691\n",
            "Epoch 518/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1903602584364453.5000 - accuracy: 0.6691\n",
            "Epoch 519/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -1904808714274695.5000 - accuracy: 0.6691\n",
            "Epoch 520/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1905924734436291.7500 - accuracy: 0.6691\n",
            "Epoch 521/1000\n",
            "272/272 [==============================] - 0s 177us/step - loss: -1907091769547354.5000 - accuracy: 0.6691\n",
            "Epoch 522/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1908272104734720.0000 - accuracy: 0.6691\n",
            "Epoch 523/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1909435733269082.5000 - accuracy: 0.6691\n",
            "Epoch 524/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1910612342495834.5000 - accuracy: 0.6691\n",
            "Epoch 525/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1911818816647409.0000 - accuracy: 0.6691\n",
            "Epoch 526/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1912942841268103.5000 - accuracy: 0.6691\n",
            "Epoch 527/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1914129524904659.0000 - accuracy: 0.6691\n",
            "Epoch 528/1000\n",
            "272/272 [==============================] - 0s 171us/step - loss: -1915295455186703.0000 - accuracy: 0.6691\n",
            "Epoch 529/1000\n",
            "272/272 [==============================] - 0s 165us/step - loss: -1916452183042409.2500 - accuracy: 0.6691\n",
            "Epoch 530/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1917590721003520.0000 - accuracy: 0.6691\n",
            "Epoch 531/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1918748080842149.5000 - accuracy: 0.6691\n",
            "Epoch 532/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1920107930698691.7500 - accuracy: 0.6691\n",
            "Epoch 533/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1921379143932506.5000 - accuracy: 0.6691\n",
            "Epoch 534/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1922580963270174.2500 - accuracy: 0.6691\n",
            "Epoch 535/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1923765670772736.0000 - accuracy: 0.6691\n",
            "Epoch 536/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1924927944793630.2500 - accuracy: 0.6691\n",
            "Epoch 537/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1926051177862806.7500 - accuracy: 0.6691\n",
            "Epoch 538/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1927241915664263.5000 - accuracy: 0.6691\n",
            "Epoch 539/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -1928354222139753.2500 - accuracy: 0.6691\n",
            "Epoch 540/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -1929547030508724.7500 - accuracy: 0.6691\n",
            "Epoch 541/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1930689717438343.5000 - accuracy: 0.6691\n",
            "Epoch 542/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1931835175322563.7500 - accuracy: 0.6691\n",
            "Epoch 543/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1932952895164175.0000 - accuracy: 0.6691\n",
            "Epoch 544/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1934092365741116.2500 - accuracy: 0.6691\n",
            "Epoch 545/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1935251525738014.2500 - accuracy: 0.6691\n",
            "Epoch 546/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1936419121898917.5000 - accuracy: 0.6691\n",
            "Epoch 547/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1937541436600561.0000 - accuracy: 0.6691\n",
            "Epoch 548/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1938671339168225.7500 - accuracy: 0.6691\n",
            "Epoch 549/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1939836435894031.0000 - accuracy: 0.6691\n",
            "Epoch 550/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1940927231388009.2500 - accuracy: 0.6691\n",
            "Epoch 551/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1942161091923727.0000 - accuracy: 0.6691\n",
            "Epoch 552/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1943219924431089.0000 - accuracy: 0.6691\n",
            "Epoch 553/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1944408622019764.7500 - accuracy: 0.6691\n",
            "Epoch 554/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1945580251613063.5000 - accuracy: 0.6691\n",
            "Epoch 555/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1946768861792135.5000 - accuracy: 0.6691\n",
            "Epoch 556/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1947879700137863.5000 - accuracy: 0.6691\n",
            "Epoch 557/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1949002676413861.5000 - accuracy: 0.6691\n",
            "Epoch 558/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1950305928748574.2500 - accuracy: 0.6691\n",
            "Epoch 559/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1951425266195998.2500 - accuracy: 0.6691\n",
            "Epoch 560/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1952644679413037.0000 - accuracy: 0.6691\n",
            "Epoch 561/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1953831370944753.0000 - accuracy: 0.6691\n",
            "Epoch 562/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1954957692070249.2500 - accuracy: 0.6691\n",
            "Epoch 563/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1956141511552301.0000 - accuracy: 0.6691\n",
            "Epoch 564/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1957352837033261.0000 - accuracy: 0.6691\n",
            "Epoch 565/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1958471898928790.7500 - accuracy: 0.6691\n",
            "Epoch 566/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1959631782535649.7500 - accuracy: 0.6691\n",
            "Epoch 567/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1960837539307038.2500 - accuracy: 0.6691\n",
            "Epoch 568/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1961983721568376.5000 - accuracy: 0.6691\n",
            "Epoch 569/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1963212617362733.0000 - accuracy: 0.6691\n",
            "Epoch 570/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1964355207052348.2500 - accuracy: 0.6691\n",
            "Epoch 571/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1965535526480233.2500 - accuracy: 0.6691\n",
            "Epoch 572/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1966706519842334.2500 - accuracy: 0.6691\n",
            "Epoch 573/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1967890685354465.7500 - accuracy: 0.6691\n",
            "Epoch 574/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1969028980477711.0000 - accuracy: 0.6691\n",
            "Epoch 575/1000\n",
            "272/272 [==============================] - 0s 122us/step - loss: -1970261282043060.7500 - accuracy: 0.6691\n",
            "Epoch 576/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1971448767408489.2500 - accuracy: 0.6691\n",
            "Epoch 577/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1972649112756705.7500 - accuracy: 0.6691\n",
            "Epoch 578/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1973838254872214.7500 - accuracy: 0.6691\n",
            "Epoch 579/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1975095084727356.2500 - accuracy: 0.6691\n",
            "Epoch 580/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1976265859368839.5000 - accuracy: 0.6691\n",
            "Epoch 581/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1977513688818145.7500 - accuracy: 0.6691\n",
            "Epoch 582/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1978711586836239.0000 - accuracy: 0.6691\n",
            "Epoch 583/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1979888664653101.0000 - accuracy: 0.6691\n",
            "Epoch 584/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1981070924224150.7500 - accuracy: 0.6691\n",
            "Epoch 585/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1982317517171049.2500 - accuracy: 0.6691\n",
            "Epoch 586/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1983521802851990.7500 - accuracy: 0.6691\n",
            "Epoch 587/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1984742324591194.5000 - accuracy: 0.6691\n",
            "Epoch 588/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1985936155876894.2500 - accuracy: 0.6691\n",
            "Epoch 589/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1987135348747565.0000 - accuracy: 0.6691\n",
            "Epoch 590/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1988321048603949.0000 - accuracy: 0.6691\n",
            "Epoch 591/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1989507751546639.0000 - accuracy: 0.6691\n",
            "Epoch 592/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1990804134922119.5000 - accuracy: 0.6691\n",
            "Epoch 593/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -1991978813103646.2500 - accuracy: 0.6691\n",
            "Epoch 594/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1993189720326144.0000 - accuracy: 0.6691\n",
            "Epoch 595/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1994450020243094.7500 - accuracy: 0.6691\n",
            "Epoch 596/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1995609324989741.0000 - accuracy: 0.6691\n",
            "Epoch 597/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1996851223862814.2500 - accuracy: 0.6691\n",
            "Epoch 598/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1998048362110494.2500 - accuracy: 0.6691\n",
            "Epoch 599/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1999294050352188.2500 - accuracy: 0.6691\n",
            "Epoch 600/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2000502364353716.7500 - accuracy: 0.6691\n",
            "Epoch 601/1000\n",
            "272/272 [==============================] - 0s 121us/step - loss: -2001722680217359.0000 - accuracy: 0.6691\n",
            "Epoch 602/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2002962174289317.5000 - accuracy: 0.6691\n",
            "Epoch 603/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2004207653124216.5000 - accuracy: 0.6691\n",
            "Epoch 604/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2005455504084751.0000 - accuracy: 0.6691\n",
            "Epoch 605/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2006814311595068.2500 - accuracy: 0.6691\n",
            "Epoch 606/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2008178372347783.5000 - accuracy: 0.6691\n",
            "Epoch 607/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2009359379055555.7500 - accuracy: 0.6691\n",
            "Epoch 608/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2010591999696414.2500 - accuracy: 0.6691\n",
            "Epoch 609/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2011766775827275.2500 - accuracy: 0.6691\n",
            "Epoch 610/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2012909715880056.5000 - accuracy: 0.6691\n",
            "Epoch 611/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2014158824869647.0000 - accuracy: 0.6691\n",
            "Epoch 612/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2015327088572536.5000 - accuracy: 0.6691\n",
            "Epoch 613/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2016525561102336.0000 - accuracy: 0.6691\n",
            "Epoch 614/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2017745615084122.5000 - accuracy: 0.6691\n",
            "Epoch 615/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2018867503046174.2500 - accuracy: 0.6691\n",
            "Epoch 616/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2020097478376508.2500 - accuracy: 0.6691\n",
            "Epoch 617/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2021322812092657.0000 - accuracy: 0.6691\n",
            "Epoch 618/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2022519275674202.5000 - accuracy: 0.6691\n",
            "Epoch 619/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2023684715469402.5000 - accuracy: 0.6691\n",
            "Epoch 620/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2024923557928478.2500 - accuracy: 0.6691\n",
            "Epoch 621/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2026145724821745.0000 - accuracy: 0.6691\n",
            "Epoch 622/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2027314132457110.7500 - accuracy: 0.6691\n",
            "Epoch 623/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2028567421271100.2500 - accuracy: 0.6691\n",
            "Epoch 624/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2029799382295973.5000 - accuracy: 0.6691\n",
            "Epoch 625/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2031022256792997.5000 - accuracy: 0.6691\n",
            "Epoch 626/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2032239513636623.0000 - accuracy: 0.6691\n",
            "Epoch 627/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2033457343958799.0000 - accuracy: 0.6691\n",
            "Epoch 628/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2034759683708566.7500 - accuracy: 0.6691\n",
            "Epoch 629/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2035958242067395.7500 - accuracy: 0.6691\n",
            "Epoch 630/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2037287984284611.7500 - accuracy: 0.6691\n",
            "Epoch 631/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2038490289005025.7500 - accuracy: 0.6691\n",
            "Epoch 632/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2039659834237409.7500 - accuracy: 0.6691\n",
            "Epoch 633/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2040925293907727.0000 - accuracy: 0.6691\n",
            "Epoch 634/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2042197709145027.7500 - accuracy: 0.6691\n",
            "Epoch 635/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2043487224409389.0000 - accuracy: 0.6691\n",
            "Epoch 636/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2044727289199676.2500 - accuracy: 0.6691\n",
            "Epoch 637/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2046001673820762.5000 - accuracy: 0.6691\n",
            "Epoch 638/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2047223343951149.0000 - accuracy: 0.6691\n",
            "Epoch 639/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2048504332789157.5000 - accuracy: 0.6691\n",
            "Epoch 640/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -2049747847895642.5000 - accuracy: 0.6691\n",
            "Epoch 641/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2051007749585016.5000 - accuracy: 0.6691\n",
            "Epoch 642/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2052269700315256.5000 - accuracy: 0.6691\n",
            "Epoch 643/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2053498076875595.2500 - accuracy: 0.6691\n",
            "Epoch 644/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2054808349010281.2500 - accuracy: 0.6691\n",
            "Epoch 645/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2056281892878456.5000 - accuracy: 0.6691\n",
            "Epoch 646/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2057567062905555.0000 - accuracy: 0.6691\n",
            "Epoch 647/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2058747993283162.5000 - accuracy: 0.6691\n",
            "Epoch 648/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2060013797499361.7500 - accuracy: 0.6691\n",
            "Epoch 649/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -2061281168627350.7500 - accuracy: 0.6691\n",
            "Epoch 650/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2062480156686456.5000 - accuracy: 0.6691\n",
            "Epoch 651/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2063694574541402.5000 - accuracy: 0.6691\n",
            "Epoch 652/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2064976595710193.0000 - accuracy: 0.6691\n",
            "Epoch 653/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2066085552802635.2500 - accuracy: 0.6691\n",
            "Epoch 654/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2067354731151360.0000 - accuracy: 0.6691\n",
            "Epoch 655/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2068532632963915.2500 - accuracy: 0.6691\n",
            "Epoch 656/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2069768871993825.7500 - accuracy: 0.6691\n",
            "Epoch 657/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2070939348064858.5000 - accuracy: 0.6691\n",
            "Epoch 658/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2072157658515335.5000 - accuracy: 0.6691\n",
            "Epoch 659/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2073542553918042.5000 - accuracy: 0.6691\n",
            "Epoch 660/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2074846612993204.7500 - accuracy: 0.6691\n",
            "Epoch 661/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2075986160054513.0000 - accuracy: 0.6691\n",
            "Epoch 662/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2077214253807736.5000 - accuracy: 0.6691\n",
            "Epoch 663/1000\n",
            "272/272 [==============================] - 0s 180us/step - loss: -2078388914903642.5000 - accuracy: 0.6691\n",
            "Epoch 664/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2079535764679981.0000 - accuracy: 0.6691\n",
            "Epoch 665/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2080752196726061.0000 - accuracy: 0.6691\n",
            "Epoch 666/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2081924640083727.0000 - accuracy: 0.6691\n",
            "Epoch 667/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2083128828216259.7500 - accuracy: 0.6691\n",
            "Epoch 668/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2084372345072941.0000 - accuracy: 0.6691\n",
            "Epoch 669/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2085544785115256.5000 - accuracy: 0.6691\n",
            "Epoch 670/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2086829954170880.0000 - accuracy: 0.6691\n",
            "Epoch 671/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2087964462331060.7500 - accuracy: 0.6691\n",
            "Epoch 672/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2089219922560903.5000 - accuracy: 0.6691\n",
            "Epoch 673/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2090471088070174.2500 - accuracy: 0.6691\n",
            "Epoch 674/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2091630059539275.2500 - accuracy: 0.6691\n",
            "Epoch 675/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2092898316251377.0000 - accuracy: 0.6691\n",
            "Epoch 676/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2094227665483896.5000 - accuracy: 0.6691\n",
            "Epoch 677/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2095476568304941.0000 - accuracy: 0.6691\n",
            "Epoch 678/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2096691528181157.5000 - accuracy: 0.6691\n",
            "Epoch 679/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2097880648430772.7500 - accuracy: 0.6691\n",
            "Epoch 680/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2099166320910817.7500 - accuracy: 0.6691\n",
            "Epoch 681/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -2100316954442812.2500 - accuracy: 0.6691\n",
            "Epoch 682/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2101654133222701.0000 - accuracy: 0.6691\n",
            "Epoch 683/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2102904678622629.5000 - accuracy: 0.6691\n",
            "Epoch 684/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2104213926946213.7500 - accuracy: 0.6691\n",
            "Epoch 685/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2105588194846117.7500 - accuracy: 0.6691\n",
            "Epoch 686/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2106784911196160.0000 - accuracy: 0.6691\n",
            "Epoch 687/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2107977590713645.2500 - accuracy: 0.6691\n",
            "Epoch 688/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2109246562400376.5000 - accuracy: 0.6691\n",
            "Epoch 689/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2110572030005007.0000 - accuracy: 0.6691\n",
            "Epoch 690/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2111762038491015.5000 - accuracy: 0.6691\n",
            "Epoch 691/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2113107744290936.5000 - accuracy: 0.6691\n",
            "Epoch 692/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2114433968905758.0000 - accuracy: 0.6691\n",
            "Epoch 693/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2115727984103183.0000 - accuracy: 0.6691\n",
            "Epoch 694/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2116864773440451.7500 - accuracy: 0.6691\n",
            "Epoch 695/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2118145843257826.0000 - accuracy: 0.6691\n",
            "Epoch 696/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2119270172011821.2500 - accuracy: 0.6691\n",
            "Epoch 697/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -2120542567684698.2500 - accuracy: 0.6691\n",
            "Epoch 698/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2121768885458582.5000 - accuracy: 0.6691\n",
            "Epoch 699/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2123079309760150.5000 - accuracy: 0.6691\n",
            "Epoch 700/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -2124249469689374.0000 - accuracy: 0.6691\n",
            "Epoch 701/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2125478948333929.5000 - accuracy: 0.6691\n",
            "Epoch 702/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2126695870589289.5000 - accuracy: 0.6691\n",
            "Epoch 703/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2127971051973933.2500 - accuracy: 0.6691\n",
            "Epoch 704/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2129212714886565.7500 - accuracy: 0.6691\n",
            "Epoch 705/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2130437817700111.0000 - accuracy: 0.6691\n",
            "Epoch 706/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2131669121607077.7500 - accuracy: 0.6691\n",
            "Epoch 707/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2132849764520658.7500 - accuracy: 0.6691\n",
            "Epoch 708/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2134108592807454.0000 - accuracy: 0.6691\n",
            "Epoch 709/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2135294467652668.2500 - accuracy: 0.6691\n",
            "Epoch 710/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2136571556736301.2500 - accuracy: 0.6691\n",
            "Epoch 711/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2137800139696489.5000 - accuracy: 0.6691\n",
            "Epoch 712/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -2139050474533105.0000 - accuracy: 0.6691\n",
            "Epoch 713/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2140335803326946.0000 - accuracy: 0.6691\n",
            "Epoch 714/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2141602098029989.7500 - accuracy: 0.6691\n",
            "Epoch 715/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2142840291829157.7500 - accuracy: 0.6691\n",
            "Epoch 716/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2144194533014588.2500 - accuracy: 0.6691\n",
            "Epoch 717/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -2145440986639661.2500 - accuracy: 0.6691\n",
            "Epoch 718/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2146604249251840.0000 - accuracy: 0.6691\n",
            "Epoch 719/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2147800206309496.5000 - accuracy: 0.6691\n",
            "Epoch 720/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2149099521318430.0000 - accuracy: 0.6691\n",
            "Epoch 721/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2150349536601509.7500 - accuracy: 0.6691\n",
            "Epoch 722/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2151609285075426.0000 - accuracy: 0.6691\n",
            "Epoch 723/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2152847797664828.2500 - accuracy: 0.6691\n",
            "Epoch 724/1000\n",
            "272/272 [==============================] - 0s 200us/step - loss: -2154105730436879.0000 - accuracy: 0.6691\n",
            "Epoch 725/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2155373085743706.2500 - accuracy: 0.6691\n",
            "Epoch 726/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2156605899353389.2500 - accuracy: 0.6691\n",
            "Epoch 727/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2157938441083482.2500 - accuracy: 0.6691\n",
            "Epoch 728/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -2159154683769073.0000 - accuracy: 0.6691\n",
            "Epoch 729/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -2160470518352715.2500 - accuracy: 0.6691\n",
            "Epoch 730/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2161690180321280.0000 - accuracy: 0.6691\n",
            "Epoch 731/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2163018271833148.2500 - accuracy: 0.6691\n",
            "Epoch 732/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2164336058865302.5000 - accuracy: 0.6691\n",
            "Epoch 733/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2165564002178710.5000 - accuracy: 0.6691\n",
            "Epoch 734/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -2166748087814023.5000 - accuracy: 0.6691\n",
            "Epoch 735/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2168123951754541.2500 - accuracy: 0.6691\n",
            "Epoch 736/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -2169528948066183.5000 - accuracy: 0.6691\n",
            "Epoch 737/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2170873668944835.7500 - accuracy: 0.6691\n",
            "Epoch 738/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -2172093639287266.0000 - accuracy: 0.6691\n",
            "Epoch 739/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2173354207670513.0000 - accuracy: 0.6691\n",
            "Epoch 740/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2174503694846795.2500 - accuracy: 0.6691\n",
            "Epoch 741/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2175770558957447.5000 - accuracy: 0.6691\n",
            "Epoch 742/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2176974207875192.5000 - accuracy: 0.6691\n",
            "Epoch 743/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2178235334640579.7500 - accuracy: 0.6691\n",
            "Epoch 744/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2179455037719491.7500 - accuracy: 0.6691\n",
            "Epoch 745/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2180726788626070.5000 - accuracy: 0.6691\n",
            "Epoch 746/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2181961926651181.2500 - accuracy: 0.6691\n",
            "Epoch 747/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2183166737082729.5000 - accuracy: 0.6691\n",
            "Epoch 748/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2184460175995121.0000 - accuracy: 0.6691\n",
            "Epoch 749/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2185632608003493.7500 - accuracy: 0.6691\n",
            "Epoch 750/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2186947064819953.0000 - accuracy: 0.6691\n",
            "Epoch 751/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2188200283379350.5000 - accuracy: 0.6691\n",
            "Epoch 752/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2189439788415096.5000 - accuracy: 0.6691\n",
            "Epoch 753/1000\n",
            "272/272 [==============================] - 0s 175us/step - loss: -2190726822162673.0000 - accuracy: 0.6691\n",
            "Epoch 754/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2191895478156348.2500 - accuracy: 0.6691\n",
            "Epoch 755/1000\n",
            "272/272 [==============================] - 0s 175us/step - loss: -2193184036194183.5000 - accuracy: 0.6691\n",
            "Epoch 756/1000\n",
            "272/272 [==============================] - 0s 166us/step - loss: -2194467369732939.2500 - accuracy: 0.6691\n",
            "Epoch 757/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2195635535887420.2500 - accuracy: 0.6691\n",
            "Epoch 758/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2197004135863958.5000 - accuracy: 0.6691\n",
            "Epoch 759/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2198252260595350.5000 - accuracy: 0.6691\n",
            "Epoch 760/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2199576515090070.5000 - accuracy: 0.6691\n",
            "Epoch 761/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2200785711776707.7500 - accuracy: 0.6691\n",
            "Epoch 762/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2202039639851971.7500 - accuracy: 0.6691\n",
            "Epoch 763/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2203332505964303.0000 - accuracy: 0.6691\n",
            "Epoch 764/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2204595009695021.2500 - accuracy: 0.6691\n",
            "Epoch 765/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2205891488799322.2500 - accuracy: 0.6691\n",
            "Epoch 766/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2207121262575616.0000 - accuracy: 0.6691\n",
            "Epoch 767/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2208355056430381.2500 - accuracy: 0.6691\n",
            "Epoch 768/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2209702315726486.5000 - accuracy: 0.6691\n",
            "Epoch 769/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2210966896470377.5000 - accuracy: 0.6691\n",
            "Epoch 770/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2212287390679040.0000 - accuracy: 0.6691\n",
            "Epoch 771/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2213607413244386.0000 - accuracy: 0.6691\n",
            "Epoch 772/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2214866857723783.5000 - accuracy: 0.6691\n",
            "Epoch 773/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2216120468573967.0000 - accuracy: 0.6691\n",
            "Epoch 774/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2217377746972913.0000 - accuracy: 0.6691\n",
            "Epoch 775/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2218657735307505.0000 - accuracy: 0.6691\n",
            "Epoch 776/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2219945039433487.0000 - accuracy: 0.6691\n",
            "Epoch 777/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2221221816843324.2500 - accuracy: 0.6691\n",
            "Epoch 778/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2222487661491380.7500 - accuracy: 0.6691\n",
            "Epoch 779/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2223830771880658.7500 - accuracy: 0.6691\n",
            "Epoch 780/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2225247220538910.0000 - accuracy: 0.6691\n",
            "Epoch 781/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2226495758008320.0000 - accuracy: 0.6691\n",
            "Epoch 782/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2227814768050176.0000 - accuracy: 0.6691\n",
            "Epoch 783/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2229051652756178.7500 - accuracy: 0.6691\n",
            "Epoch 784/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2230471482023454.0000 - accuracy: 0.6691\n",
            "Epoch 785/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2231768515269330.7500 - accuracy: 0.6691\n",
            "Epoch 786/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2233075867860028.2500 - accuracy: 0.6691\n",
            "Epoch 787/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2234391130333666.0000 - accuracy: 0.6691\n",
            "Epoch 788/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2235735856289430.5000 - accuracy: 0.6691\n",
            "Epoch 789/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2236948458750915.7500 - accuracy: 0.6691\n",
            "Epoch 790/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2238322723196687.0000 - accuracy: 0.6691\n",
            "Epoch 791/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2239619391044668.2500 - accuracy: 0.6691\n",
            "Epoch 792/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2241044966354221.2500 - accuracy: 0.6691\n",
            "Epoch 793/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2242422921818353.0000 - accuracy: 0.6691\n",
            "Epoch 794/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2243675472897445.7500 - accuracy: 0.6691\n",
            "Epoch 795/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2244979736352406.5000 - accuracy: 0.6691\n",
            "Epoch 796/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2246287627417720.5000 - accuracy: 0.6691\n",
            "Epoch 797/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2247648182040696.5000 - accuracy: 0.6691\n",
            "Epoch 798/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2248881028618842.2500 - accuracy: 0.6691\n",
            "Epoch 799/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2250218451223491.7500 - accuracy: 0.6691\n",
            "Epoch 800/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2251504177427636.7500 - accuracy: 0.6691\n",
            "Epoch 801/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2252856433126701.5000 - accuracy: 0.6691\n",
            "Epoch 802/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2254118888561242.0000 - accuracy: 0.6691\n",
            "Epoch 803/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2255478330398358.5000 - accuracy: 0.6691\n",
            "Epoch 804/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2256745019242616.5000 - accuracy: 0.6691\n",
            "Epoch 805/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -2258101170524882.5000 - accuracy: 0.6691\n",
            "Epoch 806/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2259390057137091.5000 - accuracy: 0.6691\n",
            "Epoch 807/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2260722786130522.0000 - accuracy: 0.6691\n",
            "Epoch 808/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2262046610863285.0000 - accuracy: 0.6691\n",
            "Epoch 809/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2263327312599642.0000 - accuracy: 0.6691\n",
            "Epoch 810/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2264748627020619.0000 - accuracy: 0.6691\n",
            "Epoch 811/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2266050053059765.0000 - accuracy: 0.6691\n",
            "Epoch 812/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2267412775644461.5000 - accuracy: 0.6691\n",
            "Epoch 813/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2268760914880873.5000 - accuracy: 0.6691\n",
            "Epoch 814/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2270033997478972.5000 - accuracy: 0.6691\n",
            "Epoch 815/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2271396899431846.0000 - accuracy: 0.6691\n",
            "Epoch 816/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2272870389822645.0000 - accuracy: 0.6691\n",
            "Epoch 817/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2274220965332871.5000 - accuracy: 0.6691\n",
            "Epoch 818/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2275553379876864.0000 - accuracy: 0.6691\n",
            "Epoch 819/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2276900015578654.0000 - accuracy: 0.6691\n",
            "Epoch 820/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2278219004834033.0000 - accuracy: 0.6691\n",
            "Epoch 821/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2279630807807157.0000 - accuracy: 0.6691\n",
            "Epoch 822/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2280912200370055.5000 - accuracy: 0.6691\n",
            "Epoch 823/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2282248967059576.5000 - accuracy: 0.6691\n",
            "Epoch 824/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2283610655717496.5000 - accuracy: 0.6691\n",
            "Epoch 825/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2284938107251049.5000 - accuracy: 0.6691\n",
            "Epoch 826/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2286284353075320.5000 - accuracy: 0.6691\n",
            "Epoch 827/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2287612623276875.0000 - accuracy: 0.6691\n",
            "Epoch 828/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2288978660074918.0000 - accuracy: 0.6691\n",
            "Epoch 829/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2290288709506710.5000 - accuracy: 0.6691\n",
            "Epoch 830/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2291613970295145.5000 - accuracy: 0.6691\n",
            "Epoch 831/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2292987903421741.5000 - accuracy: 0.6691\n",
            "Epoch 832/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2294344762461967.0000 - accuracy: 0.6691\n",
            "Epoch 833/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2295615355293455.0000 - accuracy: 0.6691\n",
            "Epoch 834/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2297031100187587.5000 - accuracy: 0.6691\n",
            "Epoch 835/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2298436310948442.0000 - accuracy: 0.6691\n",
            "Epoch 836/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2299784018884728.5000 - accuracy: 0.6691\n",
            "Epoch 837/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2301178210464949.0000 - accuracy: 0.6691\n",
            "Epoch 838/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -2302581053078588.5000 - accuracy: 0.6691\n",
            "Epoch 839/1000\n",
            "272/272 [==============================] - 0s 164us/step - loss: -2303822146413990.0000 - accuracy: 0.6691\n",
            "Epoch 840/1000\n",
            "272/272 [==============================] - 0s 189us/step - loss: -2305288681091313.0000 - accuracy: 0.6691\n",
            "Epoch 841/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2306562171277071.0000 - accuracy: 0.6691\n",
            "Epoch 842/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2307968473728903.5000 - accuracy: 0.6691\n",
            "Epoch 843/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2309288178791605.0000 - accuracy: 0.6691\n",
            "Epoch 844/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2310557238481618.5000 - accuracy: 0.6691\n",
            "Epoch 845/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2311977998899802.0000 - accuracy: 0.6691\n",
            "Epoch 846/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2313220027919661.5000 - accuracy: 0.6691\n",
            "Epoch 847/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2314627893243181.5000 - accuracy: 0.6691\n",
            "Epoch 848/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2315943995429586.5000 - accuracy: 0.6691\n",
            "Epoch 849/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2317306934606908.5000 - accuracy: 0.6691\n",
            "Epoch 850/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2318707387731727.0000 - accuracy: 0.6691\n",
            "Epoch 851/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2320059538819915.0000 - accuracy: 0.6691\n",
            "Epoch 852/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2321429852354680.5000 - accuracy: 0.6691\n",
            "Epoch 853/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2322924555869726.0000 - accuracy: 0.6691\n",
            "Epoch 854/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2324339974425419.0000 - accuracy: 0.6691\n",
            "Epoch 855/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2326024752911058.5000 - accuracy: 0.6691\n",
            "Epoch 856/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2327320968946146.0000 - accuracy: 0.6691\n",
            "Epoch 857/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2328603027539847.5000 - accuracy: 0.6691\n",
            "Epoch 858/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2329929999542633.5000 - accuracy: 0.6691\n",
            "Epoch 859/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2331266697519586.0000 - accuracy: 0.6691\n",
            "Epoch 860/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2332531580685131.0000 - accuracy: 0.6691\n",
            "Epoch 861/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2333954379997666.0000 - accuracy: 0.6691\n",
            "Epoch 862/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2335277730203527.5000 - accuracy: 0.6691\n",
            "Epoch 863/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2336673102946786.0000 - accuracy: 0.6691\n",
            "Epoch 864/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -2337927744732220.5000 - accuracy: 0.6691\n",
            "Epoch 865/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2339364967068853.0000 - accuracy: 0.6691\n",
            "Epoch 866/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2340692147830784.0000 - accuracy: 0.6691\n",
            "Epoch 867/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2342075126868329.5000 - accuracy: 0.6691\n",
            "Epoch 868/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2343403595127386.0000 - accuracy: 0.6691\n",
            "Epoch 869/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2344767463373884.5000 - accuracy: 0.6691\n",
            "Epoch 870/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2346106251162805.0000 - accuracy: 0.6691\n",
            "Epoch 871/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2347364315561984.0000 - accuracy: 0.6691\n",
            "Epoch 872/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2348778149565138.5000 - accuracy: 0.6691\n",
            "Epoch 873/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2350137088825946.0000 - accuracy: 0.6691\n",
            "Epoch 874/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2351464067551954.5000 - accuracy: 0.6691\n",
            "Epoch 875/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2352866967991477.0000 - accuracy: 0.6691\n",
            "Epoch 876/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2354235083988510.0000 - accuracy: 0.6691\n",
            "Epoch 877/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2355657860710400.0000 - accuracy: 0.6691\n",
            "Epoch 878/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2356977544570278.0000 - accuracy: 0.6691\n",
            "Epoch 879/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2358281750137675.0000 - accuracy: 0.6691\n",
            "Epoch 880/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2359674002519221.0000 - accuracy: 0.6691\n",
            "Epoch 881/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2361011690105193.5000 - accuracy: 0.6691\n",
            "Epoch 882/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2362453060392598.5000 - accuracy: 0.6691\n",
            "Epoch 883/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2363800826829402.0000 - accuracy: 0.6691\n",
            "Epoch 884/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2365161264752038.0000 - accuracy: 0.6691\n",
            "Epoch 885/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2366453342921186.0000 - accuracy: 0.6691\n",
            "Epoch 886/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2367818676213398.5000 - accuracy: 0.6691\n",
            "Epoch 887/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2369158674860875.0000 - accuracy: 0.6691\n",
            "Epoch 888/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2370613879073129.5000 - accuracy: 0.6691\n",
            "Epoch 889/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2371954352478810.0000 - accuracy: 0.6691\n",
            "Epoch 890/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -2373245829813910.5000 - accuracy: 0.6691\n",
            "Epoch 891/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2374611731742720.0000 - accuracy: 0.6691\n",
            "Epoch 892/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2376007419212981.0000 - accuracy: 0.6691\n",
            "Epoch 893/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2377292913435106.0000 - accuracy: 0.6691\n",
            "Epoch 894/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2378659009967285.0000 - accuracy: 0.6691\n",
            "Epoch 895/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2380003267066940.5000 - accuracy: 0.6691\n",
            "Epoch 896/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2381361348839303.5000 - accuracy: 0.6691\n",
            "Epoch 897/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2382763018990773.0000 - accuracy: 0.6691\n",
            "Epoch 898/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2384090209899218.5000 - accuracy: 0.6691\n",
            "Epoch 899/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2385452087084935.5000 - accuracy: 0.6691\n",
            "Epoch 900/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2386801299199638.5000 - accuracy: 0.6691\n",
            "Epoch 901/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2388192166427708.5000 - accuracy: 0.6691\n",
            "Epoch 902/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2389589323137987.5000 - accuracy: 0.6691\n",
            "Epoch 903/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2390958743424783.0000 - accuracy: 0.6691\n",
            "Epoch 904/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2392307769247623.5000 - accuracy: 0.6691\n",
            "Epoch 905/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2393708966491317.0000 - accuracy: 0.6691\n",
            "Epoch 906/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2395084353807782.0000 - accuracy: 0.6691\n",
            "Epoch 907/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2396401407885312.0000 - accuracy: 0.6691\n",
            "Epoch 908/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2397762144867990.5000 - accuracy: 0.6691\n",
            "Epoch 909/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2399148244591074.0000 - accuracy: 0.6691\n",
            "Epoch 910/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2400486919334249.5000 - accuracy: 0.6691\n",
            "Epoch 911/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2401994770434891.0000 - accuracy: 0.6691\n",
            "Epoch 912/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2403320894509778.5000 - accuracy: 0.6691\n",
            "Epoch 913/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2404652164132141.5000 - accuracy: 0.6691\n",
            "Epoch 914/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2406009575987802.0000 - accuracy: 0.6691\n",
            "Epoch 915/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2407425300511804.5000 - accuracy: 0.6691\n",
            "Epoch 916/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2408840126961302.5000 - accuracy: 0.6691\n",
            "Epoch 917/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2410258626510848.0000 - accuracy: 0.6691\n",
            "Epoch 918/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2411690804919356.5000 - accuracy: 0.6691\n",
            "Epoch 919/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2413121362599454.0000 - accuracy: 0.6691\n",
            "Epoch 920/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2414440420629082.0000 - accuracy: 0.6691\n",
            "Epoch 921/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2415856488993490.5000 - accuracy: 0.6691\n",
            "Epoch 922/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2417249844646610.5000 - accuracy: 0.6691\n",
            "Epoch 923/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2418578719876518.0000 - accuracy: 0.6691\n",
            "Epoch 924/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2420006007911605.0000 - accuracy: 0.6691\n",
            "Epoch 925/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2421308263806133.0000 - accuracy: 0.6691\n",
            "Epoch 926/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2422726300501895.5000 - accuracy: 0.6691\n",
            "Epoch 927/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2424088639677862.0000 - accuracy: 0.6691\n",
            "Epoch 928/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -2425619769242805.0000 - accuracy: 0.6691\n",
            "Epoch 929/1000\n",
            "272/272 [==============================] - 0s 230us/step - loss: -2427085108034620.5000 - accuracy: 0.6691\n",
            "Epoch 930/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2428516955894965.0000 - accuracy: 0.6691\n",
            "Epoch 931/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2429971419473318.0000 - accuracy: 0.6691\n",
            "Epoch 932/1000\n",
            "272/272 [==============================] - 0s 183us/step - loss: -2431312430135416.5000 - accuracy: 0.6691\n",
            "Epoch 933/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2432653941986002.5000 - accuracy: 0.6691\n",
            "Epoch 934/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2434067615618710.5000 - accuracy: 0.6691\n",
            "Epoch 935/1000\n",
            "272/272 [==============================] - 0s 173us/step - loss: -2435468296037798.0000 - accuracy: 0.6691\n",
            "Epoch 936/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2436783684668235.0000 - accuracy: 0.6691\n",
            "Epoch 937/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2438158956086211.5000 - accuracy: 0.6691\n",
            "Epoch 938/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2439493172037511.5000 - accuracy: 0.6691\n",
            "Epoch 939/1000\n",
            "272/272 [==============================] - 0s 175us/step - loss: -2440939239467369.5000 - accuracy: 0.6691\n",
            "Epoch 940/1000\n",
            "272/272 [==============================] - 0s 165us/step - loss: -2442270802814374.0000 - accuracy: 0.6691\n",
            "Epoch 941/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -2443666459197440.0000 - accuracy: 0.6691\n",
            "Epoch 942/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2445062407639762.5000 - accuracy: 0.6691\n",
            "Epoch 943/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2446403515310562.0000 - accuracy: 0.6691\n",
            "Epoch 944/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2447799088424358.0000 - accuracy: 0.6691\n",
            "Epoch 945/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2449186409414656.0000 - accuracy: 0.6691\n",
            "Epoch 946/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2450527210546598.0000 - accuracy: 0.6691\n",
            "Epoch 947/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2451931309816892.5000 - accuracy: 0.6691\n",
            "Epoch 948/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2453349415718671.0000 - accuracy: 0.6691\n",
            "Epoch 949/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2454765404545506.0000 - accuracy: 0.6691\n",
            "Epoch 950/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2456268945628702.0000 - accuracy: 0.6691\n",
            "Epoch 951/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2457785051150456.5000 - accuracy: 0.6691\n",
            "Epoch 952/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2459202226471875.5000 - accuracy: 0.6691\n",
            "Epoch 953/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2460745245115331.5000 - accuracy: 0.6691\n",
            "Epoch 954/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2462287388228668.5000 - accuracy: 0.6691\n",
            "Epoch 955/1000\n",
            "272/272 [==============================] - 0s 121us/step - loss: -2463586846275704.5000 - accuracy: 0.6691\n",
            "Epoch 956/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2464992906753205.0000 - accuracy: 0.6691\n",
            "Epoch 957/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2466326145539614.0000 - accuracy: 0.6691\n",
            "Epoch 958/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2467699683753984.0000 - accuracy: 0.6691\n",
            "Epoch 959/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2469054594570842.0000 - accuracy: 0.6691\n",
            "Epoch 960/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2470488746152658.5000 - accuracy: 0.6691\n",
            "Epoch 961/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2471749041674842.0000 - accuracy: 0.6691\n",
            "Epoch 962/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2473172170409863.5000 - accuracy: 0.6691\n",
            "Epoch 963/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2474506170061522.5000 - accuracy: 0.6691\n",
            "Epoch 964/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2475924746403840.0000 - accuracy: 0.6691\n",
            "Epoch 965/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2477297622905073.0000 - accuracy: 0.6691\n",
            "Epoch 966/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2478715859570447.0000 - accuracy: 0.6691\n",
            "Epoch 967/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2480088139246893.5000 - accuracy: 0.6691\n",
            "Epoch 968/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2481602866122511.0000 - accuracy: 0.6691\n",
            "Epoch 969/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2483105784428664.5000 - accuracy: 0.6691\n",
            "Epoch 970/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2484595239265702.0000 - accuracy: 0.6691\n",
            "Epoch 971/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2485914717311698.5000 - accuracy: 0.6691\n",
            "Epoch 972/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2487274157514270.0000 - accuracy: 0.6691\n",
            "Epoch 973/1000\n",
            "272/272 [==============================] - 0s 165us/step - loss: -2488797525797225.5000 - accuracy: 0.6691\n",
            "Epoch 974/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -2490167187210240.0000 - accuracy: 0.6691\n",
            "Epoch 975/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2491617105797602.0000 - accuracy: 0.6691\n",
            "Epoch 976/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2493125450638758.0000 - accuracy: 0.6691\n",
            "Epoch 977/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2494702557824903.5000 - accuracy: 0.6691\n",
            "Epoch 978/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2495992258193769.5000 - accuracy: 0.6691\n",
            "Epoch 979/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2497342659023570.5000 - accuracy: 0.6691\n",
            "Epoch 980/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2498751272598588.5000 - accuracy: 0.6691\n",
            "Epoch 981/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2500093651205180.5000 - accuracy: 0.6691\n",
            "Epoch 982/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2501458167525617.0000 - accuracy: 0.6691\n",
            "Epoch 983/1000\n",
            "272/272 [==============================] - 0s 190us/step - loss: -2502807955809702.0000 - accuracy: 0.6691\n",
            "Epoch 984/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2504211152317741.5000 - accuracy: 0.6691\n",
            "Epoch 985/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2505615214702833.0000 - accuracy: 0.6691\n",
            "Epoch 986/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2506915457208079.0000 - accuracy: 0.6691\n",
            "Epoch 987/1000\n",
            "272/272 [==============================] - 0s 162us/step - loss: -2508342349930014.0000 - accuracy: 0.6691\n",
            "Epoch 988/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -2509648699835331.5000 - accuracy: 0.6691\n",
            "Epoch 989/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -2510994488904523.0000 - accuracy: 0.6691\n",
            "Epoch 990/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2512378065198622.0000 - accuracy: 0.6691\n",
            "Epoch 991/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2513718623477278.0000 - accuracy: 0.6691\n",
            "Epoch 992/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2515146762709353.5000 - accuracy: 0.6691\n",
            "Epoch 993/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2516560695463695.0000 - accuracy: 0.6691\n",
            "Epoch 994/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2517928481066767.0000 - accuracy: 0.6691\n",
            "Epoch 995/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -2519305985411915.0000 - accuracy: 0.6691\n",
            "Epoch 996/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -2520640223198268.5000 - accuracy: 0.6691\n",
            "Epoch 997/1000\n",
            "272/272 [==============================] - 0s 180us/step - loss: -2522012558325880.5000 - accuracy: 0.6691\n",
            "Epoch 998/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -2523417536857991.5000 - accuracy: 0.6691\n",
            "Epoch 999/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2524801404224451.5000 - accuracy: 0.6691\n",
            "Epoch 1000/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2526250382979313.0000 - accuracy: 0.6691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff401b67a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Y3cmhJOkfO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "93c87eb3-08d6-4de0-dd88-c0a42f5a4207"
      },
      "source": [
        "ANNPred = model.predict(XTest)\n",
        "\n",
        "ANN_Pred = []\n",
        "for i in ANN_Pred:\n",
        "  for k in i:\n",
        "    ANN_Pred.append(int(k))\n",
        "  \n",
        "ANNPred = np.array(ANN_Pred) \n",
        "\n",
        "EVALUATION['ANN'] = list(ANNPred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca57ba023ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mANNPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mANN_Pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mANN_Pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2qB1D_KtmhL"
      },
      "source": [
        "###Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCU8dMUku33L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7018da23-03bb-4bd5-f586-9dec2a4a87a3"
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(64, activation = 'relu', input_shape=(15,)))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n",
        "/\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 64)                1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 149,505\n",
            "Trainable params: 149,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAsMvoECgG55"
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights.best.cnn.hdf5', verbose=1, save_best_only=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPVn9qXKu3vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e930039d-5d61-4bdf-c0cf-cf8e5bcd7e36"
      },
      "source": [
        "NN_model.fit(X_train, Y_train, epochs=1000, batch_size=30, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 868 samples, validate on 218 samples\n",
            "Epoch 1/1000\n",
            "868/868 [==============================] - 0s 272us/step - loss: 1.3422 - mean_absolute_error: 1.3422 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.38073, saving model to Weights-001--0.38073.hdf5\n",
            "Epoch 2/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.38073 to 0.38073, saving model to Weights-002--0.38073.hdf5\n",
            "Epoch 3/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.38073\n",
            "Epoch 4/1000\n",
            "868/868 [==============================] - 0s 150us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.38073\n",
            "Epoch 5/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.38073\n",
            "Epoch 6/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.38073\n",
            "Epoch 7/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.38073\n",
            "Epoch 8/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.38073\n",
            "Epoch 9/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.38073\n",
            "Epoch 10/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.38073\n",
            "Epoch 11/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.38073\n",
            "Epoch 12/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.38073\n",
            "Epoch 13/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.38073\n",
            "Epoch 14/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.38073\n",
            "Epoch 15/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.38073\n",
            "Epoch 16/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.38073\n",
            "Epoch 17/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.38073\n",
            "Epoch 18/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.38073\n",
            "Epoch 19/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.38073\n",
            "Epoch 20/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.38073\n",
            "Epoch 21/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.38073\n",
            "Epoch 22/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.38073\n",
            "Epoch 23/1000\n",
            "868/868 [==============================] - 0s 149us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.38073\n",
            "Epoch 24/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.38073\n",
            "Epoch 25/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.38073\n",
            "Epoch 26/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.38073\n",
            "Epoch 27/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.38073\n",
            "Epoch 28/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.38073\n",
            "Epoch 29/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.38073\n",
            "Epoch 30/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.38073\n",
            "Epoch 31/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.38073\n",
            "Epoch 32/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.38073\n",
            "Epoch 33/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.38073\n",
            "Epoch 34/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.38073\n",
            "Epoch 35/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.38073\n",
            "Epoch 36/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.38073\n",
            "Epoch 37/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.38073\n",
            "Epoch 38/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.38073\n",
            "Epoch 39/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.38073\n",
            "Epoch 40/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.38073\n",
            "Epoch 41/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.38073\n",
            "Epoch 42/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.38073\n",
            "Epoch 43/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.38073\n",
            "Epoch 44/1000\n",
            "868/868 [==============================] - 0s 147us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.38073\n",
            "Epoch 45/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.38073\n",
            "Epoch 46/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.38073\n",
            "Epoch 47/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.38073\n",
            "Epoch 48/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.38073\n",
            "Epoch 49/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.38073\n",
            "Epoch 50/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.38073\n",
            "Epoch 51/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.38073\n",
            "Epoch 52/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.38073\n",
            "Epoch 53/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.38073\n",
            "Epoch 54/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.38073\n",
            "Epoch 55/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.38073\n",
            "Epoch 56/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.38073\n",
            "Epoch 57/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.38073\n",
            "Epoch 58/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.38073\n",
            "Epoch 59/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.38073\n",
            "Epoch 60/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.38073\n",
            "Epoch 61/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.38073\n",
            "Epoch 62/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.38073\n",
            "Epoch 63/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.38073\n",
            "Epoch 64/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.38073\n",
            "Epoch 65/1000\n",
            "868/868 [==============================] - 0s 132us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.38073\n",
            "Epoch 66/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.38073\n",
            "Epoch 67/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.38073\n",
            "Epoch 68/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.38073\n",
            "Epoch 69/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.38073\n",
            "Epoch 70/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.38073\n",
            "Epoch 71/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.38073\n",
            "Epoch 72/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.38073\n",
            "Epoch 73/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.38073\n",
            "Epoch 74/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.38073\n",
            "Epoch 75/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.38073\n",
            "Epoch 76/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.38073\n",
            "Epoch 77/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.38073\n",
            "Epoch 78/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.38073\n",
            "Epoch 79/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.38073\n",
            "Epoch 80/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38073\n",
            "Epoch 81/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38073\n",
            "Epoch 82/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.38073\n",
            "Epoch 83/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38073\n",
            "Epoch 84/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.38073\n",
            "Epoch 85/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38073\n",
            "Epoch 86/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38073\n",
            "Epoch 87/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.38073\n",
            "Epoch 88/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.38073\n",
            "Epoch 89/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.38073\n",
            "Epoch 90/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38073\n",
            "Epoch 91/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38073\n",
            "Epoch 92/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38073\n",
            "Epoch 93/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38073\n",
            "Epoch 94/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38073\n",
            "Epoch 95/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.38073\n",
            "Epoch 96/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38073\n",
            "Epoch 97/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38073\n",
            "Epoch 98/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38073\n",
            "Epoch 99/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38073\n",
            "Epoch 100/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38073\n",
            "Epoch 101/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38073\n",
            "Epoch 102/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38073\n",
            "Epoch 103/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38073\n",
            "Epoch 104/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38073\n",
            "Epoch 105/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38073\n",
            "Epoch 106/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.38073\n",
            "Epoch 107/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.38073\n",
            "Epoch 108/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.38073\n",
            "Epoch 109/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.38073\n",
            "Epoch 110/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.38073\n",
            "Epoch 111/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.38073\n",
            "Epoch 112/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.38073\n",
            "Epoch 113/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.38073\n",
            "Epoch 114/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.38073\n",
            "Epoch 115/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.38073\n",
            "Epoch 116/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.38073\n",
            "Epoch 117/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.38073\n",
            "Epoch 118/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.38073\n",
            "Epoch 119/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.38073\n",
            "Epoch 120/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.38073\n",
            "Epoch 121/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.38073\n",
            "Epoch 122/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.38073\n",
            "Epoch 123/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.38073\n",
            "Epoch 124/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.38073\n",
            "Epoch 125/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.38073\n",
            "Epoch 126/1000\n",
            "868/868 [==============================] - 0s 153us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.38073\n",
            "Epoch 127/1000\n",
            "868/868 [==============================] - 0s 149us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.38073\n",
            "Epoch 128/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.38073\n",
            "Epoch 129/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.38073\n",
            "Epoch 130/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.38073\n",
            "Epoch 131/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.38073\n",
            "Epoch 132/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.38073\n",
            "Epoch 133/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.38073\n",
            "Epoch 134/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.38073\n",
            "Epoch 135/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.38073\n",
            "Epoch 136/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.38073\n",
            "Epoch 137/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.38073\n",
            "Epoch 138/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.38073\n",
            "Epoch 139/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.38073\n",
            "Epoch 140/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.38073\n",
            "Epoch 141/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.38073\n",
            "Epoch 142/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.38073\n",
            "Epoch 143/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.38073\n",
            "Epoch 144/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.38073\n",
            "Epoch 145/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.38073\n",
            "Epoch 146/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.38073\n",
            "Epoch 147/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.38073\n",
            "Epoch 148/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.38073\n",
            "Epoch 149/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.38073\n",
            "Epoch 150/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.38073\n",
            "Epoch 151/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.38073\n",
            "Epoch 152/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.38073\n",
            "Epoch 153/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.38073\n",
            "Epoch 154/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.38073\n",
            "Epoch 155/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.38073\n",
            "Epoch 156/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.38073\n",
            "Epoch 157/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.38073\n",
            "Epoch 158/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.38073\n",
            "Epoch 159/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.38073\n",
            "Epoch 160/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.38073\n",
            "Epoch 161/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.38073\n",
            "Epoch 162/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.38073\n",
            "Epoch 163/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.38073\n",
            "Epoch 164/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.38073\n",
            "Epoch 165/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.38073\n",
            "Epoch 166/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.38073\n",
            "Epoch 167/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.38073\n",
            "Epoch 168/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.38073\n",
            "Epoch 169/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.38073\n",
            "Epoch 170/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.38073\n",
            "Epoch 171/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.38073\n",
            "Epoch 172/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.38073\n",
            "Epoch 173/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.38073\n",
            "Epoch 174/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.38073\n",
            "Epoch 175/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.38073\n",
            "Epoch 176/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.38073\n",
            "Epoch 177/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.38073\n",
            "Epoch 178/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.38073\n",
            "Epoch 179/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.38073\n",
            "Epoch 180/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.38073\n",
            "Epoch 181/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.38073\n",
            "Epoch 182/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.38073\n",
            "Epoch 183/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.38073\n",
            "Epoch 184/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.38073\n",
            "Epoch 185/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.38073\n",
            "Epoch 186/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.38073\n",
            "Epoch 187/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.38073\n",
            "Epoch 188/1000\n",
            "868/868 [==============================] - 0s 145us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.38073\n",
            "Epoch 189/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.38073\n",
            "Epoch 190/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.38073\n",
            "Epoch 191/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.38073\n",
            "Epoch 192/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.38073\n",
            "Epoch 193/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.38073\n",
            "Epoch 194/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.38073\n",
            "Epoch 195/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.38073\n",
            "Epoch 196/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.38073\n",
            "Epoch 197/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.38073\n",
            "Epoch 198/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.38073\n",
            "Epoch 199/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.38073\n",
            "Epoch 200/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.38073\n",
            "Epoch 201/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.38073\n",
            "Epoch 202/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.38073\n",
            "Epoch 203/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.38073\n",
            "Epoch 204/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.38073\n",
            "Epoch 205/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.38073\n",
            "Epoch 206/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.38073\n",
            "Epoch 207/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.38073\n",
            "Epoch 208/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.38073\n",
            "Epoch 209/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.38073\n",
            "Epoch 210/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.38073\n",
            "Epoch 211/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.38073\n",
            "Epoch 212/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.38073\n",
            "Epoch 213/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.38073\n",
            "Epoch 214/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.38073\n",
            "Epoch 215/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.38073\n",
            "Epoch 216/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.38073\n",
            "Epoch 217/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.38073\n",
            "Epoch 218/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.38073\n",
            "Epoch 219/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.38073\n",
            "Epoch 220/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.38073\n",
            "Epoch 221/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.38073\n",
            "Epoch 222/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.38073\n",
            "Epoch 223/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.38073\n",
            "Epoch 224/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.38073\n",
            "Epoch 225/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.38073\n",
            "Epoch 226/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.38073\n",
            "Epoch 227/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.38073\n",
            "Epoch 228/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.38073\n",
            "Epoch 229/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.38073\n",
            "Epoch 230/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.38073\n",
            "Epoch 231/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.38073\n",
            "Epoch 232/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.38073\n",
            "Epoch 233/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.38073\n",
            "Epoch 234/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.38073\n",
            "Epoch 235/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.38073\n",
            "Epoch 236/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.38073\n",
            "Epoch 237/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.38073\n",
            "Epoch 238/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.38073\n",
            "Epoch 239/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.38073\n",
            "Epoch 240/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.38073\n",
            "Epoch 241/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.38073\n",
            "Epoch 242/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.38073\n",
            "Epoch 243/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.38073\n",
            "Epoch 244/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.38073\n",
            "Epoch 245/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.38073\n",
            "Epoch 246/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.38073\n",
            "Epoch 247/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.38073\n",
            "Epoch 248/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.38073\n",
            "Epoch 249/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.38073\n",
            "Epoch 250/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.38073\n",
            "Epoch 251/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.38073\n",
            "Epoch 252/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.38073\n",
            "Epoch 253/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.38073\n",
            "Epoch 254/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.38073\n",
            "Epoch 255/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.38073\n",
            "Epoch 256/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.38073\n",
            "Epoch 257/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.38073\n",
            "Epoch 258/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.38073\n",
            "Epoch 259/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.38073\n",
            "Epoch 260/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.38073\n",
            "Epoch 261/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.38073\n",
            "Epoch 262/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.38073\n",
            "Epoch 263/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.38073\n",
            "Epoch 264/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.38073\n",
            "Epoch 265/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.38073\n",
            "Epoch 266/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.38073\n",
            "Epoch 267/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.38073\n",
            "Epoch 268/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.38073\n",
            "Epoch 269/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.38073\n",
            "Epoch 270/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.38073\n",
            "Epoch 271/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.38073\n",
            "Epoch 272/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.38073\n",
            "Epoch 273/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.38073\n",
            "Epoch 274/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.38073\n",
            "Epoch 275/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.38073\n",
            "Epoch 276/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.38073\n",
            "Epoch 277/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.38073\n",
            "Epoch 278/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.38073\n",
            "Epoch 279/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.38073\n",
            "Epoch 280/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.38073\n",
            "Epoch 281/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.38073\n",
            "Epoch 282/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.38073\n",
            "Epoch 283/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.38073\n",
            "Epoch 284/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.38073\n",
            "Epoch 285/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.38073\n",
            "Epoch 286/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.38073\n",
            "Epoch 287/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.38073\n",
            "Epoch 288/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.38073\n",
            "Epoch 289/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.38073\n",
            "Epoch 290/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.38073\n",
            "Epoch 291/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.38073\n",
            "Epoch 292/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.38073\n",
            "Epoch 293/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.38073\n",
            "Epoch 294/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.38073\n",
            "Epoch 295/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.38073\n",
            "Epoch 296/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.38073\n",
            "Epoch 297/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.38073\n",
            "Epoch 298/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.38073\n",
            "Epoch 299/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.38073\n",
            "Epoch 300/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.38073\n",
            "Epoch 301/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.38073\n",
            "Epoch 302/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.38073\n",
            "Epoch 303/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.38073\n",
            "Epoch 304/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.38073\n",
            "Epoch 305/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.38073\n",
            "Epoch 306/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.38073\n",
            "Epoch 307/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.38073\n",
            "Epoch 308/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.38073\n",
            "Epoch 309/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.38073\n",
            "Epoch 310/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.38073\n",
            "Epoch 311/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.38073\n",
            "Epoch 312/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.38073\n",
            "Epoch 313/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.38073\n",
            "Epoch 314/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.38073\n",
            "Epoch 315/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.38073\n",
            "Epoch 316/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.38073\n",
            "Epoch 317/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.38073\n",
            "Epoch 318/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.38073\n",
            "Epoch 319/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.38073\n",
            "Epoch 320/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.38073\n",
            "Epoch 321/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.38073\n",
            "Epoch 322/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.38073\n",
            "Epoch 323/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.38073\n",
            "Epoch 324/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.38073\n",
            "Epoch 325/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.38073\n",
            "Epoch 326/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.38073\n",
            "Epoch 327/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.38073\n",
            "Epoch 328/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.38073\n",
            "Epoch 329/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.38073\n",
            "Epoch 330/1000\n",
            "868/868 [==============================] - 0s 141us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.38073\n",
            "Epoch 331/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.38073\n",
            "Epoch 332/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.38073\n",
            "Epoch 333/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.38073\n",
            "Epoch 334/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.38073\n",
            "Epoch 335/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.38073\n",
            "Epoch 336/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.38073\n",
            "Epoch 337/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.38073\n",
            "Epoch 338/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.38073\n",
            "Epoch 339/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.38073\n",
            "Epoch 340/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.38073\n",
            "Epoch 341/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.38073\n",
            "Epoch 342/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.38073\n",
            "Epoch 343/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.38073\n",
            "Epoch 344/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.38073\n",
            "Epoch 345/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.38073\n",
            "Epoch 346/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.38073\n",
            "Epoch 347/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.38073\n",
            "Epoch 348/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.38073\n",
            "Epoch 349/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.38073\n",
            "Epoch 350/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.38073\n",
            "Epoch 351/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.38073\n",
            "Epoch 352/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.38073\n",
            "Epoch 353/1000\n",
            "868/868 [==============================] - 0s 133us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.38073\n",
            "Epoch 354/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.38073\n",
            "Epoch 355/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.38073\n",
            "Epoch 356/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.38073\n",
            "Epoch 357/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.38073\n",
            "Epoch 358/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.38073\n",
            "Epoch 359/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.38073\n",
            "Epoch 360/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.38073\n",
            "Epoch 361/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.38073\n",
            "Epoch 362/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.38073\n",
            "Epoch 363/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.38073\n",
            "Epoch 364/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.38073\n",
            "Epoch 365/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.38073\n",
            "Epoch 366/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.38073\n",
            "Epoch 367/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.38073\n",
            "Epoch 368/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.38073\n",
            "Epoch 369/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.38073\n",
            "Epoch 370/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.38073\n",
            "Epoch 371/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.38073\n",
            "Epoch 372/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.38073\n",
            "Epoch 373/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.38073\n",
            "Epoch 374/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.38073\n",
            "Epoch 375/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.38073\n",
            "Epoch 376/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.38073\n",
            "Epoch 377/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.38073\n",
            "Epoch 378/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.38073\n",
            "Epoch 379/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.38073\n",
            "Epoch 380/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.38073\n",
            "Epoch 381/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.38073\n",
            "Epoch 382/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.38073\n",
            "Epoch 383/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.38073\n",
            "Epoch 384/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.38073\n",
            "Epoch 385/1000\n",
            "868/868 [==============================] - 0s 135us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.38073\n",
            "Epoch 386/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.38073\n",
            "Epoch 387/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.38073\n",
            "Epoch 388/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.38073\n",
            "Epoch 389/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.38073\n",
            "Epoch 390/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.38073\n",
            "Epoch 391/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.38073\n",
            "Epoch 392/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.38073\n",
            "Epoch 393/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.38073\n",
            "Epoch 394/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.38073\n",
            "Epoch 395/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.38073\n",
            "Epoch 396/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.38073\n",
            "Epoch 397/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.38073\n",
            "Epoch 398/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.38073\n",
            "Epoch 399/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.38073\n",
            "Epoch 400/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.38073\n",
            "Epoch 401/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.38073\n",
            "Epoch 402/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.38073\n",
            "Epoch 403/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.38073\n",
            "Epoch 404/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.38073\n",
            "Epoch 405/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.38073\n",
            "Epoch 406/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.38073\n",
            "Epoch 407/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.38073\n",
            "Epoch 408/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.38073\n",
            "Epoch 409/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.38073\n",
            "Epoch 410/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.38073\n",
            "Epoch 411/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.38073\n",
            "Epoch 412/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.38073\n",
            "Epoch 413/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.38073\n",
            "Epoch 414/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.38073\n",
            "Epoch 415/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.38073\n",
            "Epoch 416/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.38073\n",
            "Epoch 417/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.38073\n",
            "Epoch 418/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.38073\n",
            "Epoch 419/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.38073\n",
            "Epoch 420/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.38073\n",
            "Epoch 421/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.38073\n",
            "Epoch 422/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.38073\n",
            "Epoch 423/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.38073\n",
            "Epoch 424/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.38073\n",
            "Epoch 425/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.38073\n",
            "Epoch 426/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.38073\n",
            "Epoch 427/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.38073\n",
            "Epoch 428/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.38073\n",
            "Epoch 429/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.38073\n",
            "Epoch 430/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.38073\n",
            "Epoch 431/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.38073\n",
            "Epoch 432/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.38073\n",
            "Epoch 433/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.38073\n",
            "Epoch 434/1000\n",
            "868/868 [==============================] - 0s 133us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.38073\n",
            "Epoch 435/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.38073\n",
            "Epoch 436/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.38073\n",
            "Epoch 437/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.38073\n",
            "Epoch 438/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.38073\n",
            "Epoch 439/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.38073\n",
            "Epoch 440/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.38073\n",
            "Epoch 441/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.38073\n",
            "Epoch 442/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.38073\n",
            "Epoch 443/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.38073\n",
            "Epoch 444/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.38073\n",
            "Epoch 445/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.38073\n",
            "Epoch 446/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.38073\n",
            "Epoch 447/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.38073\n",
            "Epoch 448/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.38073\n",
            "Epoch 449/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.38073\n",
            "Epoch 450/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.38073\n",
            "Epoch 451/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.38073\n",
            "Epoch 452/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.38073\n",
            "Epoch 453/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.38073\n",
            "Epoch 454/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.38073\n",
            "Epoch 455/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.38073\n",
            "Epoch 456/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.38073\n",
            "Epoch 457/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.38073\n",
            "Epoch 458/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.38073\n",
            "Epoch 459/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.38073\n",
            "Epoch 460/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.38073\n",
            "Epoch 461/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.38073\n",
            "Epoch 462/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.38073\n",
            "Epoch 463/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.38073\n",
            "Epoch 464/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.38073\n",
            "Epoch 465/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.38073\n",
            "Epoch 466/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.38073\n",
            "Epoch 467/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.38073\n",
            "Epoch 468/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.38073\n",
            "Epoch 469/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.38073\n",
            "Epoch 470/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.38073\n",
            "Epoch 471/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.38073\n",
            "Epoch 472/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.38073\n",
            "Epoch 473/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.38073\n",
            "Epoch 474/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.38073\n",
            "Epoch 475/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.38073\n",
            "Epoch 476/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.38073\n",
            "Epoch 477/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.38073\n",
            "Epoch 478/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.38073\n",
            "Epoch 479/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.38073\n",
            "Epoch 480/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.38073\n",
            "Epoch 481/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.38073\n",
            "Epoch 482/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.38073\n",
            "Epoch 483/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.38073\n",
            "Epoch 484/1000\n",
            "868/868 [==============================] - 0s 133us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.38073\n",
            "Epoch 485/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.38073\n",
            "Epoch 486/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.38073\n",
            "Epoch 487/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.38073\n",
            "Epoch 488/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.38073\n",
            "Epoch 489/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.38073\n",
            "Epoch 490/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.38073\n",
            "Epoch 491/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.38073\n",
            "Epoch 492/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.38073\n",
            "Epoch 493/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.38073\n",
            "Epoch 494/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.38073\n",
            "Epoch 495/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.38073\n",
            "Epoch 496/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.38073\n",
            "Epoch 497/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.38073\n",
            "Epoch 498/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.38073\n",
            "Epoch 499/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.38073\n",
            "Epoch 500/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.38073\n",
            "Epoch 501/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.38073\n",
            "Epoch 502/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.38073\n",
            "Epoch 503/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.38073\n",
            "Epoch 504/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.38073\n",
            "Epoch 505/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.38073\n",
            "Epoch 506/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.38073\n",
            "Epoch 507/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.38073\n",
            "Epoch 508/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.38073\n",
            "Epoch 509/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.38073\n",
            "Epoch 510/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.38073\n",
            "Epoch 511/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.38073\n",
            "Epoch 512/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.38073\n",
            "Epoch 513/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.38073\n",
            "Epoch 514/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.38073\n",
            "Epoch 515/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.38073\n",
            "Epoch 516/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.38073\n",
            "Epoch 517/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.38073\n",
            "Epoch 518/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.38073\n",
            "Epoch 519/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.38073\n",
            "Epoch 520/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.38073\n",
            "Epoch 521/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.38073\n",
            "Epoch 522/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.38073\n",
            "Epoch 523/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.38073\n",
            "Epoch 524/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.38073\n",
            "Epoch 525/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.38073\n",
            "Epoch 526/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.38073\n",
            "Epoch 527/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.38073\n",
            "Epoch 528/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.38073\n",
            "Epoch 529/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.38073\n",
            "Epoch 530/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.38073\n",
            "Epoch 531/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.38073\n",
            "Epoch 532/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.38073\n",
            "Epoch 533/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.38073\n",
            "Epoch 534/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.38073\n",
            "Epoch 535/1000\n",
            "868/868 [==============================] - 0s 170us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.38073\n",
            "Epoch 536/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.38073\n",
            "Epoch 537/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.38073\n",
            "Epoch 538/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.38073\n",
            "Epoch 539/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.38073\n",
            "Epoch 540/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.38073\n",
            "Epoch 541/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.38073\n",
            "Epoch 542/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.38073\n",
            "Epoch 543/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.38073\n",
            "Epoch 544/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.38073\n",
            "Epoch 545/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.38073\n",
            "Epoch 546/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.38073\n",
            "Epoch 547/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.38073\n",
            "Epoch 548/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.38073\n",
            "Epoch 549/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.38073\n",
            "Epoch 550/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.38073\n",
            "Epoch 551/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.38073\n",
            "Epoch 552/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.38073\n",
            "Epoch 553/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.38073\n",
            "Epoch 554/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.38073\n",
            "Epoch 555/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.38073\n",
            "Epoch 556/1000\n",
            "868/868 [==============================] - 0s 137us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.38073\n",
            "Epoch 557/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.38073\n",
            "Epoch 558/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.38073\n",
            "Epoch 559/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.38073\n",
            "Epoch 560/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.38073\n",
            "Epoch 561/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.38073\n",
            "Epoch 562/1000\n",
            "868/868 [==============================] - 0s 131us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.38073\n",
            "Epoch 563/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.38073\n",
            "Epoch 564/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.38073\n",
            "Epoch 565/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.38073\n",
            "Epoch 566/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.38073\n",
            "Epoch 567/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.38073\n",
            "Epoch 568/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.38073\n",
            "Epoch 569/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.38073\n",
            "Epoch 570/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.38073\n",
            "Epoch 571/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.38073\n",
            "Epoch 572/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.38073\n",
            "Epoch 573/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.38073\n",
            "Epoch 574/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.38073\n",
            "Epoch 575/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.38073\n",
            "Epoch 576/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.38073\n",
            "Epoch 577/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.38073\n",
            "Epoch 578/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.38073\n",
            "Epoch 579/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.38073\n",
            "Epoch 580/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.38073\n",
            "Epoch 581/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.38073\n",
            "Epoch 582/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.38073\n",
            "Epoch 583/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.38073\n",
            "Epoch 584/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.38073\n",
            "Epoch 585/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.38073\n",
            "Epoch 586/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.38073\n",
            "Epoch 587/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.38073\n",
            "Epoch 588/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.38073\n",
            "Epoch 589/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.38073\n",
            "Epoch 590/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.38073\n",
            "Epoch 591/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.38073\n",
            "Epoch 592/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.38073\n",
            "Epoch 593/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.38073\n",
            "Epoch 594/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.38073\n",
            "Epoch 595/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.38073\n",
            "Epoch 596/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.38073\n",
            "Epoch 597/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.38073\n",
            "Epoch 598/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.38073\n",
            "Epoch 599/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.38073\n",
            "Epoch 600/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.38073\n",
            "Epoch 601/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.38073\n",
            "Epoch 602/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.38073\n",
            "Epoch 603/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.38073\n",
            "Epoch 604/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.38073\n",
            "Epoch 605/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.38073\n",
            "Epoch 606/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.38073\n",
            "Epoch 607/1000\n",
            "868/868 [==============================] - 0s 138us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.38073\n",
            "Epoch 608/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.38073\n",
            "Epoch 609/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.38073\n",
            "Epoch 610/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.38073\n",
            "Epoch 611/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.38073\n",
            "Epoch 612/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.38073\n",
            "Epoch 613/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.38073\n",
            "Epoch 614/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.38073\n",
            "Epoch 615/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.38073\n",
            "Epoch 616/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.38073\n",
            "Epoch 617/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.38073\n",
            "Epoch 618/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.38073\n",
            "Epoch 619/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.38073\n",
            "Epoch 620/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.38073\n",
            "Epoch 621/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.38073\n",
            "Epoch 622/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.38073\n",
            "Epoch 623/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.38073\n",
            "Epoch 624/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.38073\n",
            "Epoch 625/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.38073\n",
            "Epoch 626/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.38073\n",
            "Epoch 627/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.38073\n",
            "Epoch 628/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.38073\n",
            "Epoch 629/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.38073\n",
            "Epoch 630/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.38073\n",
            "Epoch 631/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.38073\n",
            "Epoch 632/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.38073\n",
            "Epoch 633/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.38073\n",
            "Epoch 634/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.38073\n",
            "Epoch 635/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.38073\n",
            "Epoch 636/1000\n",
            "868/868 [==============================] - 0s 139us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.38073\n",
            "Epoch 637/1000\n",
            "868/868 [==============================] - 0s 138us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.38073\n",
            "Epoch 638/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.38073\n",
            "Epoch 639/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.38073\n",
            "Epoch 640/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.38073\n",
            "Epoch 641/1000\n",
            "868/868 [==============================] - 0s 133us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.38073\n",
            "Epoch 642/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.38073\n",
            "Epoch 643/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.38073\n",
            "Epoch 644/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.38073\n",
            "Epoch 645/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.38073\n",
            "Epoch 646/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.38073\n",
            "Epoch 647/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.38073\n",
            "Epoch 648/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.38073\n",
            "Epoch 649/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.38073\n",
            "Epoch 650/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.38073\n",
            "Epoch 651/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.38073\n",
            "Epoch 652/1000\n",
            "868/868 [==============================] - 0s 136us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.38073\n",
            "Epoch 653/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.38073\n",
            "Epoch 654/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.38073\n",
            "Epoch 655/1000\n",
            "868/868 [==============================] - 0s 139us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.38073\n",
            "Epoch 656/1000\n",
            "868/868 [==============================] - 0s 138us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.38073\n",
            "Epoch 657/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.38073\n",
            "Epoch 658/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.38073\n",
            "Epoch 659/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.38073\n",
            "Epoch 660/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.38073\n",
            "Epoch 661/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.38073\n",
            "Epoch 662/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.38073\n",
            "Epoch 663/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.38073\n",
            "Epoch 664/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.38073\n",
            "Epoch 665/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.38073\n",
            "Epoch 666/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.38073\n",
            "Epoch 667/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.38073\n",
            "Epoch 668/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.38073\n",
            "Epoch 669/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.38073\n",
            "Epoch 670/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.38073\n",
            "Epoch 671/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.38073\n",
            "Epoch 672/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.38073\n",
            "Epoch 673/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.38073\n",
            "Epoch 674/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.38073\n",
            "Epoch 675/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.38073\n",
            "Epoch 676/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.38073\n",
            "Epoch 677/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.38073\n",
            "Epoch 678/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.38073\n",
            "Epoch 679/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.38073\n",
            "Epoch 680/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.38073\n",
            "Epoch 681/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.38073\n",
            "Epoch 682/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.38073\n",
            "Epoch 683/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.38073\n",
            "Epoch 684/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.38073\n",
            "Epoch 685/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.38073\n",
            "Epoch 686/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.38073\n",
            "Epoch 687/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.38073\n",
            "Epoch 688/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.38073\n",
            "Epoch 689/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.38073\n",
            "Epoch 690/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.38073\n",
            "Epoch 691/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.38073\n",
            "Epoch 692/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.38073\n",
            "Epoch 693/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.38073\n",
            "Epoch 694/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.38073\n",
            "Epoch 695/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.38073\n",
            "Epoch 696/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.38073\n",
            "Epoch 697/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.38073\n",
            "Epoch 698/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.38073\n",
            "Epoch 699/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.38073\n",
            "Epoch 700/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.38073\n",
            "Epoch 701/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.38073\n",
            "Epoch 702/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.38073\n",
            "Epoch 703/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.38073\n",
            "Epoch 704/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.38073\n",
            "Epoch 705/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.38073\n",
            "Epoch 706/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.38073\n",
            "Epoch 707/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.38073\n",
            "Epoch 708/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.38073\n",
            "Epoch 709/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.38073\n",
            "Epoch 710/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.38073\n",
            "Epoch 711/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.38073\n",
            "Epoch 712/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.38073\n",
            "Epoch 713/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.38073\n",
            "Epoch 714/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.38073\n",
            "Epoch 715/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.38073\n",
            "Epoch 716/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.38073\n",
            "Epoch 717/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.38073\n",
            "Epoch 718/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.38073\n",
            "Epoch 719/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.38073\n",
            "Epoch 720/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.38073\n",
            "Epoch 721/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.38073\n",
            "Epoch 722/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.38073\n",
            "Epoch 723/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.38073\n",
            "Epoch 724/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.38073\n",
            "Epoch 725/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.38073\n",
            "Epoch 726/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.38073\n",
            "Epoch 727/1000\n",
            "868/868 [==============================] - 0s 97us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.38073\n",
            "Epoch 728/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.38073\n",
            "Epoch 729/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.38073\n",
            "Epoch 730/1000\n",
            "868/868 [==============================] - 0s 99us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.38073\n",
            "Epoch 731/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.38073\n",
            "Epoch 732/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.38073\n",
            "Epoch 733/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.38073\n",
            "Epoch 734/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.38073\n",
            "Epoch 735/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.38073\n",
            "Epoch 736/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.38073\n",
            "Epoch 737/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.38073\n",
            "Epoch 738/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.38073\n",
            "Epoch 739/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.38073\n",
            "Epoch 740/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.38073\n",
            "Epoch 741/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.38073\n",
            "Epoch 742/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.38073\n",
            "Epoch 743/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.38073\n",
            "Epoch 744/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.38073\n",
            "Epoch 745/1000\n",
            "868/868 [==============================] - 0s 101us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.38073\n",
            "Epoch 746/1000\n",
            "868/868 [==============================] - 0s 101us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.38073\n",
            "Epoch 747/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.38073\n",
            "Epoch 748/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.38073\n",
            "Epoch 749/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.38073\n",
            "Epoch 750/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.38073\n",
            "Epoch 751/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.38073\n",
            "Epoch 752/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.38073\n",
            "Epoch 753/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.38073\n",
            "Epoch 754/1000\n",
            "868/868 [==============================] - 0s 101us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.38073\n",
            "Epoch 755/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.38073\n",
            "Epoch 756/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.38073\n",
            "Epoch 757/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.38073\n",
            "Epoch 758/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.38073\n",
            "Epoch 759/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.38073\n",
            "Epoch 760/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.38073\n",
            "Epoch 761/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.38073\n",
            "Epoch 762/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.38073\n",
            "Epoch 763/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.38073\n",
            "Epoch 764/1000\n",
            "868/868 [==============================] - 0s 102us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.38073\n",
            "Epoch 765/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.38073\n",
            "Epoch 766/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.38073\n",
            "Epoch 767/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.38073\n",
            "Epoch 768/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.38073\n",
            "Epoch 769/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.38073\n",
            "Epoch 770/1000\n",
            "868/868 [==============================] - 0s 101us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.38073\n",
            "Epoch 771/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.38073\n",
            "Epoch 772/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.38073\n",
            "Epoch 773/1000\n",
            "868/868 [==============================] - 0s 99us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.38073\n",
            "Epoch 774/1000\n",
            "868/868 [==============================] - 0s 99us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.38073\n",
            "Epoch 775/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.38073\n",
            "Epoch 776/1000\n",
            "868/868 [==============================] - 0s 100us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.38073\n",
            "Epoch 777/1000\n",
            "868/868 [==============================] - 0s 98us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 0.3807 - val_mean_absolute_error: 0.3807\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.38073\n",
            "Epoch 778/1000\n",
            "570/868 [==================>...........] - ETA: 0s - loss: 0.3509 - mean_absolute_error: 0.3509"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mUOEtB2dac"
      },
      "source": [
        "#NN_model.load_weights('weights.best.cnn.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi2v5_e2u3tt"
      },
      "source": [
        "#wights_file = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "#NN_model.load_weights(wights_file) \n",
        "#NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLFsCAkUu3oS"
      },
      "source": [
        "DL_pred = model.predict(X_test)\n",
        "\n",
        "#Array of list of list to Array of list\n",
        "DLPred = []\n",
        "for i in DL_pred:\n",
        "  for k in i:\n",
        "    DLPred.append(int(k))\n",
        "  \n",
        "DLPred = np.array(DLPred) \n",
        "\n",
        "#PRINT\n",
        "#Y_test, Y_new\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, DLPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxsi0GnLrP3d"
      },
      "source": [
        "print(\"Accuracy score %f\" % accuracy_score(Y_test, DLPred))\n",
        "print(classification_report(Y_test, DLPred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKjPAUSOz-9"
      },
      "source": [
        "NN_model.fit(X_test, Y_test, epochs=1000, batch_size=30, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjNlpIttOz7j"
      },
      "source": [
        "DLPred = model.predict(XTest)\n",
        "DL_Pred = []\n",
        "for i in DLPred:\n",
        "  for k in i:\n",
        "    DL_Pred.append(int(k))\n",
        "  \n",
        "DLPred = np.array(DL_Pred) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jELkiR-fXZiD"
      },
      "source": [
        "EVALUATION['DL'] = list(DLPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBAicBswr9C9"
      },
      "source": [
        "#EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4wfzemNr9R-"
      },
      "source": [
        "pd.DataFrame(EVALUATION)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}