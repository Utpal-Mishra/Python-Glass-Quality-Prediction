{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GlassQualityPredictionMutation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cZJ-h-LSye6I",
        "XZF_bu1byp_7",
        "JhNKDyAO1MiO",
        "ni0WiBNW1VdB",
        "S7qiblRq15S8",
        "zFcVKYY7V715",
        "Ltwo4bpdfUJB",
        "nZyd_Hfz2oLo",
        "Mzr01v1ThHp3",
        "MNRXrNXEH6wv",
        "o2NV_zuNP3vE",
        "HWm6WYhm4Fgj",
        "b2qB1D_KtmhL"
      ],
      "mount_file_id": "1qrqGvtw7q2n9KAsxJJsynyOQQJFyub0S",
      "authorship_tag": "ABX9TyPpXFvan3Yew4goe7KCFDM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utpal-Mishra/Python-Glass-Quality-Prediction/blob/main/GlassQualityPredictionMutation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks43SbQdy4PG"
      },
      "source": [
        "#INSTALLING PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opX7tM8zvePJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "8ebf481b-4b0e-4a5e-c9dc-5404d348f09b"
      },
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install -q tensorflow==2.0beta1\n",
        "!pip install catboost\n",
        "!pip install lightgbm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 45kB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 48.1MB/s \n",
            "\u001b[?25hCollecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/aa/e61819d04ef2bbee778bf4b3a748db1f3ad23512377e43ecfdc3211437a0/catboost-0.23.2-cp36-none-manylinux1_x86_64.whl (64.8MB)\n",
            "\u001b[K     |████████████████████████████████| 64.8MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.23.2\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zll5E-oEyKZC"
      },
      "source": [
        "#IMPORTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWc9WHyryF8w"
      },
      "source": [
        "###LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDVbe1u4wkpq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dense, Activation, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6A4uWh7A26m"
      },
      "source": [
        "###MOUNTING DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFd0sGgNIlUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc226af2-de51-4ba5-ffc3-1b9d6f41ca31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAR_wjW_A6i4"
      },
      "source": [
        "###IMPORT TRAINING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciXzTV1lVO4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7091ae3c-cba7-4bf4-b132-c6bb2f83aa08"
      },
      "source": [
        "path = \"/content/drive/My Drive/Glass Quality Prediction/Train.csv\"\n",
        "train = pd.read_csv(path)\n",
        "print(train.shape)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1358, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>36</td>\n",
              "      <td>144</td>\n",
              "      <td>172</td>\n",
              "      <td>947225</td>\n",
              "      <td>947332</td>\n",
              "      <td>439</td>\n",
              "      <td>439.09927</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>55</td>\n",
              "      <td>1144</td>\n",
              "      <td>1152</td>\n",
              "      <td>2379058</td>\n",
              "      <td>2379624</td>\n",
              "      <td>329</td>\n",
              "      <td>329.20562</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135</td>\n",
              "      <td>65</td>\n",
              "      <td>950</td>\n",
              "      <td>974</td>\n",
              "      <td>1038442</td>\n",
              "      <td>1036754</td>\n",
              "      <td>300</td>\n",
              "      <td>300.12060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>123</td>\n",
              "      <td>35</td>\n",
              "      <td>41</td>\n",
              "      <td>220</td>\n",
              "      <td>1705580</td>\n",
              "      <td>1705604</td>\n",
              "      <td>6803</td>\n",
              "      <td>6803.77862</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>69</td>\n",
              "      <td>462</td>\n",
              "      <td>466</td>\n",
              "      <td>1088124</td>\n",
              "      <td>1086579</td>\n",
              "      <td>251</td>\n",
              "      <td>251.40194</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grade_A_Component_1  grade_A_Component_2  ...  x_component_5  class\n",
              "0                    0                    1  ...              0      1\n",
              "1                    1                    0  ...              0      1\n",
              "2                    1                    0  ...              0      2\n",
              "3                    0                    1  ...              0      1\n",
              "4                    1                    0  ...              0      2\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4bDWVO9Cb2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a9d391-cd30-4027-c824-4dd3e529966a"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1.358000e+03</td>\n",
              "      <td>1.358000e+03</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "      <td>1358.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.406480</td>\n",
              "      <td>0.593520</td>\n",
              "      <td>137.893225</td>\n",
              "      <td>78.977909</td>\n",
              "      <td>567.370398</td>\n",
              "      <td>614.032401</td>\n",
              "      <td>1.660107e+06</td>\n",
              "      <td>1.660139e+06</td>\n",
              "      <td>1903.402798</td>\n",
              "      <td>1903.896240</td>\n",
              "      <td>0.081738</td>\n",
              "      <td>0.106038</td>\n",
              "      <td>0.197349</td>\n",
              "      <td>0.035346</td>\n",
              "      <td>0.027982</td>\n",
              "      <td>1.346834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.491357</td>\n",
              "      <td>0.491357</td>\n",
              "      <td>20.480512</td>\n",
              "      <td>55.324842</td>\n",
              "      <td>522.013094</td>\n",
              "      <td>500.505513</td>\n",
              "      <td>1.778153e+06</td>\n",
              "      <td>1.778177e+06</td>\n",
              "      <td>3839.156721</td>\n",
              "      <td>3839.163241</td>\n",
              "      <td>0.274066</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>0.398145</td>\n",
              "      <td>0.184721</td>\n",
              "      <td>0.164983</td>\n",
              "      <td>0.476138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.527000e+03</td>\n",
              "      <td>7.453000e+03</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.445290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>4.662742e+05</td>\n",
              "      <td>4.666918e+05</td>\n",
              "      <td>234.000000</td>\n",
              "      <td>234.335953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>407.500000</td>\n",
              "      <td>457.000000</td>\n",
              "      <td>1.216168e+06</td>\n",
              "      <td>1.214700e+06</td>\n",
              "      <td>346.000000</td>\n",
              "      <td>346.044490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>1041.750000</td>\n",
              "      <td>1064.000000</td>\n",
              "      <td>2.210012e+06</td>\n",
              "      <td>2.210076e+06</td>\n",
              "      <td>915.250000</td>\n",
              "      <td>915.367815</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>305.000000</td>\n",
              "      <td>1692.000000</td>\n",
              "      <td>1717.000000</td>\n",
              "      <td>1.291748e+07</td>\n",
              "      <td>1.291731e+07</td>\n",
              "      <td>37392.000000</td>\n",
              "      <td>37392.672970</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       grade_A_Component_1  grade_A_Component_2  ...  x_component_5        class\n",
              "count          1358.000000          1358.000000  ...    1358.000000  1358.000000\n",
              "mean              0.406480             0.593520  ...       0.027982     1.346834\n",
              "std               0.491357             0.491357  ...       0.164983     0.476138\n",
              "min               0.000000             0.000000  ...       0.000000     1.000000\n",
              "25%               0.000000             0.000000  ...       0.000000     1.000000\n",
              "50%               0.000000             1.000000  ...       0.000000     1.000000\n",
              "75%               1.000000             1.000000  ...       0.000000     2.000000\n",
              "max               1.000000             1.000000  ...       1.000000     2.000000\n",
              "\n",
              "[8 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwtT91wkA_lO"
      },
      "source": [
        "###IMPORT TESTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqMzhQKLBC_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81867ff9-096d-44e1-d83c-8764655d9844"
      },
      "source": [
        "path = \"/content/drive/My Drive/Glass Quality Prediction/Test.csv\"\n",
        "test = pd.read_csv(path)\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(583, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A_Component_1</th>\n",
              "      <th>grade_A_Component_2</th>\n",
              "      <th>max_luminosity</th>\n",
              "      <th>thickness</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>pixel_area</th>\n",
              "      <th>log_area</th>\n",
              "      <th>x_component_1</th>\n",
              "      <th>x_component_2</th>\n",
              "      <th>x_component_3</th>\n",
              "      <th>x_component_4</th>\n",
              "      <th>x_component_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>65</td>\n",
              "      <td>33</td>\n",
              "      <td>54</td>\n",
              "      <td>1646439</td>\n",
              "      <td>1646893</td>\n",
              "      <td>632</td>\n",
              "      <td>632.39175</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>99</td>\n",
              "      <td>125</td>\n",
              "      <td>132</td>\n",
              "      <td>189874</td>\n",
              "      <td>189529</td>\n",
              "      <td>421</td>\n",
              "      <td>421.92861</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>75</td>\n",
              "      <td>12986873</td>\n",
              "      <td>12986862</td>\n",
              "      <td>272</td>\n",
              "      <td>272.21221</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>39</td>\n",
              "      <td>57</td>\n",
              "      <td>177</td>\n",
              "      <td>309634</td>\n",
              "      <td>310824</td>\n",
              "      <td>3312</td>\n",
              "      <td>3312.31058</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>142</td>\n",
              "      <td>75</td>\n",
              "      <td>23</td>\n",
              "      <td>79</td>\n",
              "      <td>5368307</td>\n",
              "      <td>5367467</td>\n",
              "      <td>862</td>\n",
              "      <td>862.49918</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   grade_A_Component_1  grade_A_Component_2  ...  x_component_4  x_component_5\n",
              "0                    1                    0  ...              0              0\n",
              "1                    0                    1  ...              0              0\n",
              "2                    0                    1  ...              0              0\n",
              "3                    0                    1  ...              0              0\n",
              "4                    1                    0  ...              0              0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXhEtxJcRMYh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpUbv4CyOu8"
      },
      "source": [
        "#DATA VISUALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y91jBSk0B0OK"
      },
      "source": [
        "###TRAINING DATA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptVLVHiECEhd"
      },
      "source": [
        "NULL VALUES CHECK UP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz5ZCQtZXMbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940df5ce-82ef-47af-e044-bce89a154842"
      },
      "source": [
        "train.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLQZDFxMB2R0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87112363-39b2-4eb4-d54d-9b51627efc4a"
      },
      "source": [
        "train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN3KX3HOaqEo"
      },
      "source": [
        "#train.fillna(0, inplace = True)\n",
        "#train.isnull().values.any()\n",
        "#train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aTNXViZEbLZ"
      },
      "source": [
        "CLASS LABEL ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-cyxkE4CjWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df63bad1-3b6a-40bc-9436-f3a8063d5f4b"
      },
      "source": [
        "train['class'].plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16314b3f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-g3FwtbCjaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e518428-5511-4bc0-b0ab-89c27e0f142a"
      },
      "source": [
        "train['class'].plot.density()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f16356bdf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmRpLlR9CjUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5e7201-b57a-46ce-bd12-737a3ca352b7"
      },
      "source": [
        "train['class'].plot.bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1631573320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAt3QUAjCqkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62827ee-671c-4b51-d183-ae0de3d9c0ce"
      },
      "source": [
        "train['class'].plot.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f163083db00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SLtu_z0EqT3"
      },
      "source": [
        "CLASS LABEL VALUE COUNT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWZbXNknfle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5014ea5a-5849-4e3b-97e1-6188f81c8a9f"
      },
      "source": [
        "train['class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    887\n",
              "2    471\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWSglRRDkbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f966794-f40b-4bd7-8337-ffdd330ac25c"
      },
      "source": [
        "train['class'].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f162fc5c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Ovar5FE03B"
      },
      "source": [
        "TRAINING CORRELATION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZe7frLpxTzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f276a7f-da83-4a0c-c945-79d80b7a621d"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "cmap = cm.get_cmap('jet', 30)\n",
        "cax = ax1.imshow(train.corr(), interpolation=\"none\", cmap=cmap)\n",
        "ax1.grid(True)\n",
        "plt.title('Glass Quality Attributes Correlation')\n",
        "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcmADmiAIvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1795f2a9-54eb-4ce3-f3fd-4cdf422379db"
      },
      "source": [
        "corr = train.corr()\n",
        "ax = sns.heatmap(corr, vmin =- 1, vmax = 1, center = 0, \n",
        "                  cmap = sns.diverging_palette(20, 220, n=200),\n",
        "                  square = True)\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right'\n",
        ");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbhmQ61-FA1-"
      },
      "source": [
        "###TESTING DATA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CZkiilRFA2C"
      },
      "source": [
        "NULL VALUES CHECK UP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwJt4YiIFA2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f767d07-c6be-4ddd-cdc6-31fd566ecc6b"
      },
      "source": [
        "test.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6l6jBfNFA2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb44322-2b73-4ee0-9056-f8771e65008b"
      },
      "source": [
        "test.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0UflS-nFA2g"
      },
      "source": [
        "#test.fillna(0, inplace = True)\n",
        "#test.isnull().values.any()\n",
        "#test.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMQf9YHPFA2-"
      },
      "source": [
        "TESTING CORRELATION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKg5FKzHFA2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eff9322-11f2-4868-d797-75a6db3bff62"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "cmap = cm.get_cmap('jet', 30)\n",
        "cax = ax1.imshow(test.corr(), interpolation=\"none\", cmap=cmap)\n",
        "ax1.grid(True)\n",
        "plt.title('Glass Quality Attributes Correlation')\n",
        "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCyXgoGqFA3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40795aa9-417d-47f2-b510-6dc242198ba7"
      },
      "source": [
        "corr = test.corr()\n",
        "ax = sns.heatmap(corr, vmin =- 1, vmax = 1, center = 0, \n",
        "                  cmap = sns.diverging_palette(20, 220, n=200),\n",
        "                  square = True)\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation = 45,\n",
        "    horizontalalignment = 'right'\n",
        ");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgiZAKP9yUjf"
      },
      "source": [
        "#DATA SPLITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf_EXK5qJ6nk"
      },
      "source": [
        "TRAINING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmcNojwbxT3E"
      },
      "source": [
        "Y = train['class'].values\n",
        "X = train.drop('class', axis=1).values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zdJM5bICdao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2009e3-818c-4817-f35f-7c7cd7659b68"
      },
      "source": [
        "print(\"X_train.shape: \",X_train.shape, \"X_test.shape: \",X_test.shape,\"Y_train.shape: \", Y_train.shape, \"Y_test.shape: \",Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (1086, 15) X_test.shape:  (272, 15) Y_train.shape:  (1086,) Y_test.shape:  (272,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgwJmTVl8z6P"
      },
      "source": [
        "TESTING DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZOgMAkN80UL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec2b734-0418-46b3-9714-c50aac85eb03"
      },
      "source": [
        "XTest = test\n",
        "XTest.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(583, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzn-YoUq772k"
      },
      "source": [
        "#MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmvE41B_NKUW"
      },
      "source": [
        "EVALUATION = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJ-h-LSye6I"
      },
      "source": [
        "###ENSEMBLE MODELLING USING CROSS VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5KMllIxxT5o"
      },
      "source": [
        "models_list = []\n",
        "models_list.append(('KNN', KNeighborsClassifier()))\n",
        "models_list.append(('SVM', SVC(C=1.7, kernel='rbf'))) \n",
        "models_list.append(('NB', GaussianNB()))\n",
        "models_list.append(('DT', DecisionTreeClassifier(criterion='entropy')))\n",
        "models_list.append(('BAG', BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)))\n",
        "models_list.append(('ET', ExtraTreesClassifier(n_estimators=10, max_depth= None, min_samples_split=2)))\n",
        "models_list.append(('RF', RandomForestClassifier(criterion='entropy')))\n",
        "models_list.append(('GB', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)))\n",
        "models_list.append(('GB', XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)))\n",
        "models_list.append(('ADB', AdaBoostClassifier()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wS-9xSgxTxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a938501b-0984-4e38-ecfe-cdbb047f036f"
      },
      "source": [
        "num_folds = 10\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "for name, model in models_list:\n",
        "    kfold = KFold(n_splits=num_folds)\n",
        "    start = time.time()\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    end = time.time()\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN: 0.569971 (0.062213) (run time: 0.073188)\n",
            "SVM: 0.656566 (0.050807) (run time: 0.413186)\n",
            "NB: 0.519334 (0.036962) (run time: 0.013205)\n",
            "DT: 0.840741 (0.019902) (run time: 0.113559)\n",
            "BAG: 0.780878 (0.041058) (run time: 0.261532)\n",
            "ET: 0.836069 (0.038014) (run time: 0.152795)\n",
            "RF: 0.851741 (0.023563) (run time: 2.939437)\n",
            "GB: 0.833325 (0.041117) (run time: 1.127123)\n",
            "GB: 0.846160 (0.029811) (run time: 0.860486)\n",
            "ADB: 0.834293 (0.044211) (run time: 1.120406)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNeiWBJyxTvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b7a797-994f-468a-dc39-ff0c569aba80"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Performance Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZF_bu1byp_7"
      },
      "source": [
        "PIPELINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUhaIHb2xTth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "0f72e467-12ec-4e38-8483-6750c55c2f31"
      },
      "source": [
        "# Standardize the dataset\n",
        "pipelines = []\n",
        "\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
        "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(C=1.7, kernel='rbf'))])))\n",
        "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
        "pipelines.append(('ScaledDT', Pipeline([('Scaler', StandardScaler()),('DT', DecisionTreeClassifier(criterion='entropy'))])))\n",
        "pipelines.append(('ScaledBAG', Pipeline([('Scaler', StandardScaler()),('BAG', BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5))])))\n",
        "pipelines.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2))])))\n",
        "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier(criterion='entropy'))])))\n",
        "pipelines.append(('ScaledGB', Pipeline([('Scaler', StandardScaler()),('GB', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1))])))\n",
        "pipelines.append(('ScaledXGB', Pipeline([('Scaler', StandardScaler()),('GB', XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75))])))\n",
        "pipelines.append(('ScaledADB', Pipeline([('Scaler', StandardScaler()),('ADB', AdaBoostClassifier())])))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    kfold = KFold(n_splits=num_folds, random_state=123)\n",
        "    for name, model in pipelines:\n",
        "        start = time.time()\n",
        "        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "        end = time.time()\n",
        "        results.append(cv_results)\n",
        "        names.append(name)\n",
        "        print( \"%s: %f (%f) (run time: %f)\" % (name, cv_results.mean(), cv_results.std(), end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ScaledKNN: 0.816709 (0.031190) (run time: 0.076856)\n",
            "ScaledSVC: 0.843417 (0.020521) (run time: 0.273048)\n",
            "ScaledNB: 0.790868 (0.034386) (run time: 0.021367)\n",
            "ScaledDT: 0.836103 (0.018727) (run time: 0.138427)\n",
            "ScaledBAG: 0.792847 (0.041766) (run time: 0.265263)\n",
            "ScaledET: 0.850799 (0.026818) (run time: 0.170883)\n",
            "ScaledRF: 0.845311 (0.021720) (run time: 2.947330)\n",
            "ScaledGB: 0.832407 (0.040349) (run time: 1.146342)\n",
            "ScaledXGB: 0.847078 (0.030979) (run time: 0.860056)\n",
            "ScaledADB: 0.834293 (0.044211) (run time: 1.135983)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzzZxMIOyHAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab593ca8-116c-4e07-d17f-1502ffc6ed17"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.suptitle('Performance Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXyGIRYCpwW5"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv5XDnk8pwW6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "473bb17a-e44f-49b4-d94a-16f34383c847"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
        "model = DecisionTreeClassifier()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.705 (0.044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsCz4zCVpwW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9e73dc-be42-4dae-dd18-c5ad57c02c1d"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7022058823529411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbE5pJLFpwW_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "ad6f9029-70c0-4686-8a16-2a1b740b395e"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = DecisionTreeClassifier()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.627 (Time : 0.059)\n",
            ">3 : Features :: Accuracy : 0.683 (Time : 0.049)\n",
            ">4 : Features :: Accuracy : 0.689 (Time : 0.052)\n",
            ">5 : Features :: Accuracy : 0.703 (Time : 0.049)\n",
            ">6 : Features :: Accuracy : 0.720 (Time : 0.040)\n",
            ">7 : Features :: Accuracy : 0.765 (Time : 0.053)\n",
            ">8 : Features :: Accuracy : 0.792 (Time : 0.035)\n",
            ">9 : Features :: Accuracy : 0.793 (Time : 0.033)\n",
            ">10 : Features :: Accuracy : 0.795 (Time : 0.034)\n",
            ">11 : Features :: Accuracy : 0.801 (Time : 0.042)\n",
            ">12 : Features :: Accuracy : 0.802 (Time : 0.032)\n",
            ">13 : Features :: Accuracy : 0.818 (Time : 0.035)\n",
            ">14 : Features :: Accuracy : 0.826 (Time : 0.029)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhNKDyAO1MiO"
      },
      "source": [
        "###SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nctkC0t6yHHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e5b5fb-0f6b-4034-c33b-a6ef0d51bf55"
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
        "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "param_grid = dict(C=c_values, kernel=kernel_values)\n",
        "model = SVC()\n",
        "kfold = KFold(n_splits=num_folds)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.844343 using {'C': 1.7, 'kernel': 'rbf'}\n",
            "0.808435 (0.022773) with: {'C': 0.1, 'kernel': 'linear'}\n",
            "0.770753 (0.054460) with: {'C': 0.1, 'kernel': 'poly'}\n",
            "0.815834 (0.023997) with: {'C': 0.1, 'kernel': 'rbf'}\n",
            "0.795642 (0.036841) with: {'C': 0.1, 'kernel': 'sigmoid'}\n",
            "0.813056 (0.021174) with: {'C': 0.3, 'kernel': 'linear'}\n",
            "0.807467 (0.036493) with: {'C': 0.3, 'kernel': 'poly'}\n",
            "0.823216 (0.015710) with: {'C': 0.3, 'kernel': 'rbf'}\n",
            "0.780810 (0.031634) with: {'C': 0.3, 'kernel': 'sigmoid'}\n",
            "0.813048 (0.023527) with: {'C': 0.5, 'kernel': 'linear'}\n",
            "0.813931 (0.032490) with: {'C': 0.5, 'kernel': 'poly'}\n",
            "0.833325 (0.013435) with: {'C': 0.5, 'kernel': 'rbf'}\n",
            "0.776155 (0.034119) with: {'C': 0.5, 'kernel': 'sigmoid'}\n",
            "0.814891 (0.022521) with: {'C': 0.7, 'kernel': 'linear'}\n",
            "0.820370 (0.031818) with: {'C': 0.7, 'kernel': 'poly'}\n",
            "0.838846 (0.019086) with: {'C': 0.7, 'kernel': 'rbf'}\n",
            "0.763260 (0.039317) with: {'C': 0.7, 'kernel': 'sigmoid'}\n",
            "0.815809 (0.022754) with: {'C': 0.9, 'kernel': 'linear'}\n",
            "0.825892 (0.032417) with: {'C': 0.9, 'kernel': 'poly'}\n",
            "0.839755 (0.020855) with: {'C': 0.9, 'kernel': 'rbf'}\n",
            "0.759582 (0.039355) with: {'C': 0.9, 'kernel': 'sigmoid'}\n",
            "0.816735 (0.022883) with: {'C': 1.0, 'kernel': 'linear'}\n",
            "0.828687 (0.031350) with: {'C': 1.0, 'kernel': 'poly'}\n",
            "0.839764 (0.019085) with: {'C': 1.0, 'kernel': 'rbf'}\n",
            "0.757773 (0.041196) with: {'C': 1.0, 'kernel': 'sigmoid'}\n",
            "0.817652 (0.024457) with: {'C': 1.3, 'kernel': 'linear'}\n",
            "0.836060 (0.032036) with: {'C': 1.3, 'kernel': 'poly'}\n",
            "0.841573 (0.018716) with: {'C': 1.3, 'kernel': 'rbf'}\n",
            "0.747638 (0.042236) with: {'C': 1.3, 'kernel': 'sigmoid'}\n",
            "0.817652 (0.024457) with: {'C': 1.5, 'kernel': 'linear'}\n",
            "0.832390 (0.026474) with: {'C': 1.5, 'kernel': 'poly'}\n",
            "0.842499 (0.022656) with: {'C': 1.5, 'kernel': 'rbf'}\n",
            "0.740324 (0.046970) with: {'C': 1.5, 'kernel': 'sigmoid'}\n",
            "0.818569 (0.024569) with: {'C': 1.7, 'kernel': 'linear'}\n",
            "0.832390 (0.030058) with: {'C': 1.7, 'kernel': 'poly'}\n",
            "0.844343 (0.021485) with: {'C': 1.7, 'kernel': 'rbf'}\n",
            "0.725620 (0.042670) with: {'C': 1.7, 'kernel': 'sigmoid'}\n",
            "0.818569 (0.024569) with: {'C': 2.0, 'kernel': 'linear'}\n",
            "0.837903 (0.028528) with: {'C': 2.0, 'kernel': 'poly'}\n",
            "0.844326 (0.023476) with: {'C': 2.0, 'kernel': 'rbf'}\n",
            "0.728423 (0.050752) with: {'C': 2.0, 'kernel': 'sigmoid'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebA8q_w4yHM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dce9cbd-8666-4451-8e87-ed1bff33fa9e"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "SVM = SVC(C=2.0, kernel='rbf')\n",
        "start = time.time()\n",
        "SVM.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.032509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijCBx7I_yHFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396276dd-a003-4a43-ecbc-e65643e84e6c"
      },
      "source": [
        "# estimate accuracy on validation dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "SVMPred = SVM.predict(X_test_scaled)\n",
        "\n",
        "print(confusion_matrix(Y_test, SVMPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[154  19]\n",
            " [ 27  72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7-k4ARyd1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b49149-1ca2-49ac-bdad-903e99044d85"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, SVMPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, SVMPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 83.09%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.89      0.87       173\n",
            "           2       0.79      0.73      0.76        99\n",
            "\n",
            "    accuracy                           0.83       272\n",
            "   macro avg       0.82      0.81      0.81       272\n",
            "weighted avg       0.83      0.83      0.83       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZAKR83_N9e1"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "SVMPred = SVM.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['SVC'] = list(SVMPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THL-enXHzZMF"
      },
      "source": [
        "###LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnSM8M9FzZMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9694bb19-8617-4cb8-e68d-702089fe0962"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "LSVC = LinearSVC()\n",
        "start = time.time()\n",
        "LSVC.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.045703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDhUCfOzZMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12909100-7162-41fe-b21b-272b4c336957"
      },
      "source": [
        "# estimate accuracy on test dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "LSVCPred = LSVC.predict(X_test_scaled)\n",
        "LSVC.fit(X_test_scaled, Y_test)\n",
        "print(confusion_matrix(Y_test, LSVCPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[150  23]\n",
            " [ 26  73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dl-8V6zZMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196a916e-8233-40ca-9395-08ae776b75de"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, LSVCPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, LSVCPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 81.99%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.87      0.86       173\n",
            "           2       0.76      0.74      0.75        99\n",
            "\n",
            "    accuracy                           0.82       272\n",
            "   macro avg       0.81      0.80      0.80       272\n",
            "weighted avg       0.82      0.82      0.82       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_-BN_1RNpYu"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "LSVCPred = LSVC.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['LinearSVC'] = list(LSVCPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH6lpjR8pczw"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABvshcK4pczy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750fb605-088e-4a34-fcbe-dc49d372a233"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator= LinearSVC(), n_features_to_select=5)\n",
        "model = LinearSVC()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.544 (0.132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alNWXH0mpcz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf22691-ed9f-4cbb-8ff1-b9e838accc4d"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6360294117647058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SE0QArspc0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c878ab81-afda-4cdc-bc07-54f283b1d935"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator= LinearSVC(), n_features_to_select=i)\n",
        "\t\tmodel = LinearSVC()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.580 (Time : 0.143)\n",
            ">3 : Features :: Accuracy : 0.587 (Time : 0.148)\n",
            ">4 : Features :: Accuracy : 0.544 (Time : 0.144)\n",
            ">5 : Features :: Accuracy : 0.592 (Time : 0.123)\n",
            ">6 : Features :: Accuracy : 0.578 (Time : 0.118)\n",
            ">7 : Features :: Accuracy : 0.545 (Time : 0.136)\n",
            ">8 : Features :: Accuracy : 0.570 (Time : 0.129)\n",
            ">9 : Features :: Accuracy : 0.565 (Time : 0.128)\n",
            ">10 : Features :: Accuracy : 0.564 (Time : 0.133)\n",
            ">11 : Features :: Accuracy : 0.531 (Time : 0.137)\n",
            ">12 : Features :: Accuracy : 0.573 (Time : 0.119)\n",
            ">13 : Features :: Accuracy : 0.528 (Time : 0.141)\n",
            ">14 : Features :: Accuracy : 0.584 (Time : 0.122)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0KZ5Tra1lFL"
      },
      "source": [
        "###CALIBERATED CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-QFdLyi1lFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2681ffc-63b7-41be-a8cb-845d9d953504"
      },
      "source": [
        "# prepare the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "CCCV = CalibratedClassifierCV(base_estimator = LSVC)\n",
        "start = time.time()\n",
        "CCCV.fit(X_train_scaled, Y_train)\n",
        "end = time.time()\n",
        "print( \"Run Time: %f\" % (end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run Time: 0.195899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEE_IaTe1lFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94eb9a70-4bc1-4e64-bcc6-933cce81011c"
      },
      "source": [
        "# estimate accuracy on test dataset\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "CCCVPred = CCCV.predict(X_test_scaled)\n",
        "CCCV.fit(X_test_scaled, Y_test)\n",
        "print(confusion_matrix(Y_test, CCCVPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[151  22]\n",
            " [ 27  72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rr2TLdp1lFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf55bbe-261c-4beb-daae-cec70bcbbb4c"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, CCCVPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, CCCVPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 81.99%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.87      0.86       173\n",
            "           2       0.77      0.73      0.75        99\n",
            "\n",
            "    accuracy                           0.82       272\n",
            "   macro avg       0.81      0.80      0.80       272\n",
            "weighted avg       0.82      0.82      0.82       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpbbf8LSNDvb"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    XTest_scaled = scaler.transform(XTest)\n",
        "CCCVPred = CCCV.predict(XTest_scaled)\n",
        "\n",
        "EVALUATION['CaliberatedCV'] = list(CCCVPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni0WiBNW1VdB"
      },
      "source": [
        "###NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_DOUaaZ1ZJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171c6d33-b54a-4e49-e330-e2ead751bab4"
      },
      "source": [
        "NB = GaussianNB()\n",
        "NB.fit(X_train,Y_train)\n",
        "NBPred = NB.predict(X_test)\n",
        "NB.score(X_test, Y_test)\n",
        "NB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNO56LS1Y93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ea2144-c5dd-4481-be0d-9c4574a7e5f4"
      },
      "source": [
        "cm = confusion_matrix(Y_test, NBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 69 104]\n",
            " [ 11  88]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1PpE1bYQtLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1fb08e-554a-44d6-8b7c-de526bff4cb9"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, NBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, NBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 57.72%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.40      0.55       173\n",
            "           2       0.46      0.89      0.60        99\n",
            "\n",
            "    accuracy                           0.58       272\n",
            "   macro avg       0.66      0.64      0.58       272\n",
            "weighted avg       0.72      0.58      0.57       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jachJLptJ5lo"
      },
      "source": [
        "NBPred = NB.predict(XTest)\n",
        "\n",
        "EVALUATION['NB'] = list(NBPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7qiblRq15S8"
      },
      "source": [
        "###DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_9z888W2DQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c90b91a-d4e6-4155-e2a9-d6710b6b0e86"
      },
      "source": [
        "DT = DecisionTreeClassifier(criterion='entropy')\n",
        "DT.fit(X_train,Y_train)\n",
        "DTPred = DT.predict(X_test)\n",
        "DT.score(X_test, Y_test)\n",
        "DT.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F96rdu8v2LNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec4a039-4179-40f1-c969-88d5520be475"
      },
      "source": [
        "cm = confusion_matrix(Y_test, DTPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[150  23]\n",
            " [ 25  74]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjgcubEkQqoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bc6b39-6bc0-43a4-edb7-f854b11cdb72"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, DTPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, DTPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 82.35%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.87      0.86       173\n",
            "           2       0.76      0.75      0.76        99\n",
            "\n",
            "    accuracy                           0.82       272\n",
            "   macro avg       0.81      0.81      0.81       272\n",
            "weighted avg       0.82      0.82      0.82       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yu4CTTqJ_c7"
      },
      "source": [
        "DTPred = DT.predict(XTest)\n",
        "\n",
        "EVALUATION['DT'] = list(DTPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArN_lqoNpCDB"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwwDo7SFjRon",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e703b07-4843-44d9-ca83-c1959c36a70a"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
        "model = DecisionTreeClassifier()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.710 (0.042)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zJXrub-jgJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95f49857-f4be-4a5b-99dd-d90c0d457fac"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7169117647058824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8kQ441skPeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "c3e10014-e3d3-47b6-930b-02150d559b17"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = DecisionTreeClassifier()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.631 (Time : 0.052)\n",
            ">3 : Features :: Accuracy : 0.681 (Time : 0.048)\n",
            ">4 : Features :: Accuracy : 0.703 (Time : 0.043)\n",
            ">5 : Features :: Accuracy : 0.706 (Time : 0.047)\n",
            ">6 : Features :: Accuracy : 0.727 (Time : 0.041)\n",
            ">7 : Features :: Accuracy : 0.766 (Time : 0.051)\n",
            ">8 : Features :: Accuracy : 0.787 (Time : 0.035)\n",
            ">9 : Features :: Accuracy : 0.793 (Time : 0.033)\n",
            ">10 : Features :: Accuracy : 0.796 (Time : 0.029)\n",
            ">11 : Features :: Accuracy : 0.805 (Time : 0.034)\n",
            ">12 : Features :: Accuracy : 0.801 (Time : 0.035)\n",
            ">13 : Features :: Accuracy : 0.814 (Time : 0.042)\n",
            ">14 : Features :: Accuracy : 0.822 (Time : 0.034)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAeeUlEQVR4nO3dfZAdV3nn8e9PY9kCG/AMEiyWZEuwMhnXLLFhSiGxA9Y6NjKbsoippaQkVVZlQOUqJBLCesvsuIKRVrVkFxZSKheKYnnjZWEUcLAtUsS2EuSwk8KJRkYSeoltIV4kGexrS8Du2rJHM8/+cXtE62pmbo+m+770/D5Vt3S7b3c/p6XRc8+c0+ccRQRmZlZes5pdADMzK5YTvZlZyTnRm5mVnBO9mVnJOdGbmZXcBc0uQK25c+fGokWLml0MM7O2snv37hciYt54n7Vcol+0aBFDQ0PNLoaZWVuR9KOJPnPTjZlZyTnRm5mVnBO9mVnJOdGbmZWcE72ZWcllSvSSlkt6StJhSXeO8/kVkv5e0j5Jj0takPrsNknPJK/b8iy8mZnVVzfRS+oA7gFuBq4CVkm6quawzwL/MyLeAawH/ktybhfwKeDXgKXApyR15ld8MzOrJ0uNfilwOCKORMSrwDZgRc0xVwHfSt7vTH3+PmBHRJyIiJPADmD59IttZmZZZUn084Gjqe1jyb60vcCtyfvfAV4n6Y0Zz0XSGklDkoYqlUrWspuZlYakSV/TkVdn7H8A3ivpu8B7gePASNaTI2JLRPRGRO+8eeOO4DUzK7WIOPOq3Z7uAlFZpkA4DixMbS9I9qUL+CxJjV7SJcAHI+Jnko4D19ec+/g0ymtmZlOUpUa/C1giabGkC4GVwPb0AZLmShq71ieB+5L3jwI3SepMOmFvSvaZmVmD1E30EXEaWEs1QR8CvhoRByStl3RLctj1wFOSngbeDGxMzj0BbKD6ZbELWJ/sMzOzBlGrLQ7e29sbnr3SzFpFvY7QInKopClfV9LuiOgd77OWm6bYzKyV1Cbc80nCzeYpEMzMSs6J3sys5JzozcxKzonezKzknOjNzErOid7MrOSc6M3MSs7P0ZvVMdmAmXZ7ntpmJid6szrSybwdB8uYuenGzKzknOjNzErOTTdmljv3a7QWJ3prW04mratR/Rr+GcjGid7aljtJzT8D2WRqo5e0XNJTkg5LunOczy+XtFPSdyXtk/T+ZP8iSS9L2pO8Nud9A2ZmNrm6NXpJHcA9wI3AMWCXpO0RcTB12F1UV576oqSrgG8Ci5LPvh8RV+dbbDMzyypLjX4pcDgijkTEq8A2YEXNMQG8Pnn/BuDZ/IpoVn6SJn2ZTUeWRD8fOJraPpbsS7sb+H1Jx6jW5telPlucNOn8g6TfnE5hzcoqIs561e7Li79QZqa8nqNfBfxlRCwA3g98SdIs4CfA5RFxDfDHwFckvb72ZElrJA1JGqpUKjkVycxqTfZl4o7M8sqS6I8DC1PbC5J9aX3AVwEi4jvAHGBuRLwSES8m+3cD3weurA0QEVsiojcieufNmzf1uzAzswllSfS7gCWSFku6EFgJbK855sfADQCSuqkm+oqkeUlnLpLeCiwBjuRVeDMzq69uoo+I08Ba4FHgENWnaw5IWi/pluSwTwAfkbQXGABWR/X3wPcA+yTtAR4Abo+IE0XciJlZXrq6uibtxxhvf1dXV5NLPTG1Wrtcb29vDA0NNbsY1mYaNVimTHHKdC95xzmfa53POV1dXZw8eXJK53R2dnLixLn1ZUm7I6J3vHM8MtbMrElOnjx5Xl8oU+XZK83MSs6J3sys5JzozcxKzonezHIx0ZMqMPGI3FZ+UqVMnOjNLBdjHYtTeU31iZOZoPJShdWPrOaFl1/I7ZpO9GZmLWTzvs08+dyTbN6b36zufo7eSqEdn9Vudpy8YzTq2fNWvdZ5xb/7DWdtVjpmcfOCy3hl1iwuGh3lkWPPMndkdJzzfj5efD9HbzZTTTYoZ6JnsicalNNs9QYYjXc/rXovAPr0L876ctj8xAZGn3kQRocZveAiNt/4Ce56911nnyMRd08tjptuzEquTG3nzb6XItrP09d++PDDDI8OAzA8OsxDhx/KJZYTvZlZRkW0n6evPRpnN9OMxmgusZzozcwyGKtxB5FbTTtt7/N7z9TmxwyPDrPn+T3Tvrbb6M3MasSnXn9OR+nmN3YyesklMEuMDp9i87293PXiybPPmYYHbnlgWudPxonerEajOvzK1rFYJrWdpJWXKjz89ZsZHnkFgOFZ4qHOudz+4SHmvmZu9Zzz6CRtFDfdZDAwMEBPTw8dHR309PQwMDDQ7CJZgRrV4dfsjsVGKLLzspGKbD9vBCf6OgYGBujv72fTpk2cOnWKTZs20d/f72RvlkGRnZeNVGT7eSNkGjAlaTnwZ0AHcG9EfKbm88uB+4FLk2PujIhvJp99kuqasiPAxyLi0clitdqAqZ6eHjZt2sSyZcvO7Nu5cyfr1q1j//79TSxZ66o3X3YRA4HacbBM2c7Jc/DPVMtVeanCHd++g8++97NnmlLqnXM+cVr5nMkGTNVN9Mmar08DNwLHqK4huyoiDqaO2QJ8NyK+KOkq4JsRsSh5PwAsBS4D/g64MiJGJorXaom+o6ODU6dOMXv27DP7hoeHmTNnDiMjE96GJdpxJGmz/8OW5ZwNT2zgwWceZHh0mNmzZnPrklvHH/wzhTgTHb/hiQ187amv8aG3f2jaMdr1nOmOjF0KHI6II8nFtgErgIOpYwIY63J+A/Bs8n4FsC0iXgF+IOlwcr3vZIjbErq7uxkcHDyrRj84OEh3d3cTS2VlMN6THZnOycFkNeC8rj/e4J/bf/X23OPVPvZYRIx2lyXRzweOpraPAb9Wc8zdwGOS1gEXA7+VOveJmnPn1waQtAZYA3D55ZdnKXfD9Pf309fXx9atW7nuuusYHBykr6+PjRs3Nrto1uZqn+zIdE5OT3ak285ra8B5mKzzMu946VhFxWh3eT1euQr4y4j4nKRfB74kqSfryRGxBdgC1aabnMqUi1WrVgGwbt06Dh06RHd3Nxs3bjyz3xqrTPO2TCTv2nbtbw6Vjlk8vOAyYtYsHjo0wO07PndO2/l0f3MoqvNyonsZnjXrTIzae8rrt6CiTHUN2M7OzinHyJLojwMLU9sLkn1pfcBygIj4jqQ5wNyM57a8VatWFZ7YJ/vHbrUZRpupUYspN1Pete1GTZyVVtTgn8nuZUztPbXy8+0T/Szn3beV5fHKXcASSYslXQisBLbXHPNj4IakgN3AHKCSHLdS0kWSFgNLgH/Oq/Blkn5WerxtmxmKHmZf5MRZzdDujz02St0afUSclrQWeJTqo5P3RcQBSeuBoYjYDnwC+AtJH6faMbs6qhnqgKSvUu24PQ18dLInbsxaWdEdmFB8e3Mj284bochpA8ok04CpiPhmRFwZEW+LiI3Jvj9JkjwRcTAiro2IX42IqyPisdS5G5Pz3h4Rf1vMbZgVr+jBP42obbsGPDN5rhuzDBrxCF8jatuuAWfXiE7SRvEUCGYZjNekkjfXtlvHZPMNTfR5Kz/Z5Rq9WR2NGvzj2rYVxTV6szrafeZCM9foZ5BmTDZWBm5SsXbnRD+D1CbyRk041u7K0KTSqI7FMnVglokTvVnJNWr0ZaPi2NQ50VtbacSMj82cVdLq828NU5dp4ZFGarX56JuhHedwb1SM8a5Xb8RqXnOet+s5jbiW4zQ/xmTz0fupG2t7ZVmuzqwoTvTW1oqaBEzSlF7T6bxsRByb2Zzora0VMWK1UaMiyzb60lqXE721rbJNuWtWFCd6y0VXV9e4TQ0wcfNEV1fXtGJ6xKpZNn680nLRjJWfPGLVLBsnemtbZRixajamtuJTuz2dRzozNd1IWi7pKUmHJd05zuefl7QneT0t6Wepz0ZSn9UuQWhmZkzeOT/d5/br1ugldQD3ADcCx4BdkrZHxMFUAT+eOn4dcE3qEi9HxNXTKqWZmZ23LDX6pcDhiDgSEa8C24AVkxy/ChjIo3BmZjZ9WRL9fOBoavtYsu8ckq4AFgPfSu2eI2lI0hOSPjDBeWuSY4YqlUrGopuZWRZ5P165EnggIkZS+65I5l/4XeALkt5We1JEbImI3ojonTdvXs5FMjOb2bIk+uPAwtT2gmTfeFZS02wTEceTP48Aj3N2+72ZmRUsS6LfBSyRtFjShVST+TlPz0j6FaAT+E5qX6eki5L3c4FrgYO155qZWXHqJvqIOA2sBR4FDgFfjYgDktZLuiV16EpgW5z9HFA3MCRpL7AT+Ez6aR0r3kQjVicbtTrdEatm1lo8H30Lavac4z6nMddqdpwiY0w26rnImGX5tzkfk81H75GxZpa7VkyEM5knNTMzKzkn+jomWwTCJld5qcLqR1Z72mCzJnPTTR3pX0FbtW2uFYy3oPbmN3by5OsuYfO9vdz14snxzzkPXhzabGqc6C0X+vQvzvoSrLxU4eGv30yMvMJDnXO5/cND5yzcLYm4e2pxJvqi9Zew2cTcdGOFKGKJPzM7P070ljsv8WfWWpzom6TMA5m8xN/U1fsZKCLORHGtfNxG3yTNWHqvUbzE39Q1qn/B/RgzkxO95c5L/Jm1FjfdmJmVXNvW6Os1Y/hXVMvLZIs2++fM2kHb1uhrF83NcyHdiTpKYeKRsu3SUWpTV9SCzTZ9je7ALjJOkdo20RdprKN0Kq+TJ88d+dnKPD2BlUEjvoTr/d9vB23bdFNWlZcq3PHtO/jsez97zkjS8zHe1AQw+fQE5zs1gZm1Jif6FrN532aefO5JNu/dzF3vvmva16udmgDqT09wPlMTmFnrypToJS0H/gzoAO6NiM/UfP55YFmy+VrgTRFxafLZbcBYxvrPEXF/HgVvd+PVtCsds3h4wWXErFk8dGiA23d8jrkjo2efk4PxpifI5UvFk42ZtaS6K0xJ6gCeBm4EjlFdQ3bVREsCSloHXBMRfyCpCxgCeoEAdgPviogJG7TPZ4WpvCe0asQqRuMdv+GJDTz4zIMMjw4ze9Zsbl1y61kJOI9yVV6qcPPXb+aVkVfO7Luo4yIe+eAjZ2r1XpHJrP1MtsJUls7YpcDhiDgSEa8C24AVkxy/ChhI3r8P2BERJ5LkvgNYnr3oM0ej5ofx9ARmM0+WRD8fOJraPpbsO4ekK4DFwLemcq6kNZKGJA1VKpUs5S6dRiVgT09gNvPk3Rm7EnggIkamclJEbAG2QLXpJucytYVGJWBPT2A282RJ9MeBhantBcm+8awEPlpz7vU15z6evXgzhxOwmRUlS9PNLmCJpMWSLqSazLfXHiTpV4BO4Dup3Y8CN0nqlNQJ3JTsMzOzBqlbo4+I05LWUk3QHcB9EXFA0npgKCLGkv5KYFukHn2IiBOSNlD9sgBYHxEn8r0FMzObTN3HKxttJj9e2c7nNOJarRDHrFVN9/FKw3PDmFn7cqLPKD01gZlZO3Giz2BsMFMQXuTazNqOE30G480NY2bWLtwZO57UZGOVjlncvOAyXpn1y+/Ei0ZHeeTYs2dNOFY97+eZQ7Ryx6o7Y83az2SdsZ6meBzpqX03P7GB0WcehNSo1dELLmLzjZ84d8KxuxtdUjOz+tx0U4fnhjGzdtdWNfqurq4Jl+ybaC70zs5OTpw4/zFanprAzNpdWyX6sbVcp6KVF+/1Qh1m1ghtlejLZLIvrCJG+k6Fv1DMysWJvuQa+YViZq3JnbFmZiXnRG9mVnJO9GZmJec2+gm4A9PMyiJTjV7ScklPSTos6c4JjvmQpIOSDkj6Smr/iKQ9yeuclalaUUSM+5rss+k8q182ks68arfzfNx1sjhm9kt1a/SSOoB7gBuBY8AuSdsj4mDqmCXAJ4FrI+KkpDelLvFyRFydc7mthTXqSR4/MWSWTZYa/VLgcEQciYhXgW3AippjPgLcExEnASLi+XyLaWZm5ytLop8PHE1tH0v2pV0JXCnpHyU9IWl56rM5koaS/R+YZnnNzGyK8uqMvQBYAlwPLAC+LenfRMTPgCsi4riktwLfkvS9iPh++mRJa4A1AJdffnlORTIzM8hWoz8OLExtL0j2pR0DtkfEcET8AHiaauInIo4nfx4BHgeuqQ0QEVsiojcieufNmzflmzAzs4llSfS7gCWSFku6EFgJ1D498xDV2jyS5lJtyjkiqVPSRan91wIHMTOzhqnbdBMRpyWtBR4FOoD7IuKApPXAUERsTz67SdJBYAS4IyJelPQbwJ9LGqX6pfKZ9NM6ZmZWvPZaSjC1xN+UTGGJv8mUbVk8T2pmVh6lWUowvcRf5nO8xJ+ZzXBtP9dN5aUKqx9ZzQsvv9DsopiZtaS2T/Sb923myeeeZPPezc0uiplZS2rrRF95qcLDhx8mCB46/JBr9WZm42jrRL9532ZGYxSA0Rh1rd7MbBxtm+jHavPDo8MADI8Ou1ZvZjaOtk306dr8GNfqW8fAwAA9PT10dHTQ09PDwMBAs4tkNmO11eOVaXuf33umNj9meHSYPc/vaVKJbMzAwAD9/f1s3bqV6667jsHBQfr6+gBYtWpVk0tnNvO01YCp8xngk+egoLINZCoqTk9PD5s2bWLZsmVn9u3cuZN169axf//+3OOZ2eQDppzom3StMsfp6Ojg1KlTzJ49+8y+4eFh5syZw8jISO7xzGzyRN+2bfTWurq7uxkcHDxr3+DgIN3d3U0qkdnM5kRvuevv76evr4+dO3cyPDzMzp076evro7+/v9lFM5uR2rYz1lrXWIfrunXrOHToEN3d3WzcuNEdsWZN4jb6Jl1rJsUxs+KVZvbKMpM04baTsZlNhxN9i3AyN7OiZOqMlbRc0lOSDku6c4JjPiTpoKQDkr6S2n+bpGeS1215FdzMzLKpW6OX1AHcA9xIdRHwXZK2p5cElLQE+CRwbUSclPSmZH8X8CmgFwhgd3LuyfxvxczMxpOlRr8UOBwRRyLiVWAbsKLmmI8A94wl8Ih4Ptn/PmBHRJxIPtsBLM+n6DZVks561e4zs3LKkujnA0dT28eSfWlXAldK+kdJT0haPoVzkbRG0pCkoUqlkr30NiURMenLzMoprwFTFwBLgOuBVcBfSLo068kRsSUieiOid968eTkVyczMIFuiPw4sTG0vSPalHQO2R8RwRPwAeJpq4s9yrpmZFShLot8FLJG0WNKFwEpge80xD1GtzSNpLtWmnCPAo8BNkjoldQI3JfvMzKxB6j51ExGnJa2lmqA7gPsi4oCk9cBQRGznlwn9IDAC3BERLwJI2kD1ywJgfUScmE6Bp9pp2NnZOZ1wZmZtr62mQJiIpwwws5nO0xSbmc1gngKhDs9BY2btzom+DidzM2t3broxMys5J3ozs5JzojczKzknejOzknOiz2BgYICenh46Ojro6elhYGCg2UUyM8vMT93UMTAwQH9/P1u3buW6665jcHCQvr4+AC92bWZtwTX6OjZu3MjWrVtZtmwZs2fPZtmyZWzdupWNGzc2u2hmZpl4CoQ6Ojo6OHXqFLNnzz6zb3h4mDlz5jAyMlJITDOzqfIUCNPQ3d3N4ODgWfsGBwfp7u5uUonMzKbGib6O/v5++vr62LlzJ8PDw+zcuZO+vj76+/ubXTQzs0zcGVvHWIfrunXrOHToEN3d3WzcuNEdsWbWNtxGb2ZWAm6jNzObwTIleknLJT0l6bCkO8f5fLWkiqQ9yevDqc9GUvtrlyA0M7OC1W2jl9QB3APcSHUR8F2StkfEwZpD/yoi1o5ziZcj4urpF9XMzM5Hlhr9UuBwRByJiFeBbcCKYotlZmZ5yZLo5wNHU9vHkn21Pihpn6QHJC1M7Z8jaUjSE5I+MF4ASWuSY4YqlUr20puZWV15dcZ+A1gUEe8AdgD3pz67IukJ/l3gC5LeVntyRGyJiN6I6J03b15ORTIzM8iW6I8D6Rr6gmTfGRHxYkS8kmzeC7wr9dnx5M8jwOPANdMor5mZTVGWRL8LWCJpsaQLgZXAWU/PSHpLavMW4FCyv1PSRcn7ucC1QG0nrpmZFajuUzcRcVrSWuBRoAO4LyIOSFoPDEXEduBjkm4BTgMngNXJ6d3An0sapfql8plxntYxM7MCeWSsmVkJeGSsmdkM5kRvZlZyTvRmZiXnRG9mVnJO9GZmJedEb2ZWck70ZmYl50RvZlZyTvRmZiXnRG9mVnJO9GZmJedEb2ZWck70ZmYl50RvZlZyTvRmZiXnRG9mVnKZEr2k5ZKeknRY0p3jfL5aUkXSnuT14dRnt0l6JnndlmfhzcysvrpLCUrqAO4BbgSOAbskbR9nScC/ioi1Ned2AZ8CeoEAdifnnsyl9GZmVleWGv1S4HBEHImIV4FtwIqM138fsCMiTiTJfQew/PyKamZm5yNLop8PHE1tH0v21fqgpH2SHpC0cCrnSlojaUjSUKVSyVRwSWdetdtj+8zMLL/O2G8AiyLiHVRr7fdP5eSI2BIRvRHRO2/evKznTPoyM7OqLIn+OLAwtb0g2XdGRLwYEa8km/cC78p6rpmZFStLot8FLJG0WNKFwEpge/oASW9Jbd4CHErePwrcJKlTUidwU7LPzMwapO5TNxFxWtJaqgm6A7gvIg5IWg8MRcR24GOSbgFOAyeA1cm5JyRtoPplAbA+Ik4UcB9mZjYBtVp7dm9vbwwNDTW7GGZmbUXS7ojoHe8zj4w1Mys5J3ozs5JzojczKzknejOzkmu5zlhJFeBHUzxtLvBCAcVxnPaI4TitG8NxGhfjiogYd8RpyyX68yFpaKLeZsdpbpwy3UvZ4pTpXsoWJ+8YbroxMys5J3ozs5IrS6Lf4jgtG6dM91K2OGW6l7LFyTVGKdrozcxsYmWp0ZuZ2QSc6M3MSq6tE72khZJ2Sjoo6YCkPywozhxJ/yxpbxLn00XESWJ1SPqupL8pMMYPJX0vWci9sBnkJF2arDj2L5IOSfr1AmK8PbUo/R5Jv5D0RwXE+Xjyb79f0oCkOXnHSOL8YRLjQJ73Iek+Sc9L2p/a1yVph6Rnkj87C4rz75P7GZWUyyODE8T5b8nP2j5JD0q6tIAYG5Lr75H0mKTLphNjojipzz4hKSTNnVaQeis1tfILeAvwzuT964CngasKiCPgkuT9bOCfgHcXdE9/DHwF+JsC/95+CMxtwL/P/cCHk/cXApcWHK8D+CnVgSN5Xnc+8APgNcn2V4HVBZS/B9gPvJbqFOJ/B/zrnK79HuCdwP7Uvv8K3Jm8vxP404LidANvBx4Hegu8n5uAC5L3fzrd+5kgxutT7z8GbC7iXpL9C6lOD/+j6f5/besafUT8JCKeTN7/H6oLnoy3nu1040RE/N9kc3byyr0XW9IC4N9RXaWrrUl6A9Uf4K0AEfFqRPys4LA3AN+PiKmOrM7iAuA1ki6gmoifLSBGN/BPEfFSRJwG/gG4NY8LR8S3qa4VkbaCXy77eT/wgSLiRMShiHhqutfOEOex5O8N4AmqK9rlHeMXqc2LySEPTPBvA/B54D/mEaOtE32apEXANVRr20Vcv0PSHuB5YEdEFBHnC1T/YUcLuHZaAI9J2i1pTUExFgMV4H8kTVH3Srq4oFhjVgIDeV80Io4DnwV+DPwE+HlEPJZ3HKq1+d+U9EZJrwXez9lLcebtzRHxk+T9T4E3Fxir0f4A+NsiLixpo6SjwO8Bf1JQjBXA8YjYm8f1SpHoJV0C/DXwRzXfuLmJiJGIuJpqLWGppJ48ry/pt4HnI2J3ntedwHUR8U7gZuCjkt5TQIwLqP46+sWIuAb4f1SbBwqRLHN5C/C1Aq7dSbX2uxi4DLhY0u/nHSciDlFtcngMeATYA4zkHWeC2EEBv6U2g6R+qqvdfbmI60dEf0QsTK6/Nu/rJ1/y/4kcv0TaPtFLmk01yX85Ir5edLyk+WEnsDznS18L3CLph8A24N9K+l85xwDO1FCJiOeBB4GlBYQ5BhxL/ebzANXEX5SbgScj4rkCrv1bwA8iohIRw8DXgd8oIA4RsTUi3hUR7wFOUu13KspzY+s9J38+X2CshpC0Gvht4PeSL68ifRn4YAHXfRvVSsXeJB8sAJ6U9K/O94JtnegliWob8KGI+O8Fxpk31oMv6TXAjcC/5BkjIj4ZEQsiYhHVJohvRUTutUZJF0t63dh7qh1Y5/T2T1dE/BQ4Kuntya4bgIN5x0lZRQHNNokfA++W9NrkZ+4Gqv1BuZP0puTPy6m2z3+liDiJ7cBtyfvbgIcLjFU4ScupNn3eEhEvFRRjSWpzBTnnAYCI+F5EvCkiFiX54BjVh05+Op2Ltu0LuI7qr5v7qP6auwd4fwFx3gF8N4mzH/iTgu/regp66gZ4K7A3eR0A+gu8j6uBoeTv7SGgs6A4FwMvAm8o8F4+TfU/9X7gS8BFBcX531S/EPcCN+R43QGq/QvDSeLoA94I/D3wDNUnfLoKivM7yftXgOeARwuKcxg4msoF03oiZoIYf538DOwDvgHML+Jeaj7/IdN86sZTIJiZlVxbN92YmVl9TvRmZiXnRG9mVnJO9GZmJedEb2ZWck70ZmYl50RvZlZy/x/wB8OpL+CADAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcVKYY7V715"
      },
      "source": [
        "###DECISON TREE BAGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvTIZqkeV6SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe063fe-7c08-40bf-c169-3fa53af24f21"
      },
      "source": [
        "Bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
        "Bagging.fit(X_train, Y_train)\n",
        "BPred = Bagging.predict(X_test)\n",
        "Bagging.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHA989gFc0cY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a436eac-bb97-4dfa-ab53-45fb6acc1ed4"
      },
      "source": [
        "cm = confusion_matrix(Y_test, BPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[158  15]\n",
            " [ 36  63]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJoehLjQmXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476efbe5-8134-4087-a650-21227f004bdd"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, BPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, BPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 81.25%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.81      0.91      0.86       173\n",
            "           2       0.81      0.64      0.71        99\n",
            "\n",
            "    accuracy                           0.81       272\n",
            "   macro avg       0.81      0.77      0.79       272\n",
            "weighted avg       0.81      0.81      0.81       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZA9w42rKDO6"
      },
      "source": [
        "BPred = Bagging.predict(XTest)\n",
        "\n",
        "EVALUATION['Bagging'] = list(BPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltwo4bpdfUJB"
      },
      "source": [
        "###EXTRA TREE CLASSFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOxJXgyfYKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1709e857-d16f-4f23-97bd-75365b3870b2"
      },
      "source": [
        "ET = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
        "ET.fit(X_train, Y_train)\n",
        "ET.score(X_train, Y_train)\n",
        "ETPred = ET.predict(X_test)\n",
        "ET.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8529411764705882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65zKOlAHfX-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726969d9-7a9d-4280-952c-03e18028e541"
      },
      "source": [
        "cm = confusion_matrix(Y_test, ETPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[157  16]\n",
            " [ 24  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFlomRnPQiGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fad50c0-1b2d-40a6-ed88-533422a5de48"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, ETPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, ETPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.91      0.89       173\n",
            "           2       0.82      0.76      0.79        99\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.85      0.83      0.84       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLRS_VI8KFrH"
      },
      "source": [
        "ETPred = ET.predict(XTest)\n",
        "\n",
        "EVALUATION['ExtraTree'] = list(ETPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZyd_Hfz2oLo"
      },
      "source": [
        "###RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiR0t-PM2NMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0273c450-1e23-4538-9cb8-5409aa0ef570"
      },
      "source": [
        "RF = RandomForestClassifier(criterion='entropy', random_state=0)\n",
        "RF.fit(X_train,Y_train)\n",
        "RFPred = RF.predict(X_test)\n",
        "RF.score(X_test, Y_test)\n",
        "RF.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfNHsez_2THS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5555a64c-780a-4774-9298-7c62134a7950"
      },
      "source": [
        "cm = confusion_matrix(Y_test, RFPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[156  17]\n",
            " [ 23  76]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fn_dacEQTpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9c49f7-2994-4937-cf21-ed74abf9906d"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, RFPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, RFPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.90      0.89       173\n",
            "           2       0.82      0.77      0.79        99\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.84      0.83      0.84       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWx56HeKMKN"
      },
      "source": [
        "RFPred = RF.predict(XTest)\n",
        "\n",
        "EVALUATION['RF'] = list(RFPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CcI353upKsy"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq3BOh6ZpKs0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4c6aaaf-e99b-4d22-f745-7199d765160e"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator= RandomForestClassifier(criterion='entropy', random_state=0), n_features_to_select=5)\n",
        "model = RandomForestClassifier(criterion='entropy', random_state=0)\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.746 (0.038)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PjxF1-spKs8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42ea11c7-b3b7-4e9d-f6bd-62b485835b80"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7058823529411765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07YeIBqNpKtF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "be86392d-ca1c-4f40-c6d5-b6ad3a475da9"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator=RandomForestClassifier(criterion='entropy'), n_features_to_select=i)\n",
        "\t\tmodel = RandomForestClassifier(criterion='entropy')\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.625 (Time : 0.034)\n",
            ">3 : Features :: Accuracy : 0.718 (Time : 0.035)\n",
            ">4 : Features :: Accuracy : 0.755 (Time : 0.038)\n",
            ">5 : Features :: Accuracy : 0.744 (Time : 0.038)\n",
            ">6 : Features :: Accuracy : 0.749 (Time : 0.039)\n",
            ">7 : Features :: Accuracy : 0.754 (Time : 0.039)\n",
            ">8 : Features :: Accuracy : 0.759 (Time : 0.038)\n",
            ">9 : Features :: Accuracy : 0.770 (Time : 0.037)\n",
            ">10 : Features :: Accuracy : 0.793 (Time : 0.037)\n",
            ">11 : Features :: Accuracy : 0.823 (Time : 0.037)\n",
            ">12 : Features :: Accuracy : 0.817 (Time : 0.032)\n",
            ">13 : Features :: Accuracy : 0.838 (Time : 0.032)\n",
            ">14 : Features :: Accuracy : 0.848 (Time : 0.032)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzr01v1ThHp3"
      },
      "source": [
        "###GRADIANT DESCENT CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBjwDnwdhZFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "21436e70-66e0-4718-d2f7-e356276b7700"
      },
      "source": [
        "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
        "GB.fit(X_train, Y_train)\n",
        "GBPred = GB.predict(X_test)\n",
        "#GB.score(X_test, Y_test)\n",
        "GB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7X4yjnWhY_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "584b0e7c-acc2-40da-ded3-591c6728eecd"
      },
      "source": [
        "cm = confusion_matrix(Y_test, GBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[152  21]\n",
            " [ 20  79]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97yLVNwHQLMN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ddb14cd1-2e32-49f7-cd6d-9bcadb40035b"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, GBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, GBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.93%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.88      0.88       173\n",
            "           2       0.79      0.80      0.79        99\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.84      0.84      0.84       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1rjByoJzMa"
      },
      "source": [
        "GBPred = GB.predict(XTest)\n",
        "\n",
        "EVALUATION['GB'] = list(GBPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HqC8d2vpPMF"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3-C4pnpPMH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b60a275-e2da-4b0b-a186-c33f71eeeab9"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1), n_features_to_select=5)\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.794 (0.036)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcnHoBbfpPMM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33985c35-b0cb-4f1d-afa6-d925e8e5a0b1"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7830882352941176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igqa49ydpPMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "b4e427b2-f6c5-4eb0-8cd9-465091fd528e"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1), n_features_to_select=i)\n",
        "\t\tmodel = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.719 (Time : 0.037)\n",
            ">3 : Features :: Accuracy : 0.736 (Time : 0.035)\n",
            ">4 : Features :: Accuracy : 0.794 (Time : 0.031)\n",
            ">5 : Features :: Accuracy : 0.793 (Time : 0.034)\n",
            ">6 : Features :: Accuracy : 0.798 (Time : 0.033)\n",
            ">7 : Features :: Accuracy : 0.809 (Time : 0.029)\n",
            ">8 : Features :: Accuracy : 0.817 (Time : 0.028)\n",
            ">9 : Features :: Accuracy : 0.826 (Time : 0.028)\n",
            ">10 : Features :: Accuracy : 0.834 (Time : 0.031)\n",
            ">11 : Features :: Accuracy : 0.835 (Time : 0.031)\n",
            ">12 : Features :: Accuracy : 0.837 (Time : 0.033)\n",
            ">13 : Features :: Accuracy : 0.837 (Time : 0.030)\n",
            ">14 : Features :: Accuracy : 0.837 (Time : 0.029)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNRXrNXEH6wv"
      },
      "source": [
        "###XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4V9tFc9H5fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac67bef9-07cc-40fc-9ace-8eca5ed1438a"
      },
      "source": [
        "XGB = XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)\n",
        "XGB.fit(X_train, Y_train, verbose=False)\n",
        "XGBPred = XGB.predict(X_test)\n",
        "XGB.fit(X_test, Y_test, verbose=False)\n",
        "print(XGB.score(X_train, Y_train))\n",
        "print(XGB.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8084714548802947\n",
            "0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itRViZGOWts9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49efa882-2bad-4ac8-d6d0-0fca77c386d4"
      },
      "source": [
        "cm = confusion_matrix(Y_test, XGBPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[158  15]\n",
            " [ 24  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsnTJsQ5O5js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b35d63-929b-4be2-8dc2-9a9abd2eb7b1"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, XGBPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, XGBPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.66%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.91      0.89       173\n",
            "           2       0.83      0.76      0.79        99\n",
            "\n",
            "    accuracy                           0.86       272\n",
            "   macro avg       0.85      0.84      0.84       272\n",
            "weighted avg       0.86      0.86      0.86       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFVUvOyaIjkr"
      },
      "source": [
        "#XGBPred = XGB.predict(XTest)\n",
        "#XGBPred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYAqmoHTpShi"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUtYfhCwpShj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87a34207-a088-4539-9611-a5ee0b3d9714"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator= XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75), n_features_to_select=5)\n",
        "model = XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.799 (0.034)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqpoD-1hpShl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b471dc1-3a66-432c-dfc2-424b381960cd"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7683823529411765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjW5C7SRpShp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "2653776d-8357-4275-fb3c-bd67ab757edb"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator= XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75), n_features_to_select=i)\n",
        "\t\tmodel = XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75)\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.657 (Time : 0.003)\n",
            ">3 : Features :: Accuracy : 0.727 (Time : 0.039)\n",
            ">4 : Features :: Accuracy : 0.760 (Time : 0.034)\n",
            ">5 : Features :: Accuracy : 0.799 (Time : 0.034)\n",
            ">6 : Features :: Accuracy : 0.828 (Time : 0.033)\n",
            ">7 : Features :: Accuracy : 0.832 (Time : 0.030)\n",
            ">8 : Features :: Accuracy : 0.839 (Time : 0.033)\n",
            ">9 : Features :: Accuracy : 0.838 (Time : 0.034)\n",
            ">10 : Features :: Accuracy : 0.841 (Time : 0.026)\n",
            ">11 : Features :: Accuracy : 0.837 (Time : 0.028)\n",
            ">12 : Features :: Accuracy : 0.838 (Time : 0.028)\n",
            ">13 : Features :: Accuracy : 0.839 (Time : 0.026)\n",
            ">14 : Features :: Accuracy : 0.839 (Time : 0.028)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2NV_zuNP3vE"
      },
      "source": [
        "###ADABOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awv3ur1UP6kr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dcf1c9-3b6d-42b6-eaa6-fa16e60b455d"
      },
      "source": [
        "ADAB = AdaBoostClassifier()\n",
        "ADAB.fit(X_train, Y_train)\n",
        "ADABPred = ADAB.predict(X_test)\n",
        "ADAB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
              "                   n_estimators=50, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzJNvUimbL_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6539a6ab-76a8-41bf-91c0-cfe485fdf4cd"
      },
      "source": [
        "cm = confusion_matrix(Y_test, ADABPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[157  16]\n",
            " [ 24  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-FsxAUWP6gK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34cfd60-be56-4b19-bf37-9ab5709b2877"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, ADABPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, ADABPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.91      0.89       173\n",
            "           2       0.82      0.76      0.79        99\n",
            "\n",
            "    accuracy                           0.85       272\n",
            "   macro avg       0.85      0.83      0.84       272\n",
            "weighted avg       0.85      0.85      0.85       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve606tulIS_O"
      },
      "source": [
        "ADABPred = ADAB.predict(XTest)\n",
        "EVALUATION['ADB'] = list(ADABPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAOfVTW9pRiK"
      },
      "source": [
        "###RECURSIVE FEATURE ELIMINATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thA01SVzpRiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ee55845-c5d2-4413-aa32-7942f04b1d03"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "# create pipeline\n",
        "rfe = RFE(estimator=AdaBoostClassifier(), n_features_to_select=5)\n",
        "model = AdaBoostClassifier()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.702 (0.034)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAgZk0WbpRij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "551f927a-a03d-493c-cf84-1054be712239"
      },
      "source": [
        "# fit the model on all available data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "# make a prediction for one example\n",
        "YP = pipeline.predict(X_test)\n",
        "print(accuracy_score(Y_test, YP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6580882352941176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMMTBvuWpRi2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "eb397e02-afb0-4732-f421-3403bd05a334"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\t#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "\treturn X_train, Y_train\n",
        " \n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(2, 15):\n",
        "\t\trfe = RFE(estimator= AdaBoostClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = AdaBoostClassifier()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        " \n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        " \n",
        "# define dataset\n",
        "X_train, Y_train = get_dataset()\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s : Features :: Accuracy : %.3f (Time : %.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">2 : Features :: Accuracy : 0.653 (Time : 0.030)\n",
            ">3 : Features :: Accuracy : 0.683 (Time : 0.043)\n",
            ">4 : Features :: Accuracy : 0.705 (Time : 0.034)\n",
            ">5 : Features :: Accuracy : 0.706 (Time : 0.032)\n",
            ">6 : Features :: Accuracy : 0.708 (Time : 0.031)\n",
            ">7 : Features :: Accuracy : 0.711 (Time : 0.032)\n",
            ">8 : Features :: Accuracy : 0.712 (Time : 0.032)\n",
            ">9 : Features :: Accuracy : 0.723 (Time : 0.034)\n",
            ">10 : Features :: Accuracy : 0.755 (Time : 0.043)\n",
            ">11 : Features :: Accuracy : 0.776 (Time : 0.034)\n",
            ">12 : Features :: Accuracy : 0.792 (Time : 0.047)\n",
            ">13 : Features :: Accuracy : 0.815 (Time : 0.037)\n",
            ">14 : Features :: Accuracy : 0.832 (Time : 0.034)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAe90lEQVR4nO3dfXBd9X3n8ffHwtgEApGRkwaMsdN1UhE1DYmGZBtvikshhu3ghuw2VtodmCjxMBPUpmGzA3uZYOxqNk0zm2Y8TBwX02aziQxLA/F2sjykiE21G1LLYBPbKmDsADZJEFiBbYhBtr77xz0y1+Je6Uo65z4cfV4zd3zvefr+jiV/7/HvURGBmZnl17x6F8DMzLLlRG9mlnNO9GZmOedEb2aWc070ZmY5d0q9CzBRW1tbLFu2rN7FMDNrKjt37nwhIhaX29dwiX7ZsmUMDg7WuxhmZk1F0tOV9rnqxsws55zozcxyzonezCznnOjNzHLOid7MLOec6M3MGkRfXx8dHR20tLTQ0dFBX19fKtdtuO6VZmZzUV9fH4VCga1bt7Jy5UoGBgbo7u4GoKura1bXVqNNU9zZ2RnuR29mc01HRwebNm1i1apVJ7b19/fT09PDnj17pjxf0s6I6Cy7z4nezKz+WlpaOHr0KPPnzz+xbXR0lIULF3L8+PEpz58s0buO3sysAbS3tzMwMHDStoGBAdrb22d9bSd6M7MGUCgU6O7upr+/n9HRUfr7++nu7qZQKMz62m6MNTNrAOMNrj09PQwNDdHe3k5vb++sG2LBdfRmZrngOnozsznMid7MLOec6M3Mcs6J3sws55zozcxyzt0rzaxpSaq4r9F6FNaTE72ZNa3SZC7Jyb2CqqpuJK2W9Lik/ZJuKLP/fEn/IOkxSQ9JWlKy72pJTyavq9MsvJmZTW3KRC+pBbgVuBy4AOiSdMGEw74M/LeIeA+wAfgvybmLgJuBDwAXATdLak2v+GZmNpVqnugvAvZHxIGIeA3YBqyZcMwFwIPJ+/6S/R8BHoiIIxExAjwArJ59sc3MrFrVJPpzgWdLPh9KtpXaDVyVvP8o8GZJZ1d5LpLWSRqUNDg8PFxt2c3MrAppda/8j8DvSHoU+B3gMDD1BMqJiNgSEZ0R0bl48eKUimRmZlBdr5vDwHkln5ck206IiOdInuglnQF8LCJ+IekwcPGEcx+aRXnNzGyaqnmi3wGskLRc0qnAWmB76QGS2iSNX+tG4Pbk/X3AZZJak0bYy5JtZmZWI1Mm+og4BlxHMUEPAXdGxF5JGyRdmRx2MfC4pCeAtwG9yblHgI0Uvyx2ABuSbWZmViOej97McmGuD5jyfPRmZnOYE72ZWc450ZuZ5ZwTvZlZznn2SjOzSUw2FTKkNx1ylnGc6M3MJjExwWbVuyfLKZdddWNmlnNO9GZmOedEb2aWc070ZmY550RvZpZzTvRmZjnnRG9mlnNO9GZmOedEb2aWc070ZmY550RvZpZzTvRmZjlXVaKXtFrS45L2S7qhzP6lkvolPSrpMUlXJNuXSfqVpF3Ja3PaN2BmZpObcvZKSS3ArcClwCFgh6TtEbGv5LCbKC4a/jVJFwDfA5Yl+56KiPemW2wza2STTbk7l9d1rZdqnugvAvZHxIGIeA3YBqyZcEwAZybvzwKeS6+IZtZsIuLEq9xnK1q0aBGS3vACym6XxKJFi6Ydp5r56M8Fni35fAj4wIRj1gP3S+oBTgd+r2TfckmPAi8DN0XEP04MIGkdsA5g6dKlVRfezKyZjYyMTPvLb6oFSspJqzG2C/jbiFgCXAF8U9I84KfA0oi4EPgc8G1JZ048OSK2RERnRHQuXrw4pSKZmRlUl+gPA+eVfF6SbCvVDdwJEBE/BBYCbRHxakS8mGzfCTwFvHO2hTYzs+pVk+h3ACskLZd0KrAW2D7hmGeASwAktVNM9MOSFieNuUh6B7ACOJBW4c1seirV+5bWDVvluvPJ6s9nUndeK1PW0UfEMUnXAfcBLcDtEbFX0gZgMCK2A9cDfy3pzyg2zF4TESHpw8AGSaPAGHBtRBzJ7G7MbFJZrkuaJ7WqO68VNdoPurOzMwYHB+tdDLPcq1Wib8Y4M7lWvc+RtDMiOsud45GxZmY550RvZtZAhl8Z5pp7r+GFX72Q2jWd6M3MGsjmxzbzyM8fYfPu9GaMcaI3M2sQw68M89393yUI7tl/T2pP9U70ZtY08tbtcaLNj21mLMYAGIux1J7qnejNrGmMd3uczmtkZKTexa7K+NP86NgoAKNjo6k91TvRm5k1gNKn+XFpPdVXM6mZmZllIG4+E9afBcDuc36N0QWnnrR/dGyUXY99E+79y5PPmSYnejOzOtEtL58Y/HRXtedIxPrpxXHVjZlZzjnRm5lVKYvBTLXgRG9mVqUsBjPVghO9mVkVshrMVAtujDUzm6C0N8y4zWe3MnbGGTBPjI0eZfNtndz04sjJ5zQoJ3ozswlKe8NA8jT/ncsZPf4qAKPzxD2tbVz7qUHaTmsrnjOD3jC14qobM7MpZDmYqRac6M3MprD7+d0npiYYNzo2yq7nd9WpRNPjqhszsyncdWW1w5kaU1VP9JJWS3pc0n5JN5TZv1RSv6RHJT0m6YqSfTcm5z0u6SNpFt7MrNlNtWD7xFdra+u0Y0z5RC+pBbgVuBQ4BOyQtD0i9pUcdhNwZ0R8TdIFwPeAZcn7tcC7gXOA70t6Z0Qcn3ZJzcxyptJ6sWmvs1vNE/1FwP6IOBARrwHbgDUTjglgvG/RWcBzyfs1wLaIeDUiDgL7k+uZmVmNVJPozwWeLfl8KNlWaj3wx5IOUXya75nGuUhaJ2lQ0uDw8HCVRTczs2qk1eumC/jbiFgCXAF8U1LV146ILRHRGRGdixcvTqlIZmYzV4u681qpJhkfBs4r+bwk2VaqG7gTICJ+CCwE2qo818xsVtKebGyyFasq7T9y5EgqsbNQTaLfAayQtFzSqRQbV7dPOOYZ4BIASe0UE/1wctxaSQskLQdWAP+UVuHN+vr66OjooKWlhY6ODvr6+updJKuDZp1srFamTPQRcQy4DrgPGKLYu2avpA2SrkwOux74tKTdQB9wTRTtpfikvw+4F/iMe9xYWvr6+igUCmzatImjR4+yadMmCoWCk32dVFq4GypXg6SxcHczTzZWK0qzC08aOjs7Y3BwsN7FsCbQ0dHBpk2bWLVq1Ylt/f399PT0sGfPnjqWrDmk3YVvJteb7jnljt/48EbufvJuRsdGmT9vPletuIqbPnjTrMqVZplrFUPSzojoLLfPUyDMIVM1JjWboaEhVq5cedK2lStXMjQ0VKcSWa2NP82PT08wOjbqp/oynOjnkKkalZpNe3s7AwMDJ20bGBigvb29TiWyWmv2ycZqxYnemlahUKC7u5v+/n5GR0fp7++nu7ubQqFQ76JZjTT7ZGO14knNrGl1dXUB0NPTw9DQEO3t7fT29p7YbvnX7JON1YobY+ewWjQqWeNqhEbKNBpjszinltdLK4YbY5tAnhpJ86YWP5u8NZSPS3sgk82ME32DyFMjad7U4meTZUN5vfq3gwcyNQonerOcGxkZmXRIf7nXyMjI1BeeggcyNQ4nejPLRGnXR3d5rC/3ujGz1FUayHTtb11L22ltM75u3HwmrD9r+ufMcU70Zpa6yQYylU5PMF265eWZ9bpZP+OQueCqGzNLnQcyNRY/0ZtZ6jyQafomdqOd+Hk2va+c6M3MGkCWXamd6M0sFW4obVxO9GaWCjeUNi43xprNQZ6aYG5xorfU1WreljzODVMrzTw1wVS/XxNfra2t9S5y3TnRW+omm7Mlq/lhyn228pp5aoLJpm2otP/IkSN1LnX9VZXoJa2W9Lik/ZJuKLP/K5J2Ja8nJP2iZN/xkn3b0yy8mU2fpyaYe6acj15SC/AEcClwCNgBdEXEvgrH9wAXRsQnk8//EhFnVFugRpuPfrJqgKyeHGs1T3yjzqvtOCnHKekJM9wyj8uXnMOr815/xlswNsa9h56j7fjYhPNemlaYmVSZtba2pvbE3ZQ/mxRNNh99Nb1uLgL2R8SB5GLbgDVA2UQPdAE3z6Sgjaj0B9qoP2CzyZT2htn88EbGnrwbSkatjp2ygM2XXn/S1AQz6Q1T6d+G/93UXzVVN+cCz5Z8PpRsewNJ5wPLgQdLNi+UNCjpYUl/UOG8dckxg8PDw1UW3cymy1MTzE1p96NfC9wVEcdLtp0fEYclvQN4UNKPI+Kp0pMiYguwBYpVNymXycwSnppgbqrmif4wcF7J5yXJtnLWAn2lGyLicPLnAeAh4MJpl9LMzGasmkS/A1ghabmkUykm8zf0npH0G0Ar8MOSba2SFiTv24APUblu32xOqbTE32TL/KW1xJ/NLVNW3UTEMUnXAfcBLcDtEbFX0gZgMCLGk/5aYFuc3OrSDnxd0hjFL5UvVuqtYzbXjC/xNx0eDGYzMWX3ylprtO6VpfLWfcvdK+sbZybXauRzanGtuRRnuibrXumRsWZmOedEb2aWc070ZmY550RfJ3nrcVHpfqDybIONfD9m46b6nW4GXnikTvLW46JW97No0SJGRkamdb3pzqcyWYxKcdKcs8UaSyM2vE6Xn+itqYx/oUznNVnSrleMyXhREEubE71Zg8liURAv1jG3OdGbNZAsFgWZ7kIdXqwjf1xHX0Yt6oFrxfXNjStuPvOkueIBNp/dytgZZ8A8MTZ6lM23dXLTiyMnn9MEJv5elX5Os867VnGanUfGllGLkYSNPMJxrp9Tr3INvzLM5d+5nFePv3pi24KWBdz7sXtpO61txnHSLLM1Lo+MNWsCpUv8jfNSf5YGJ3qzBuFFQSwrrqO3pjf8yjCf/8Hn+fLvfPlEFUczxvGiIJYVJ3pLRbmGxarOSUFpd8TSdU9nqtK9bD67lUfefMYbGkhPnGPWoNwYW0aeG/xqec5UT8BpN2BObLicaZxK9zJZnJney3Sl2SPKjbH54sbYJlKrUZG1iJPFwJ9yMcYbMLNsuMwizmQjbSvtd7dXmwkn+gZTi+RYizhZDPyBkmqV9WcxvLGV7w5tO9GAOTo2yj1DfbywsfXEMaw/a9bVKuP3clKcFO/JLGtO9A0kq+RYjzhZPWnrlpdh/Uuw/iU2X/o5xk5ZcNL+sVMWsPnS608cw/qXiufMgrs9WrOrqjFW0mrgqxTXjL0tIr44Yf9XgFXJxzcBb42ItyT7rgbGW8j+PCK+kUbBp6rfbMa6x3LJcbaNi/UYfVnpCfja37o21d4qteqO6G6P1uymbIyV1AI8AVwKHAJ2AF2VFvmW1ANcGBGflLQIGAQ6gQB2Au+PiIpj8mfSGJt2o1I9GmOzGhVZjzgbH97I3U/efVJynD9vPletuOqkL65GbVyu9xqrWVyvXjGsdmbbGHsRsD8iDkTEa8A2YM0kx3cBfcn7jwAPRMSRJLk/AKyuvuhzR62qB2oRx0/AZo2lmqqbc4FnSz4fAj5Q7kBJ5wPLgQcnOffcMuetA9YBLF26tIoi5U+eqiE88MessaQ9YGotcFdEHJ/OSRGxBdgCxaqblMvUkCbWnVdMjQefgUfOev2cWcpDEp5u/3PPrW5zXTWJ/jBwXsnnJcm2ctYCn5lw7sUTzn2o+uLll255eWb1wOuzKU+zqPR35vpms8qqqaPfAayQtFzSqRST+faJB0n6DaAV+GHJ5vuAyyS1SmoFLku2mZlZjUz5RB8RxyRdRzFBtwC3R8ReSRuAwYgYT/prgW1R8lgVEUckbaT4ZQGwISI8tM/MrIZyMddNI3Rta9QufHk7p1Gv1Qi/g40Yw2rHc92koFZz0DSzPC1Anad7MXOir1Kt5qBpVnlagNqTjVneONFXoVZz0JiZZcELj5QxsY/7VHPDnDjHzKwBOdGXUdrHffiVYb77ncsZTeaGGZ0n7mlt49pPDb5x4Yn19SitmdnkXHUzhTxMUeuGRbO5zU/0U8hybphaDOWfrPucu9c1jnK/C6Xb/HOy2WiqRL9o0SJGRsrPcFwpac52jc2s5oZxArZS/nlblpoq0Y+MjNRkAWYzszxxHb1ZA+nr66Ojo4OWlhY6Ojro6+ub+iSzKTTVE71ZnvX19VEoFNi6dSsrV65kYGCA7u5uALq6uupcOmtmfqI3axC9vb1s3bqVVatWMX/+fFatWsXWrVvp7e2td9GsyTXVpGaNPNlWvSfVatQ4ebqXrOO0tLRw9OhR5s+ff2Lb6OgoCxcu5Pjxaa3lU9FUbVaNlg+sep7UzKwJtLe3MzAwcNK2gYEB2tvbU4sx2Tw+TvL55Tp6swZRKBT4+Mc/zumnn84zzzzD0qVL+eUvf8lXv/rVehfNmpyf6M0akJ+uLU1O9Ja60ukUJn72uIbKent7ueOOOzh48CBjY2McPHiQO+64w42xNmtNn+i9IEj1yiXcLBJwreqBa3EvtTQ0NMTKlStP2rZy5UqGhobqVCLLi6oSvaTVkh6XtF/SDRWO+UNJ+yTtlfTtku3HJe1KXm9YVHy2vCBI9fLWEJene4HaNMba3DRlopfUAtwKXA5cAHRJumDCMSuAG4EPRcS7gc+W7P5VRLw3eV2ZXtG9IIjlS6FQoLu7m/7+fkZHR+nv76e7u5tCoVDvolmTq6bXzUXA/og4ACBpG7AG2FdyzKeBWyNiBCAink+7oOWUTiE8PnXwTR+8qRahzVI3Pvq1p6eHoaEh2tvb6e3t9ahYm7UpB0xJ+nfA6oj4VPL5PwAfiIjrSo65B3gC+BDQAqyPiHuTfceAXcAx4IsRcU+ZGOuAdQBLly59/9NPP12+MCWrPg23zOPyJefw6rzX/1OyYGyMew89R9vxsQnnvTTpPZYpz5wYMGXT55+NNarJBkyl1Y/+FGAFcDGwBPiBpN+MiF8A50fEYUnvAB6U9OOIeKr05IjYAmyB4sjYSkFKV37a/PBGxp68G0rmih87ZQGbL73+pKd6r/xkZnNdNY2xh4HzSj4vSbaVOgRsj4jRiDhI8el+BUBEHE7+PAA8BFw4yzID2S4IYmaWJ9U80e8AVkhaTjHBrwU+MeGYe4Au4G8ktQHvBA5IagVeiYhXk+0fAr6URsGzWhDErJ76+vro7e09UUdfKBRcR2+zNmWij4hjkq4D7qNY/357ROyVtAEYjIjtyb7LJO0DjgOfj4gXJf028HVJYxT/9/DFiNhXIZTZnOZpii0rnr2ywjnTNdslCyeL2Wg/o7ksy8bYjo4ONm3axKpVq05s6+/vp6enhz179mQS0/JjssbYpkv00zXbBDwxfqP9fVltNfs0xZZfuZmmeLJRkJX2pZXkzbLmkbGWlaZK9GZ55pGxlhUnemtqeVpMu6uri97eXnp6eli4cCE9PT0eGWup8MIj1rTy2Eulq6uractujctP9Na0vJi2WXWaqtdNJXlYGNqmr1a9VNz11ZpBbnrdmJWqVS+VvM17b3OPE701LfdSMauOG2OtaXn+drPquI6+AeOYmU2X6+jNzOYwJ3ozs5xzordM5GnEqlmzc2OspS6PI1bNmpmf6C11HrFq1ljc66YB4zQ7z6tuVnvudWM15XnVzRqLE72lziNWzRpLVY2xklYDX6W4OPhtEfHFMsf8IbAeCGB3RHwi2X41cFNy2J9HxDdSKLc1MI9YNWssU9bRS2oBngAuBQ4BO4CuiNhXcswK4E7gdyNiRNJbI+J5SYuAQaCT4hfATuD9ETFSKZ7r6M3Mpm+2dfQXAfsj4kBEvAZsA9ZMOObTwK3jCTwink+2fwR4ICKOJPseAFbP5CbqRdKJV7nPZmaNrppEfy7wbMnnQ8m2Uu8E3inp/0h6OKnqqfZcJK2TNChpcHh4uPrS14CnqDWzZpdWY+wpwArgYqAL+GtJb6n25IjYEhGdEdG5ePHilIpkZmZQXaI/DJxX8nlJsq3UIWB7RIxGxEGKdforqjx3RiarUnG1ipnZ66pJ9DuAFZKWSzoVWAtsn3DMPRSf5pHURrEq5wBwH3CZpFZJrcBlybZZm6xKxdUqZmavm7J7ZUQck3QdxQTdAtweEXslbQAGI2I7ryf0fcBx4PMR8SKApI0UvywANkTEkSxuxMzMysvFFAhmZnOdp0AwM5vDnOjNzHLOid7MLOec6M3Mcs6J3sws55zozcxyzonezCznnOjNzHLOid7MLOec6M3Mcq6pE31fXx8dHR20tLTQ0dFBX19fvYtkZtZwqlozthH19fVRKBTYunUrK1euZGBggO7ubgCvTWpmVqJpJzXr6Ohg06ZNrFq16sS2/v5+enp62LNnT5ZFNDNrOJNNata0ib6lpYWjR48yf/78E9tGR0dZuHAhx48fz7KIZmYNJ5ezV7a3tzMwMHDStoGBAdrb2+tUIjOzxtS0ib5QKNDd3U1/fz+jo6P09/fT3d1NoVCod9HMzBpK0zbGjje49vT0MDQ0RHt7O729vW6INTOboGnr6M3M7HWzrqOXtFrS45L2S7qhzP5rJA1L2pW8PlWy73jJ9omLipuZWcamrLqR1ALcClwKHAJ2SNoeEfsmHHpHRFxX5hK/ioj3zr6oZmY2E9U80V8E7I+IAxHxGrANWJNtsczMLC3VJPpzgWdLPh9Ktk30MUmPSbpL0nkl2xdKGpT0sKQ/KBdA0rrkmMHh4eHqS29mZlNKq3vl/wSWRcR7gAeAb5TsOz9pIPgE8FeSfn3iyRGxJSI6I6Jz8eLFKRXJzMyguu6Vh4HSJ/QlybYTIuLFko+3AV8q2Xc4+fOApIeAC4GnKgXbuXPnC5KerqJcpdqAF6Z5zkw4TmPGcJzGjeE4tYtxfsU9ETHpi+KXwQFgOXAqsBt494Rj3l7y/qPAw8n7VmBB8r4NeBK4YKqY030Bg2lf03GaJ4bjNG4Mx2mMGFM+0UfEMUnXAfcBLcDtEbFX0oakMNuBP5F0JXAMOAJck5zeDnxd0hjFaqIvxht765iZWYaqGhkbEd8Dvjdh2xdK3t8I3FjmvP8L/OYsy2hmZrPQtHPdTLDFcRo2Tp7uJW9x8nQveYuTaoyGmwLBzMzSlZcnejMzq8CJ3sws55o60Us6T1K/pH2S9kr604ziLJT0T5J2J3FuySJOEqtF0qOS/j7DGD+R9ONkornMpgqV9JZkpPQ/SxqS9K8ziPGukknzdkl6WdJnM4jzZ8nPfo+kPkkL046RxPnTJMbeNO9D0u2Snpe0p2TbIkkPSHoy+bM1ozj/PrmfMUllZ1dMKc5fJr9rj0m6W9JbMoixMbn+Lkn3SzpnNjEqxSnZd72kkNQ2qyC16HeaYV/TtwPvS96/GXiCbPrpCzgjeT8f+BHwwYzu6XPAt4G/z/Dv7SdAWw1+Pt8APpW8PxV4S8bxWoCfURyNneZ1zwUOAqcln+8Ersmg/B3AHuBNFHvEfR/4Vyld+8PA+4A9Jdu+BNyQvL8B+IuM4rQD7wIeAjozvJ/LgFOS938x2/upEOPMkvd/AmzO4l6S7edR7Nb+9Gz/vTb1E31E/DQiHkne/z9giPLz8Mw2TkTEvyQf5yev1FuxJS0B/i3F0cVNTdJZFH+BtwJExGsR8YuMw14CPBUR0x1ZXY1TgNMknUIxET+XQYx24EcR8UpEHAP+N3BVGheOiB9QHONSag2vT1fyDaDsXFSzjRMRQxHx+GyvXUWc+5O/N4CHKY7iTzvGyyUfTyeFPFDhZwPwFeA/pRGjqRN9KUnLKE6v8KOMrt8iaRfwPPBARGQR568o/mDHMrh2qQDul7RT0rqMYiwHhoG/SaqibpN0ekaxxq0F+tK+aBSn8fgy8AzwU+CliLg/7TgUn+b/jaSzJb0JuIKTpx9J29si4qfJ+58Bb8swVq19EvhfWVxYUq+kZ4E/Ar4w1fEzjLEGOBwRu9O4Xi4SvaQzgL8DPjvhGzc1EXE8ivPqLwEuktSR5vUl/T7wfETsTPO6FayMiPcBlwOfkfThDGKcQvG/o1+LiAuBX1KsHsiEpFOBK4H/kcG1Wyk+/S4HzgFOl/THaceJiCGKVQ73A/cCu4DjacepEDvI4H+p9SCpQHGU/reyuH5EFCLivOT65dbgmJXkS/4/k+KXSNMneknzKSb5b0XEd7KOl1Q/9AOrU770h4ArJf2E4pz/vyvpv6ccAzhpornngbsprjmQtkPAoZL/+dxFMfFn5XLgkYj4eQbX/j3gYEQMR8Qo8B3gtzOIQ0RsjYj3R8SHgRGK7U5Z+bmktwMkfz6fYayakHQN8PvAHyVfXln6FvCxDK776xQfKnYn+WAJ8IikX5vpBZs60UsSxTrgoYj4rxnGWTzegi/pNIqrbf1zmjEi4saIWBIRyyhWQTwYEak/NUo6XdKbx99TbMB6Q2v/bEXEz4BnJb0r2XQJkOU8R11kUG2TeAb4oKQ3Jb9zl1BsD0qdpLcmfy6lWD//7SziJLYDVyfvrwa+m2GszElaTbHq88qIeCWjGCtKPq4h5TwAEBE/joi3RsSyJB8cotjp5GezuWjTvoCVFP+7+RjF/+buAq7IIM57gEeTOHuAL2R8XxeTUa8b4B0UZyDdDewFChnex3uBweTv7R6gNaM4pwMvAmdleC+3UPxHvQf4JsmsrBnE+UeKX4i7gUtSvG4fxfaF0SRxdANnA/9AcVbZ7wOLMorz0eT9q8DPgfsyirOf4iJJ47lgVj1iKsT4u+R34DGK63Ccm8W9TNj/E2bZ68ZTIJiZ5VxTV92YmdnUnOjNzHLOid7MLOec6M3Mcs6J3sws55zozcxyzonezCzn/j95bMWMfcdgPAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2oHitvkCnI3"
      },
      "source": [
        "###CATBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uzwvAH_CnI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0f4d3ee-3bd7-4171-8f90-43689021176a"
      },
      "source": [
        "CDAB = CatBoostClassifier()\n",
        "CDAB.fit(X_train, Y_train)\n",
        "CDABPred = ADAB.predict(X_test)\n",
        "CDAB.fit(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate set to 0.007604\n",
            "0:\tlearn: 0.6900262\ttotal: 2.54ms\tremaining: 2.53s\n",
            "1:\tlearn: 0.6868474\ttotal: 3.96ms\tremaining: 1.98s\n",
            "2:\tlearn: 0.6837811\ttotal: 5.42ms\tremaining: 1.8s\n",
            "3:\tlearn: 0.6808996\ttotal: 6.71ms\tremaining: 1.67s\n",
            "4:\tlearn: 0.6775490\ttotal: 7.93ms\tremaining: 1.58s\n",
            "5:\tlearn: 0.6745177\ttotal: 9.3ms\tremaining: 1.54s\n",
            "6:\tlearn: 0.6717776\ttotal: 10.7ms\tremaining: 1.51s\n",
            "7:\tlearn: 0.6691892\ttotal: 12ms\tremaining: 1.49s\n",
            "8:\tlearn: 0.6659417\ttotal: 13.4ms\tremaining: 1.48s\n",
            "9:\tlearn: 0.6630031\ttotal: 14.8ms\tremaining: 1.47s\n",
            "10:\tlearn: 0.6597517\ttotal: 16.3ms\tremaining: 1.46s\n",
            "11:\tlearn: 0.6564772\ttotal: 17.7ms\tremaining: 1.46s\n",
            "12:\tlearn: 0.6535877\ttotal: 19.1ms\tremaining: 1.45s\n",
            "13:\tlearn: 0.6504673\ttotal: 20.5ms\tremaining: 1.44s\n",
            "14:\tlearn: 0.6474915\ttotal: 21.9ms\tremaining: 1.44s\n",
            "15:\tlearn: 0.6447413\ttotal: 23.3ms\tremaining: 1.43s\n",
            "16:\tlearn: 0.6419467\ttotal: 24.6ms\tremaining: 1.42s\n",
            "17:\tlearn: 0.6389404\ttotal: 26ms\tremaining: 1.42s\n",
            "18:\tlearn: 0.6364524\ttotal: 27.3ms\tremaining: 1.41s\n",
            "19:\tlearn: 0.6341659\ttotal: 28.8ms\tremaining: 1.41s\n",
            "20:\tlearn: 0.6316960\ttotal: 30.2ms\tremaining: 1.41s\n",
            "21:\tlearn: 0.6296680\ttotal: 31ms\tremaining: 1.38s\n",
            "22:\tlearn: 0.6269802\ttotal: 32.3ms\tremaining: 1.37s\n",
            "23:\tlearn: 0.6247053\ttotal: 33.7ms\tremaining: 1.37s\n",
            "24:\tlearn: 0.6224534\ttotal: 35.1ms\tremaining: 1.37s\n",
            "25:\tlearn: 0.6200580\ttotal: 36.4ms\tremaining: 1.36s\n",
            "26:\tlearn: 0.6183733\ttotal: 37.1ms\tremaining: 1.34s\n",
            "27:\tlearn: 0.6160282\ttotal: 38.5ms\tremaining: 1.33s\n",
            "28:\tlearn: 0.6136852\ttotal: 39.9ms\tremaining: 1.34s\n",
            "29:\tlearn: 0.6117618\ttotal: 41.3ms\tremaining: 1.33s\n",
            "30:\tlearn: 0.6099515\ttotal: 42.2ms\tremaining: 1.32s\n",
            "31:\tlearn: 0.6078810\ttotal: 43.6ms\tremaining: 1.32s\n",
            "32:\tlearn: 0.6056375\ttotal: 44.8ms\tremaining: 1.31s\n",
            "33:\tlearn: 0.6031465\ttotal: 46.2ms\tremaining: 1.31s\n",
            "34:\tlearn: 0.6016490\ttotal: 46.8ms\tremaining: 1.29s\n",
            "35:\tlearn: 0.5995852\ttotal: 48.1ms\tremaining: 1.29s\n",
            "36:\tlearn: 0.5976882\ttotal: 49.4ms\tremaining: 1.28s\n",
            "37:\tlearn: 0.5955132\ttotal: 50.7ms\tremaining: 1.28s\n",
            "38:\tlearn: 0.5936107\ttotal: 52.1ms\tremaining: 1.28s\n",
            "39:\tlearn: 0.5915467\ttotal: 53.5ms\tremaining: 1.28s\n",
            "40:\tlearn: 0.5897944\ttotal: 54.7ms\tremaining: 1.28s\n",
            "41:\tlearn: 0.5879138\ttotal: 56ms\tremaining: 1.28s\n",
            "42:\tlearn: 0.5859905\ttotal: 57.3ms\tremaining: 1.27s\n",
            "43:\tlearn: 0.5839758\ttotal: 58.7ms\tremaining: 1.28s\n",
            "44:\tlearn: 0.5827741\ttotal: 59.3ms\tremaining: 1.26s\n",
            "45:\tlearn: 0.5810204\ttotal: 60.7ms\tremaining: 1.26s\n",
            "46:\tlearn: 0.5793844\ttotal: 62.1ms\tremaining: 1.26s\n",
            "47:\tlearn: 0.5774264\ttotal: 63.5ms\tremaining: 1.26s\n",
            "48:\tlearn: 0.5756055\ttotal: 64.8ms\tremaining: 1.26s\n",
            "49:\tlearn: 0.5737446\ttotal: 66.1ms\tremaining: 1.26s\n",
            "50:\tlearn: 0.5725621\ttotal: 66.8ms\tremaining: 1.24s\n",
            "51:\tlearn: 0.5710279\ttotal: 68.1ms\tremaining: 1.24s\n",
            "52:\tlearn: 0.5689944\ttotal: 69.5ms\tremaining: 1.24s\n",
            "53:\tlearn: 0.5672357\ttotal: 70.8ms\tremaining: 1.24s\n",
            "54:\tlearn: 0.5656560\ttotal: 72.2ms\tremaining: 1.24s\n",
            "55:\tlearn: 0.5642138\ttotal: 73.2ms\tremaining: 1.23s\n",
            "56:\tlearn: 0.5630579\ttotal: 74.2ms\tremaining: 1.23s\n",
            "57:\tlearn: 0.5611549\ttotal: 75.5ms\tremaining: 1.23s\n",
            "58:\tlearn: 0.5593497\ttotal: 76.8ms\tremaining: 1.23s\n",
            "59:\tlearn: 0.5575974\ttotal: 78.2ms\tremaining: 1.23s\n",
            "60:\tlearn: 0.5562057\ttotal: 79.6ms\tremaining: 1.23s\n",
            "61:\tlearn: 0.5549662\ttotal: 80.9ms\tremaining: 1.22s\n",
            "62:\tlearn: 0.5537647\ttotal: 81.9ms\tremaining: 1.22s\n",
            "63:\tlearn: 0.5523476\ttotal: 83.3ms\tremaining: 1.22s\n",
            "64:\tlearn: 0.5511645\ttotal: 84.7ms\tremaining: 1.22s\n",
            "65:\tlearn: 0.5492679\ttotal: 86ms\tremaining: 1.22s\n",
            "66:\tlearn: 0.5478847\ttotal: 87.5ms\tremaining: 1.22s\n",
            "67:\tlearn: 0.5470874\ttotal: 88ms\tremaining: 1.21s\n",
            "68:\tlearn: 0.5451594\ttotal: 89.3ms\tremaining: 1.21s\n",
            "69:\tlearn: 0.5438539\ttotal: 90.7ms\tremaining: 1.21s\n",
            "70:\tlearn: 0.5426807\ttotal: 92ms\tremaining: 1.2s\n",
            "71:\tlearn: 0.5416608\ttotal: 93.3ms\tremaining: 1.2s\n",
            "72:\tlearn: 0.5404719\ttotal: 94.5ms\tremaining: 1.2s\n",
            "73:\tlearn: 0.5392913\ttotal: 95.9ms\tremaining: 1.2s\n",
            "74:\tlearn: 0.5379455\ttotal: 97.3ms\tremaining: 1.2s\n",
            "75:\tlearn: 0.5370898\ttotal: 98.3ms\tremaining: 1.2s\n",
            "76:\tlearn: 0.5355597\ttotal: 99.7ms\tremaining: 1.2s\n",
            "77:\tlearn: 0.5341729\ttotal: 101ms\tremaining: 1.19s\n",
            "78:\tlearn: 0.5332205\ttotal: 102ms\tremaining: 1.19s\n",
            "79:\tlearn: 0.5321173\ttotal: 104ms\tremaining: 1.19s\n",
            "80:\tlearn: 0.5309154\ttotal: 105ms\tremaining: 1.19s\n",
            "81:\tlearn: 0.5301513\ttotal: 106ms\tremaining: 1.19s\n",
            "82:\tlearn: 0.5298398\ttotal: 107ms\tremaining: 1.18s\n",
            "83:\tlearn: 0.5284369\ttotal: 108ms\tremaining: 1.18s\n",
            "84:\tlearn: 0.5276568\ttotal: 109ms\tremaining: 1.17s\n",
            "85:\tlearn: 0.5265973\ttotal: 110ms\tremaining: 1.17s\n",
            "86:\tlearn: 0.5252960\ttotal: 112ms\tremaining: 1.17s\n",
            "87:\tlearn: 0.5238648\ttotal: 113ms\tremaining: 1.17s\n",
            "88:\tlearn: 0.5223784\ttotal: 114ms\tremaining: 1.17s\n",
            "89:\tlearn: 0.5214406\ttotal: 118ms\tremaining: 1.19s\n",
            "90:\tlearn: 0.5201586\ttotal: 120ms\tremaining: 1.2s\n",
            "91:\tlearn: 0.5191662\ttotal: 122ms\tremaining: 1.21s\n",
            "92:\tlearn: 0.5180519\ttotal: 124ms\tremaining: 1.21s\n",
            "93:\tlearn: 0.5170111\ttotal: 127ms\tremaining: 1.22s\n",
            "94:\tlearn: 0.5158943\ttotal: 128ms\tremaining: 1.22s\n",
            "95:\tlearn: 0.5150246\ttotal: 129ms\tremaining: 1.22s\n",
            "96:\tlearn: 0.5139937\ttotal: 137ms\tremaining: 1.27s\n",
            "97:\tlearn: 0.5127763\ttotal: 139ms\tremaining: 1.28s\n",
            "98:\tlearn: 0.5114530\ttotal: 141ms\tremaining: 1.28s\n",
            "99:\tlearn: 0.5100642\ttotal: 142ms\tremaining: 1.28s\n",
            "100:\tlearn: 0.5086903\ttotal: 144ms\tremaining: 1.28s\n",
            "101:\tlearn: 0.5074450\ttotal: 146ms\tremaining: 1.28s\n",
            "102:\tlearn: 0.5068368\ttotal: 148ms\tremaining: 1.29s\n",
            "103:\tlearn: 0.5060368\ttotal: 150ms\tremaining: 1.29s\n",
            "104:\tlearn: 0.5049365\ttotal: 151ms\tremaining: 1.29s\n",
            "105:\tlearn: 0.5040500\ttotal: 153ms\tremaining: 1.29s\n",
            "106:\tlearn: 0.5037260\ttotal: 153ms\tremaining: 1.28s\n",
            "107:\tlearn: 0.5030276\ttotal: 155ms\tremaining: 1.28s\n",
            "108:\tlearn: 0.5020080\ttotal: 156ms\tremaining: 1.27s\n",
            "109:\tlearn: 0.5010642\ttotal: 157ms\tremaining: 1.27s\n",
            "110:\tlearn: 0.5004341\ttotal: 159ms\tremaining: 1.27s\n",
            "111:\tlearn: 0.4993352\ttotal: 160ms\tremaining: 1.27s\n",
            "112:\tlearn: 0.4982725\ttotal: 161ms\tremaining: 1.27s\n",
            "113:\tlearn: 0.4980939\ttotal: 162ms\tremaining: 1.26s\n",
            "114:\tlearn: 0.4969519\ttotal: 163ms\tremaining: 1.25s\n",
            "115:\tlearn: 0.4963278\ttotal: 164ms\tremaining: 1.25s\n",
            "116:\tlearn: 0.4955929\ttotal: 166ms\tremaining: 1.25s\n",
            "117:\tlearn: 0.4951583\ttotal: 167ms\tremaining: 1.25s\n",
            "118:\tlearn: 0.4941918\ttotal: 168ms\tremaining: 1.24s\n",
            "119:\tlearn: 0.4934245\ttotal: 170ms\tremaining: 1.24s\n",
            "120:\tlearn: 0.4930056\ttotal: 170ms\tremaining: 1.24s\n",
            "121:\tlearn: 0.4922638\ttotal: 172ms\tremaining: 1.23s\n",
            "122:\tlearn: 0.4915740\ttotal: 173ms\tremaining: 1.23s\n",
            "123:\tlearn: 0.4911065\ttotal: 174ms\tremaining: 1.23s\n",
            "124:\tlearn: 0.4904551\ttotal: 176ms\tremaining: 1.23s\n",
            "125:\tlearn: 0.4889945\ttotal: 177ms\tremaining: 1.23s\n",
            "126:\tlearn: 0.4883694\ttotal: 178ms\tremaining: 1.23s\n",
            "127:\tlearn: 0.4875796\ttotal: 180ms\tremaining: 1.22s\n",
            "128:\tlearn: 0.4873038\ttotal: 180ms\tremaining: 1.22s\n",
            "129:\tlearn: 0.4867831\ttotal: 182ms\tremaining: 1.22s\n",
            "130:\tlearn: 0.4859407\ttotal: 183ms\tremaining: 1.21s\n",
            "131:\tlearn: 0.4852104\ttotal: 188ms\tremaining: 1.24s\n",
            "132:\tlearn: 0.4847334\ttotal: 190ms\tremaining: 1.24s\n",
            "133:\tlearn: 0.4840303\ttotal: 191ms\tremaining: 1.23s\n",
            "134:\tlearn: 0.4832248\ttotal: 192ms\tremaining: 1.23s\n",
            "135:\tlearn: 0.4823323\ttotal: 194ms\tremaining: 1.23s\n",
            "136:\tlearn: 0.4816662\ttotal: 195ms\tremaining: 1.23s\n",
            "137:\tlearn: 0.4810538\ttotal: 196ms\tremaining: 1.23s\n",
            "138:\tlearn: 0.4804068\ttotal: 198ms\tremaining: 1.23s\n",
            "139:\tlearn: 0.4797252\ttotal: 205ms\tremaining: 1.26s\n",
            "140:\tlearn: 0.4788537\ttotal: 207ms\tremaining: 1.26s\n",
            "141:\tlearn: 0.4783290\ttotal: 209ms\tremaining: 1.26s\n",
            "142:\tlearn: 0.4774722\ttotal: 210ms\tremaining: 1.26s\n",
            "143:\tlearn: 0.4766471\ttotal: 212ms\tremaining: 1.26s\n",
            "144:\tlearn: 0.4764217\ttotal: 212ms\tremaining: 1.25s\n",
            "145:\tlearn: 0.4758422\ttotal: 214ms\tremaining: 1.25s\n",
            "146:\tlearn: 0.4753790\ttotal: 215ms\tremaining: 1.25s\n",
            "147:\tlearn: 0.4744605\ttotal: 217ms\tremaining: 1.25s\n",
            "148:\tlearn: 0.4737925\ttotal: 218ms\tremaining: 1.25s\n",
            "149:\tlearn: 0.4732585\ttotal: 220ms\tremaining: 1.24s\n",
            "150:\tlearn: 0.4725651\ttotal: 221ms\tremaining: 1.24s\n",
            "151:\tlearn: 0.4719649\ttotal: 222ms\tremaining: 1.24s\n",
            "152:\tlearn: 0.4713553\ttotal: 224ms\tremaining: 1.24s\n",
            "153:\tlearn: 0.4707316\ttotal: 226ms\tremaining: 1.24s\n",
            "154:\tlearn: 0.4701360\ttotal: 227ms\tremaining: 1.24s\n",
            "155:\tlearn: 0.4696378\ttotal: 231ms\tremaining: 1.25s\n",
            "156:\tlearn: 0.4688135\ttotal: 232ms\tremaining: 1.25s\n",
            "157:\tlearn: 0.4685551\ttotal: 233ms\tremaining: 1.24s\n",
            "158:\tlearn: 0.4677726\ttotal: 235ms\tremaining: 1.24s\n",
            "159:\tlearn: 0.4670527\ttotal: 236ms\tremaining: 1.24s\n",
            "160:\tlearn: 0.4664707\ttotal: 237ms\tremaining: 1.24s\n",
            "161:\tlearn: 0.4659262\ttotal: 239ms\tremaining: 1.24s\n",
            "162:\tlearn: 0.4653927\ttotal: 240ms\tremaining: 1.23s\n",
            "163:\tlearn: 0.4650948\ttotal: 242ms\tremaining: 1.23s\n",
            "164:\tlearn: 0.4647238\ttotal: 243ms\tremaining: 1.23s\n",
            "165:\tlearn: 0.4643888\ttotal: 244ms\tremaining: 1.23s\n",
            "166:\tlearn: 0.4637137\ttotal: 246ms\tremaining: 1.23s\n",
            "167:\tlearn: 0.4630898\ttotal: 247ms\tremaining: 1.22s\n",
            "168:\tlearn: 0.4627589\ttotal: 248ms\tremaining: 1.22s\n",
            "169:\tlearn: 0.4623128\ttotal: 250ms\tremaining: 1.22s\n",
            "170:\tlearn: 0.4615905\ttotal: 251ms\tremaining: 1.22s\n",
            "171:\tlearn: 0.4612365\ttotal: 252ms\tremaining: 1.21s\n",
            "172:\tlearn: 0.4608518\ttotal: 254ms\tremaining: 1.21s\n",
            "173:\tlearn: 0.4603622\ttotal: 255ms\tremaining: 1.21s\n",
            "174:\tlearn: 0.4597015\ttotal: 256ms\tremaining: 1.21s\n",
            "175:\tlearn: 0.4590327\ttotal: 258ms\tremaining: 1.21s\n",
            "176:\tlearn: 0.4584035\ttotal: 259ms\tremaining: 1.21s\n",
            "177:\tlearn: 0.4580819\ttotal: 261ms\tremaining: 1.2s\n",
            "178:\tlearn: 0.4573215\ttotal: 262ms\tremaining: 1.2s\n",
            "179:\tlearn: 0.4564203\ttotal: 263ms\tremaining: 1.2s\n",
            "180:\tlearn: 0.4560389\ttotal: 265ms\tremaining: 1.2s\n",
            "181:\tlearn: 0.4553614\ttotal: 266ms\tremaining: 1.2s\n",
            "182:\tlearn: 0.4548844\ttotal: 268ms\tremaining: 1.19s\n",
            "183:\tlearn: 0.4546160\ttotal: 269ms\tremaining: 1.19s\n",
            "184:\tlearn: 0.4537506\ttotal: 270ms\tremaining: 1.19s\n",
            "185:\tlearn: 0.4534056\ttotal: 272ms\tremaining: 1.19s\n",
            "186:\tlearn: 0.4527797\ttotal: 273ms\tremaining: 1.19s\n",
            "187:\tlearn: 0.4523513\ttotal: 275ms\tremaining: 1.19s\n",
            "188:\tlearn: 0.4517328\ttotal: 276ms\tremaining: 1.18s\n",
            "189:\tlearn: 0.4512546\ttotal: 277ms\tremaining: 1.18s\n",
            "190:\tlearn: 0.4506240\ttotal: 279ms\tremaining: 1.18s\n",
            "191:\tlearn: 0.4501522\ttotal: 280ms\tremaining: 1.18s\n",
            "192:\tlearn: 0.4496941\ttotal: 281ms\tremaining: 1.18s\n",
            "193:\tlearn: 0.4490077\ttotal: 283ms\tremaining: 1.18s\n",
            "194:\tlearn: 0.4484110\ttotal: 284ms\tremaining: 1.17s\n",
            "195:\tlearn: 0.4479508\ttotal: 286ms\tremaining: 1.17s\n",
            "196:\tlearn: 0.4474504\ttotal: 287ms\tremaining: 1.17s\n",
            "197:\tlearn: 0.4469864\ttotal: 288ms\tremaining: 1.17s\n",
            "198:\tlearn: 0.4465240\ttotal: 290ms\tremaining: 1.17s\n",
            "199:\tlearn: 0.4459990\ttotal: 291ms\tremaining: 1.16s\n",
            "200:\tlearn: 0.4455765\ttotal: 293ms\tremaining: 1.16s\n",
            "201:\tlearn: 0.4452070\ttotal: 294ms\tremaining: 1.16s\n",
            "202:\tlearn: 0.4449300\ttotal: 295ms\tremaining: 1.16s\n",
            "203:\tlearn: 0.4443073\ttotal: 297ms\tremaining: 1.16s\n",
            "204:\tlearn: 0.4438478\ttotal: 298ms\tremaining: 1.16s\n",
            "205:\tlearn: 0.4435509\ttotal: 300ms\tremaining: 1.16s\n",
            "206:\tlearn: 0.4429136\ttotal: 301ms\tremaining: 1.15s\n",
            "207:\tlearn: 0.4422718\ttotal: 302ms\tremaining: 1.15s\n",
            "208:\tlearn: 0.4419431\ttotal: 304ms\tremaining: 1.15s\n",
            "209:\tlearn: 0.4414529\ttotal: 305ms\tremaining: 1.15s\n",
            "210:\tlearn: 0.4410157\ttotal: 307ms\tremaining: 1.15s\n",
            "211:\tlearn: 0.4406574\ttotal: 308ms\tremaining: 1.14s\n",
            "212:\tlearn: 0.4399715\ttotal: 309ms\tremaining: 1.14s\n",
            "213:\tlearn: 0.4394190\ttotal: 311ms\tremaining: 1.14s\n",
            "214:\tlearn: 0.4389221\ttotal: 312ms\tremaining: 1.14s\n",
            "215:\tlearn: 0.4384605\ttotal: 314ms\tremaining: 1.14s\n",
            "216:\tlearn: 0.4381906\ttotal: 315ms\tremaining: 1.14s\n",
            "217:\tlearn: 0.4375578\ttotal: 317ms\tremaining: 1.14s\n",
            "218:\tlearn: 0.4372508\ttotal: 318ms\tremaining: 1.13s\n",
            "219:\tlearn: 0.4367132\ttotal: 319ms\tremaining: 1.13s\n",
            "220:\tlearn: 0.4363700\ttotal: 321ms\tremaining: 1.13s\n",
            "221:\tlearn: 0.4359618\ttotal: 322ms\tremaining: 1.13s\n",
            "222:\tlearn: 0.4357491\ttotal: 328ms\tremaining: 1.14s\n",
            "223:\tlearn: 0.4353639\ttotal: 330ms\tremaining: 1.14s\n",
            "224:\tlearn: 0.4350359\ttotal: 331ms\tremaining: 1.14s\n",
            "225:\tlearn: 0.4346879\ttotal: 333ms\tremaining: 1.14s\n",
            "226:\tlearn: 0.4344624\ttotal: 334ms\tremaining: 1.14s\n",
            "227:\tlearn: 0.4342069\ttotal: 339ms\tremaining: 1.15s\n",
            "228:\tlearn: 0.4334273\ttotal: 341ms\tremaining: 1.15s\n",
            "229:\tlearn: 0.4329256\ttotal: 342ms\tremaining: 1.15s\n",
            "230:\tlearn: 0.4325601\ttotal: 344ms\tremaining: 1.14s\n",
            "231:\tlearn: 0.4322688\ttotal: 345ms\tremaining: 1.14s\n",
            "232:\tlearn: 0.4319213\ttotal: 346ms\tremaining: 1.14s\n",
            "233:\tlearn: 0.4315466\ttotal: 348ms\tremaining: 1.14s\n",
            "234:\tlearn: 0.4312319\ttotal: 349ms\tremaining: 1.14s\n",
            "235:\tlearn: 0.4307661\ttotal: 351ms\tremaining: 1.13s\n",
            "236:\tlearn: 0.4304976\ttotal: 352ms\tremaining: 1.13s\n",
            "237:\tlearn: 0.4300573\ttotal: 353ms\tremaining: 1.13s\n",
            "238:\tlearn: 0.4298968\ttotal: 355ms\tremaining: 1.13s\n",
            "239:\tlearn: 0.4293571\ttotal: 356ms\tremaining: 1.13s\n",
            "240:\tlearn: 0.4289669\ttotal: 357ms\tremaining: 1.13s\n",
            "241:\tlearn: 0.4283438\ttotal: 359ms\tremaining: 1.12s\n",
            "242:\tlearn: 0.4279075\ttotal: 360ms\tremaining: 1.12s\n",
            "243:\tlearn: 0.4277089\ttotal: 362ms\tremaining: 1.12s\n",
            "244:\tlearn: 0.4273241\ttotal: 363ms\tremaining: 1.12s\n",
            "245:\tlearn: 0.4270480\ttotal: 364ms\tremaining: 1.12s\n",
            "246:\tlearn: 0.4267493\ttotal: 366ms\tremaining: 1.11s\n",
            "247:\tlearn: 0.4262952\ttotal: 367ms\tremaining: 1.11s\n",
            "248:\tlearn: 0.4259590\ttotal: 369ms\tremaining: 1.11s\n",
            "249:\tlearn: 0.4253369\ttotal: 370ms\tremaining: 1.11s\n",
            "250:\tlearn: 0.4250255\ttotal: 371ms\tremaining: 1.11s\n",
            "251:\tlearn: 0.4247021\ttotal: 373ms\tremaining: 1.1s\n",
            "252:\tlearn: 0.4243897\ttotal: 374ms\tremaining: 1.1s\n",
            "253:\tlearn: 0.4237730\ttotal: 376ms\tremaining: 1.1s\n",
            "254:\tlearn: 0.4236638\ttotal: 377ms\tremaining: 1.1s\n",
            "255:\tlearn: 0.4234012\ttotal: 380ms\tremaining: 1.1s\n",
            "256:\tlearn: 0.4230044\ttotal: 381ms\tremaining: 1.1s\n",
            "257:\tlearn: 0.4228529\ttotal: 383ms\tremaining: 1.1s\n",
            "258:\tlearn: 0.4224484\ttotal: 384ms\tremaining: 1.1s\n",
            "259:\tlearn: 0.4220070\ttotal: 386ms\tremaining: 1.1s\n",
            "260:\tlearn: 0.4214462\ttotal: 387ms\tremaining: 1.09s\n",
            "261:\tlearn: 0.4210615\ttotal: 388ms\tremaining: 1.09s\n",
            "262:\tlearn: 0.4208163\ttotal: 390ms\tremaining: 1.09s\n",
            "263:\tlearn: 0.4204036\ttotal: 391ms\tremaining: 1.09s\n",
            "264:\tlearn: 0.4201453\ttotal: 393ms\tremaining: 1.09s\n",
            "265:\tlearn: 0.4195994\ttotal: 394ms\tremaining: 1.09s\n",
            "266:\tlearn: 0.4192299\ttotal: 396ms\tremaining: 1.09s\n",
            "267:\tlearn: 0.4188992\ttotal: 397ms\tremaining: 1.08s\n",
            "268:\tlearn: 0.4185553\ttotal: 398ms\tremaining: 1.08s\n",
            "269:\tlearn: 0.4182731\ttotal: 400ms\tremaining: 1.08s\n",
            "270:\tlearn: 0.4180622\ttotal: 401ms\tremaining: 1.08s\n",
            "271:\tlearn: 0.4176650\ttotal: 403ms\tremaining: 1.08s\n",
            "272:\tlearn: 0.4172966\ttotal: 404ms\tremaining: 1.07s\n",
            "273:\tlearn: 0.4171798\ttotal: 405ms\tremaining: 1.07s\n",
            "274:\tlearn: 0.4167911\ttotal: 406ms\tremaining: 1.07s\n",
            "275:\tlearn: 0.4165292\ttotal: 407ms\tremaining: 1.07s\n",
            "276:\tlearn: 0.4161892\ttotal: 409ms\tremaining: 1.07s\n",
            "277:\tlearn: 0.4157970\ttotal: 410ms\tremaining: 1.06s\n",
            "278:\tlearn: 0.4153918\ttotal: 412ms\tremaining: 1.06s\n",
            "279:\tlearn: 0.4149949\ttotal: 413ms\tremaining: 1.06s\n",
            "280:\tlearn: 0.4149356\ttotal: 414ms\tremaining: 1.06s\n",
            "281:\tlearn: 0.4145313\ttotal: 415ms\tremaining: 1.06s\n",
            "282:\tlearn: 0.4142964\ttotal: 416ms\tremaining: 1.05s\n",
            "283:\tlearn: 0.4142470\ttotal: 417ms\tremaining: 1.05s\n",
            "284:\tlearn: 0.4139735\ttotal: 418ms\tremaining: 1.05s\n",
            "285:\tlearn: 0.4135891\ttotal: 420ms\tremaining: 1.05s\n",
            "286:\tlearn: 0.4131584\ttotal: 421ms\tremaining: 1.05s\n",
            "287:\tlearn: 0.4127791\ttotal: 422ms\tremaining: 1.04s\n",
            "288:\tlearn: 0.4123009\ttotal: 424ms\tremaining: 1.04s\n",
            "289:\tlearn: 0.4120162\ttotal: 425ms\tremaining: 1.04s\n",
            "290:\tlearn: 0.4118899\ttotal: 426ms\tremaining: 1.04s\n",
            "291:\tlearn: 0.4116569\ttotal: 428ms\tremaining: 1.04s\n",
            "292:\tlearn: 0.4114455\ttotal: 429ms\tremaining: 1.03s\n",
            "293:\tlearn: 0.4111212\ttotal: 430ms\tremaining: 1.03s\n",
            "294:\tlearn: 0.4107226\ttotal: 432ms\tremaining: 1.03s\n",
            "295:\tlearn: 0.4102605\ttotal: 433ms\tremaining: 1.03s\n",
            "296:\tlearn: 0.4100252\ttotal: 435ms\tremaining: 1.03s\n",
            "297:\tlearn: 0.4098221\ttotal: 436ms\tremaining: 1.03s\n",
            "298:\tlearn: 0.4095718\ttotal: 437ms\tremaining: 1.02s\n",
            "299:\tlearn: 0.4092185\ttotal: 439ms\tremaining: 1.02s\n",
            "300:\tlearn: 0.4090495\ttotal: 440ms\tremaining: 1.02s\n",
            "301:\tlearn: 0.4084561\ttotal: 441ms\tremaining: 1.02s\n",
            "302:\tlearn: 0.4081260\ttotal: 443ms\tremaining: 1.02s\n",
            "303:\tlearn: 0.4077798\ttotal: 444ms\tremaining: 1.02s\n",
            "304:\tlearn: 0.4076153\ttotal: 446ms\tremaining: 1.01s\n",
            "305:\tlearn: 0.4075488\ttotal: 446ms\tremaining: 1.01s\n",
            "306:\tlearn: 0.4071826\ttotal: 448ms\tremaining: 1.01s\n",
            "307:\tlearn: 0.4069476\ttotal: 449ms\tremaining: 1.01s\n",
            "308:\tlearn: 0.4064669\ttotal: 450ms\tremaining: 1.01s\n",
            "309:\tlearn: 0.4060837\ttotal: 452ms\tremaining: 1s\n",
            "310:\tlearn: 0.4057495\ttotal: 453ms\tremaining: 1s\n",
            "311:\tlearn: 0.4054908\ttotal: 455ms\tremaining: 1s\n",
            "312:\tlearn: 0.4052795\ttotal: 456ms\tremaining: 1s\n",
            "313:\tlearn: 0.4049636\ttotal: 457ms\tremaining: 999ms\n",
            "314:\tlearn: 0.4045246\ttotal: 459ms\tremaining: 997ms\n",
            "315:\tlearn: 0.4041346\ttotal: 460ms\tremaining: 996ms\n",
            "316:\tlearn: 0.4038742\ttotal: 461ms\tremaining: 994ms\n",
            "317:\tlearn: 0.4036407\ttotal: 462ms\tremaining: 992ms\n",
            "318:\tlearn: 0.4033990\ttotal: 464ms\tremaining: 990ms\n",
            "319:\tlearn: 0.4030229\ttotal: 465ms\tremaining: 988ms\n",
            "320:\tlearn: 0.4029615\ttotal: 466ms\tremaining: 985ms\n",
            "321:\tlearn: 0.4027948\ttotal: 467ms\tremaining: 984ms\n",
            "322:\tlearn: 0.4025943\ttotal: 468ms\tremaining: 982ms\n",
            "323:\tlearn: 0.4023564\ttotal: 470ms\tremaining: 980ms\n",
            "324:\tlearn: 0.4021189\ttotal: 471ms\tremaining: 978ms\n",
            "325:\tlearn: 0.4018848\ttotal: 472ms\tremaining: 977ms\n",
            "326:\tlearn: 0.4013895\ttotal: 474ms\tremaining: 975ms\n",
            "327:\tlearn: 0.4012020\ttotal: 475ms\tremaining: 973ms\n",
            "328:\tlearn: 0.4007520\ttotal: 477ms\tremaining: 972ms\n",
            "329:\tlearn: 0.4000813\ttotal: 478ms\tremaining: 970ms\n",
            "330:\tlearn: 0.3996605\ttotal: 479ms\tremaining: 968ms\n",
            "331:\tlearn: 0.3994917\ttotal: 480ms\tremaining: 967ms\n",
            "332:\tlearn: 0.3991997\ttotal: 482ms\tremaining: 965ms\n",
            "333:\tlearn: 0.3987761\ttotal: 483ms\tremaining: 964ms\n",
            "334:\tlearn: 0.3985064\ttotal: 485ms\tremaining: 962ms\n",
            "335:\tlearn: 0.3981965\ttotal: 486ms\tremaining: 961ms\n",
            "336:\tlearn: 0.3978331\ttotal: 488ms\tremaining: 959ms\n",
            "337:\tlearn: 0.3976233\ttotal: 489ms\tremaining: 957ms\n",
            "338:\tlearn: 0.3974683\ttotal: 490ms\tremaining: 956ms\n",
            "339:\tlearn: 0.3970733\ttotal: 492ms\tremaining: 954ms\n",
            "340:\tlearn: 0.3965986\ttotal: 493ms\tremaining: 952ms\n",
            "341:\tlearn: 0.3962262\ttotal: 494ms\tremaining: 951ms\n",
            "342:\tlearn: 0.3958898\ttotal: 496ms\tremaining: 949ms\n",
            "343:\tlearn: 0.3957051\ttotal: 497ms\tremaining: 948ms\n",
            "344:\tlearn: 0.3953858\ttotal: 498ms\tremaining: 946ms\n",
            "345:\tlearn: 0.3952232\ttotal: 500ms\tremaining: 945ms\n",
            "346:\tlearn: 0.3950918\ttotal: 501ms\tremaining: 943ms\n",
            "347:\tlearn: 0.3948650\ttotal: 503ms\tremaining: 942ms\n",
            "348:\tlearn: 0.3944726\ttotal: 504ms\tremaining: 940ms\n",
            "349:\tlearn: 0.3942687\ttotal: 505ms\tremaining: 939ms\n",
            "350:\tlearn: 0.3935843\ttotal: 507ms\tremaining: 937ms\n",
            "351:\tlearn: 0.3934071\ttotal: 508ms\tremaining: 936ms\n",
            "352:\tlearn: 0.3931531\ttotal: 509ms\tremaining: 934ms\n",
            "353:\tlearn: 0.3929021\ttotal: 511ms\tremaining: 932ms\n",
            "354:\tlearn: 0.3926902\ttotal: 512ms\tremaining: 930ms\n",
            "355:\tlearn: 0.3924554\ttotal: 514ms\tremaining: 929ms\n",
            "356:\tlearn: 0.3921403\ttotal: 516ms\tremaining: 929ms\n",
            "357:\tlearn: 0.3920436\ttotal: 518ms\tremaining: 930ms\n",
            "358:\tlearn: 0.3917320\ttotal: 521ms\tremaining: 930ms\n",
            "359:\tlearn: 0.3913235\ttotal: 522ms\tremaining: 928ms\n",
            "360:\tlearn: 0.3909495\ttotal: 524ms\tremaining: 927ms\n",
            "361:\tlearn: 0.3905667\ttotal: 526ms\tremaining: 926ms\n",
            "362:\tlearn: 0.3902484\ttotal: 528ms\tremaining: 926ms\n",
            "363:\tlearn: 0.3897056\ttotal: 529ms\tremaining: 925ms\n",
            "364:\tlearn: 0.3893462\ttotal: 531ms\tremaining: 923ms\n",
            "365:\tlearn: 0.3891513\ttotal: 532ms\tremaining: 922ms\n",
            "366:\tlearn: 0.3888656\ttotal: 534ms\tremaining: 920ms\n",
            "367:\tlearn: 0.3887129\ttotal: 535ms\tremaining: 919ms\n",
            "368:\tlearn: 0.3885327\ttotal: 536ms\tremaining: 917ms\n",
            "369:\tlearn: 0.3885067\ttotal: 537ms\tremaining: 914ms\n",
            "370:\tlearn: 0.3882699\ttotal: 538ms\tremaining: 912ms\n",
            "371:\tlearn: 0.3880154\ttotal: 539ms\tremaining: 910ms\n",
            "372:\tlearn: 0.3878389\ttotal: 540ms\tremaining: 908ms\n",
            "373:\tlearn: 0.3877208\ttotal: 542ms\tremaining: 907ms\n",
            "374:\tlearn: 0.3874990\ttotal: 543ms\tremaining: 905ms\n",
            "375:\tlearn: 0.3872934\ttotal: 545ms\tremaining: 904ms\n",
            "376:\tlearn: 0.3869818\ttotal: 546ms\tremaining: 902ms\n",
            "377:\tlearn: 0.3865158\ttotal: 547ms\tremaining: 900ms\n",
            "378:\tlearn: 0.3860576\ttotal: 548ms\tremaining: 899ms\n",
            "379:\tlearn: 0.3858560\ttotal: 550ms\tremaining: 897ms\n",
            "380:\tlearn: 0.3856092\ttotal: 551ms\tremaining: 895ms\n",
            "381:\tlearn: 0.3853365\ttotal: 552ms\tremaining: 894ms\n",
            "382:\tlearn: 0.3851688\ttotal: 554ms\tremaining: 892ms\n",
            "383:\tlearn: 0.3847895\ttotal: 555ms\tremaining: 890ms\n",
            "384:\tlearn: 0.3844883\ttotal: 556ms\tremaining: 889ms\n",
            "385:\tlearn: 0.3840781\ttotal: 558ms\tremaining: 887ms\n",
            "386:\tlearn: 0.3839708\ttotal: 559ms\tremaining: 886ms\n",
            "387:\tlearn: 0.3835175\ttotal: 561ms\tremaining: 884ms\n",
            "388:\tlearn: 0.3832560\ttotal: 562ms\tremaining: 883ms\n",
            "389:\tlearn: 0.3830685\ttotal: 563ms\tremaining: 881ms\n",
            "390:\tlearn: 0.3828290\ttotal: 565ms\tremaining: 880ms\n",
            "391:\tlearn: 0.3825635\ttotal: 567ms\tremaining: 880ms\n",
            "392:\tlearn: 0.3822761\ttotal: 569ms\tremaining: 879ms\n",
            "393:\tlearn: 0.3820369\ttotal: 571ms\tremaining: 878ms\n",
            "394:\tlearn: 0.3816654\ttotal: 572ms\tremaining: 876ms\n",
            "395:\tlearn: 0.3813767\ttotal: 574ms\tremaining: 875ms\n",
            "396:\tlearn: 0.3809776\ttotal: 575ms\tremaining: 873ms\n",
            "397:\tlearn: 0.3806715\ttotal: 576ms\tremaining: 872ms\n",
            "398:\tlearn: 0.3803396\ttotal: 578ms\tremaining: 870ms\n",
            "399:\tlearn: 0.3800407\ttotal: 579ms\tremaining: 869ms\n",
            "400:\tlearn: 0.3797238\ttotal: 581ms\tremaining: 867ms\n",
            "401:\tlearn: 0.3794942\ttotal: 582ms\tremaining: 866ms\n",
            "402:\tlearn: 0.3793610\ttotal: 584ms\tremaining: 864ms\n",
            "403:\tlearn: 0.3791274\ttotal: 585ms\tremaining: 863ms\n",
            "404:\tlearn: 0.3788753\ttotal: 586ms\tremaining: 861ms\n",
            "405:\tlearn: 0.3784762\ttotal: 588ms\tremaining: 860ms\n",
            "406:\tlearn: 0.3782507\ttotal: 589ms\tremaining: 858ms\n",
            "407:\tlearn: 0.3779943\ttotal: 590ms\tremaining: 857ms\n",
            "408:\tlearn: 0.3774969\ttotal: 592ms\tremaining: 855ms\n",
            "409:\tlearn: 0.3773386\ttotal: 593ms\tremaining: 854ms\n",
            "410:\tlearn: 0.3771486\ttotal: 595ms\tremaining: 852ms\n",
            "411:\tlearn: 0.3768997\ttotal: 596ms\tremaining: 851ms\n",
            "412:\tlearn: 0.3765855\ttotal: 598ms\tremaining: 850ms\n",
            "413:\tlearn: 0.3762827\ttotal: 599ms\tremaining: 848ms\n",
            "414:\tlearn: 0.3760660\ttotal: 600ms\tremaining: 846ms\n",
            "415:\tlearn: 0.3756905\ttotal: 602ms\tremaining: 845ms\n",
            "416:\tlearn: 0.3756373\ttotal: 603ms\tremaining: 843ms\n",
            "417:\tlearn: 0.3754598\ttotal: 604ms\tremaining: 841ms\n",
            "418:\tlearn: 0.3752213\ttotal: 605ms\tremaining: 839ms\n",
            "419:\tlearn: 0.3749601\ttotal: 607ms\tremaining: 838ms\n",
            "420:\tlearn: 0.3746358\ttotal: 608ms\tremaining: 836ms\n",
            "421:\tlearn: 0.3742651\ttotal: 609ms\tremaining: 835ms\n",
            "422:\tlearn: 0.3740563\ttotal: 611ms\tremaining: 833ms\n",
            "423:\tlearn: 0.3738600\ttotal: 612ms\tremaining: 832ms\n",
            "424:\tlearn: 0.3736279\ttotal: 614ms\tremaining: 830ms\n",
            "425:\tlearn: 0.3734187\ttotal: 615ms\tremaining: 829ms\n",
            "426:\tlearn: 0.3730890\ttotal: 616ms\tremaining: 827ms\n",
            "427:\tlearn: 0.3729498\ttotal: 618ms\tremaining: 825ms\n",
            "428:\tlearn: 0.3727259\ttotal: 619ms\tremaining: 824ms\n",
            "429:\tlearn: 0.3724999\ttotal: 620ms\tremaining: 822ms\n",
            "430:\tlearn: 0.3722558\ttotal: 622ms\tremaining: 821ms\n",
            "431:\tlearn: 0.3718926\ttotal: 623ms\tremaining: 819ms\n",
            "432:\tlearn: 0.3717870\ttotal: 624ms\tremaining: 818ms\n",
            "433:\tlearn: 0.3715137\ttotal: 626ms\tremaining: 816ms\n",
            "434:\tlearn: 0.3713480\ttotal: 627ms\tremaining: 815ms\n",
            "435:\tlearn: 0.3711337\ttotal: 629ms\tremaining: 813ms\n",
            "436:\tlearn: 0.3708337\ttotal: 630ms\tremaining: 812ms\n",
            "437:\tlearn: 0.3703718\ttotal: 631ms\tremaining: 810ms\n",
            "438:\tlearn: 0.3701511\ttotal: 633ms\tremaining: 809ms\n",
            "439:\tlearn: 0.3700604\ttotal: 634ms\tremaining: 807ms\n",
            "440:\tlearn: 0.3698816\ttotal: 635ms\tremaining: 805ms\n",
            "441:\tlearn: 0.3697316\ttotal: 636ms\tremaining: 803ms\n",
            "442:\tlearn: 0.3695889\ttotal: 638ms\tremaining: 802ms\n",
            "443:\tlearn: 0.3694082\ttotal: 639ms\tremaining: 800ms\n",
            "444:\tlearn: 0.3692535\ttotal: 640ms\tremaining: 799ms\n",
            "445:\tlearn: 0.3689964\ttotal: 642ms\tremaining: 797ms\n",
            "446:\tlearn: 0.3687607\ttotal: 643ms\tremaining: 796ms\n",
            "447:\tlearn: 0.3684056\ttotal: 644ms\tremaining: 794ms\n",
            "448:\tlearn: 0.3680237\ttotal: 646ms\tremaining: 792ms\n",
            "449:\tlearn: 0.3678436\ttotal: 647ms\tremaining: 791ms\n",
            "450:\tlearn: 0.3673952\ttotal: 648ms\tremaining: 789ms\n",
            "451:\tlearn: 0.3672410\ttotal: 650ms\tremaining: 788ms\n",
            "452:\tlearn: 0.3670298\ttotal: 651ms\tremaining: 786ms\n",
            "453:\tlearn: 0.3667561\ttotal: 653ms\tremaining: 785ms\n",
            "454:\tlearn: 0.3664840\ttotal: 654ms\tremaining: 783ms\n",
            "455:\tlearn: 0.3662485\ttotal: 655ms\tremaining: 782ms\n",
            "456:\tlearn: 0.3660450\ttotal: 657ms\tremaining: 780ms\n",
            "457:\tlearn: 0.3657279\ttotal: 658ms\tremaining: 779ms\n",
            "458:\tlearn: 0.3655447\ttotal: 659ms\tremaining: 777ms\n",
            "459:\tlearn: 0.3652145\ttotal: 661ms\tremaining: 776ms\n",
            "460:\tlearn: 0.3650372\ttotal: 662ms\tremaining: 774ms\n",
            "461:\tlearn: 0.3649841\ttotal: 664ms\tremaining: 773ms\n",
            "462:\tlearn: 0.3647926\ttotal: 665ms\tremaining: 771ms\n",
            "463:\tlearn: 0.3644634\ttotal: 666ms\tremaining: 770ms\n",
            "464:\tlearn: 0.3642908\ttotal: 668ms\tremaining: 768ms\n",
            "465:\tlearn: 0.3640854\ttotal: 669ms\tremaining: 767ms\n",
            "466:\tlearn: 0.3638685\ttotal: 671ms\tremaining: 766ms\n",
            "467:\tlearn: 0.3637320\ttotal: 672ms\tremaining: 764ms\n",
            "468:\tlearn: 0.3635145\ttotal: 673ms\tremaining: 763ms\n",
            "469:\tlearn: 0.3631853\ttotal: 675ms\tremaining: 761ms\n",
            "470:\tlearn: 0.3629969\ttotal: 676ms\tremaining: 759ms\n",
            "471:\tlearn: 0.3628468\ttotal: 678ms\tremaining: 758ms\n",
            "472:\tlearn: 0.3627331\ttotal: 679ms\tremaining: 756ms\n",
            "473:\tlearn: 0.3622758\ttotal: 680ms\tremaining: 755ms\n",
            "474:\tlearn: 0.3620439\ttotal: 682ms\tremaining: 754ms\n",
            "475:\tlearn: 0.3617738\ttotal: 683ms\tremaining: 752ms\n",
            "476:\tlearn: 0.3616561\ttotal: 685ms\tremaining: 751ms\n",
            "477:\tlearn: 0.3615329\ttotal: 686ms\tremaining: 749ms\n",
            "478:\tlearn: 0.3613444\ttotal: 687ms\tremaining: 747ms\n",
            "479:\tlearn: 0.3611904\ttotal: 689ms\tremaining: 746ms\n",
            "480:\tlearn: 0.3609952\ttotal: 690ms\tremaining: 745ms\n",
            "481:\tlearn: 0.3608309\ttotal: 691ms\tremaining: 743ms\n",
            "482:\tlearn: 0.3605618\ttotal: 693ms\tremaining: 742ms\n",
            "483:\tlearn: 0.3602507\ttotal: 694ms\tremaining: 740ms\n",
            "484:\tlearn: 0.3601483\ttotal: 695ms\tremaining: 738ms\n",
            "485:\tlearn: 0.3599756\ttotal: 697ms\tremaining: 737ms\n",
            "486:\tlearn: 0.3597975\ttotal: 698ms\tremaining: 736ms\n",
            "487:\tlearn: 0.3596621\ttotal: 700ms\tremaining: 734ms\n",
            "488:\tlearn: 0.3593031\ttotal: 701ms\tremaining: 733ms\n",
            "489:\tlearn: 0.3589010\ttotal: 703ms\tremaining: 732ms\n",
            "490:\tlearn: 0.3588962\ttotal: 704ms\tremaining: 730ms\n",
            "491:\tlearn: 0.3586641\ttotal: 706ms\tremaining: 729ms\n",
            "492:\tlearn: 0.3585361\ttotal: 709ms\tremaining: 729ms\n",
            "493:\tlearn: 0.3579723\ttotal: 710ms\tremaining: 727ms\n",
            "494:\tlearn: 0.3577718\ttotal: 711ms\tremaining: 726ms\n",
            "495:\tlearn: 0.3575399\ttotal: 716ms\tremaining: 727ms\n",
            "496:\tlearn: 0.3573814\ttotal: 718ms\tremaining: 726ms\n",
            "497:\tlearn: 0.3572488\ttotal: 720ms\tremaining: 725ms\n",
            "498:\tlearn: 0.3570601\ttotal: 721ms\tremaining: 724ms\n",
            "499:\tlearn: 0.3567754\ttotal: 723ms\tremaining: 723ms\n",
            "500:\tlearn: 0.3563982\ttotal: 725ms\tremaining: 722ms\n",
            "501:\tlearn: 0.3561053\ttotal: 726ms\tremaining: 720ms\n",
            "502:\tlearn: 0.3560311\ttotal: 728ms\tremaining: 719ms\n",
            "503:\tlearn: 0.3558176\ttotal: 729ms\tremaining: 717ms\n",
            "504:\tlearn: 0.3554604\ttotal: 730ms\tremaining: 716ms\n",
            "505:\tlearn: 0.3552933\ttotal: 732ms\tremaining: 714ms\n",
            "506:\tlearn: 0.3547858\ttotal: 733ms\tremaining: 713ms\n",
            "507:\tlearn: 0.3545499\ttotal: 734ms\tremaining: 711ms\n",
            "508:\tlearn: 0.3542393\ttotal: 736ms\tremaining: 710ms\n",
            "509:\tlearn: 0.3539179\ttotal: 737ms\tremaining: 708ms\n",
            "510:\tlearn: 0.3536299\ttotal: 739ms\tremaining: 707ms\n",
            "511:\tlearn: 0.3533470\ttotal: 740ms\tremaining: 705ms\n",
            "512:\tlearn: 0.3531338\ttotal: 741ms\tremaining: 704ms\n",
            "513:\tlearn: 0.3526640\ttotal: 743ms\tremaining: 702ms\n",
            "514:\tlearn: 0.3525333\ttotal: 744ms\tremaining: 701ms\n",
            "515:\tlearn: 0.3522638\ttotal: 745ms\tremaining: 699ms\n",
            "516:\tlearn: 0.3520497\ttotal: 747ms\tremaining: 698ms\n",
            "517:\tlearn: 0.3517054\ttotal: 748ms\tremaining: 696ms\n",
            "518:\tlearn: 0.3516667\ttotal: 749ms\tremaining: 694ms\n",
            "519:\tlearn: 0.3514765\ttotal: 751ms\tremaining: 693ms\n",
            "520:\tlearn: 0.3513935\ttotal: 752ms\tremaining: 691ms\n",
            "521:\tlearn: 0.3512016\ttotal: 753ms\tremaining: 690ms\n",
            "522:\tlearn: 0.3509246\ttotal: 759ms\tremaining: 692ms\n",
            "523:\tlearn: 0.3508046\ttotal: 761ms\tremaining: 691ms\n",
            "524:\tlearn: 0.3503821\ttotal: 762ms\tremaining: 689ms\n",
            "525:\tlearn: 0.3499203\ttotal: 763ms\tremaining: 688ms\n",
            "526:\tlearn: 0.3496147\ttotal: 765ms\tremaining: 686ms\n",
            "527:\tlearn: 0.3494882\ttotal: 766ms\tremaining: 685ms\n",
            "528:\tlearn: 0.3492191\ttotal: 768ms\tremaining: 683ms\n",
            "529:\tlearn: 0.3490648\ttotal: 769ms\tremaining: 682ms\n",
            "530:\tlearn: 0.3486947\ttotal: 770ms\tremaining: 680ms\n",
            "531:\tlearn: 0.3483497\ttotal: 772ms\tremaining: 679ms\n",
            "532:\tlearn: 0.3480993\ttotal: 773ms\tremaining: 677ms\n",
            "533:\tlearn: 0.3478715\ttotal: 775ms\tremaining: 676ms\n",
            "534:\tlearn: 0.3476747\ttotal: 776ms\tremaining: 674ms\n",
            "535:\tlearn: 0.3472915\ttotal: 778ms\tremaining: 673ms\n",
            "536:\tlearn: 0.3471161\ttotal: 779ms\tremaining: 672ms\n",
            "537:\tlearn: 0.3468528\ttotal: 781ms\tremaining: 671ms\n",
            "538:\tlearn: 0.3467376\ttotal: 782ms\tremaining: 669ms\n",
            "539:\tlearn: 0.3465984\ttotal: 784ms\tremaining: 668ms\n",
            "540:\tlearn: 0.3461710\ttotal: 785ms\tremaining: 666ms\n",
            "541:\tlearn: 0.3458445\ttotal: 787ms\tremaining: 665ms\n",
            "542:\tlearn: 0.3457370\ttotal: 788ms\tremaining: 663ms\n",
            "543:\tlearn: 0.3455379\ttotal: 789ms\tremaining: 662ms\n",
            "544:\tlearn: 0.3452172\ttotal: 791ms\tremaining: 660ms\n",
            "545:\tlearn: 0.3450518\ttotal: 792ms\tremaining: 659ms\n",
            "546:\tlearn: 0.3448267\ttotal: 794ms\tremaining: 657ms\n",
            "547:\tlearn: 0.3446981\ttotal: 795ms\tremaining: 656ms\n",
            "548:\tlearn: 0.3445036\ttotal: 796ms\tremaining: 654ms\n",
            "549:\tlearn: 0.3442250\ttotal: 798ms\tremaining: 653ms\n",
            "550:\tlearn: 0.3440262\ttotal: 799ms\tremaining: 651ms\n",
            "551:\tlearn: 0.3437895\ttotal: 801ms\tremaining: 650ms\n",
            "552:\tlearn: 0.3436689\ttotal: 802ms\tremaining: 648ms\n",
            "553:\tlearn: 0.3435176\ttotal: 803ms\tremaining: 647ms\n",
            "554:\tlearn: 0.3431619\ttotal: 805ms\tremaining: 645ms\n",
            "555:\tlearn: 0.3428535\ttotal: 806ms\tremaining: 644ms\n",
            "556:\tlearn: 0.3425426\ttotal: 807ms\tremaining: 642ms\n",
            "557:\tlearn: 0.3424543\ttotal: 809ms\tremaining: 641ms\n",
            "558:\tlearn: 0.3424019\ttotal: 810ms\tremaining: 639ms\n",
            "559:\tlearn: 0.3421700\ttotal: 812ms\tremaining: 638ms\n",
            "560:\tlearn: 0.3420616\ttotal: 813ms\tremaining: 636ms\n",
            "561:\tlearn: 0.3419059\ttotal: 814ms\tremaining: 634ms\n",
            "562:\tlearn: 0.3417625\ttotal: 815ms\tremaining: 633ms\n",
            "563:\tlearn: 0.3414676\ttotal: 817ms\tremaining: 631ms\n",
            "564:\tlearn: 0.3413539\ttotal: 818ms\tremaining: 630ms\n",
            "565:\tlearn: 0.3412421\ttotal: 819ms\tremaining: 628ms\n",
            "566:\tlearn: 0.3411510\ttotal: 821ms\tremaining: 627ms\n",
            "567:\tlearn: 0.3408673\ttotal: 822ms\tremaining: 625ms\n",
            "568:\tlearn: 0.3408247\ttotal: 824ms\tremaining: 624ms\n",
            "569:\tlearn: 0.3406591\ttotal: 825ms\tremaining: 622ms\n",
            "570:\tlearn: 0.3403564\ttotal: 826ms\tremaining: 621ms\n",
            "571:\tlearn: 0.3402205\ttotal: 828ms\tremaining: 619ms\n",
            "572:\tlearn: 0.3398843\ttotal: 829ms\tremaining: 618ms\n",
            "573:\tlearn: 0.3395569\ttotal: 831ms\tremaining: 616ms\n",
            "574:\tlearn: 0.3393523\ttotal: 832ms\tremaining: 615ms\n",
            "575:\tlearn: 0.3392015\ttotal: 833ms\tremaining: 613ms\n",
            "576:\tlearn: 0.3391968\ttotal: 834ms\tremaining: 611ms\n",
            "577:\tlearn: 0.3390513\ttotal: 835ms\tremaining: 610ms\n",
            "578:\tlearn: 0.3387256\ttotal: 837ms\tremaining: 608ms\n",
            "579:\tlearn: 0.3385281\ttotal: 838ms\tremaining: 607ms\n",
            "580:\tlearn: 0.3383235\ttotal: 839ms\tremaining: 605ms\n",
            "581:\tlearn: 0.3379932\ttotal: 841ms\tremaining: 604ms\n",
            "582:\tlearn: 0.3378484\ttotal: 842ms\tremaining: 602ms\n",
            "583:\tlearn: 0.3376040\ttotal: 843ms\tremaining: 601ms\n",
            "584:\tlearn: 0.3374765\ttotal: 845ms\tremaining: 599ms\n",
            "585:\tlearn: 0.3373434\ttotal: 846ms\tremaining: 598ms\n",
            "586:\tlearn: 0.3369991\ttotal: 848ms\tremaining: 596ms\n",
            "587:\tlearn: 0.3366757\ttotal: 849ms\tremaining: 595ms\n",
            "588:\tlearn: 0.3364343\ttotal: 851ms\tremaining: 593ms\n",
            "589:\tlearn: 0.3362049\ttotal: 852ms\tremaining: 592ms\n",
            "590:\tlearn: 0.3360937\ttotal: 853ms\tremaining: 590ms\n",
            "591:\tlearn: 0.3357422\ttotal: 855ms\tremaining: 589ms\n",
            "592:\tlearn: 0.3356254\ttotal: 856ms\tremaining: 588ms\n",
            "593:\tlearn: 0.3353375\ttotal: 858ms\tremaining: 586ms\n",
            "594:\tlearn: 0.3350665\ttotal: 859ms\tremaining: 585ms\n",
            "595:\tlearn: 0.3349051\ttotal: 860ms\tremaining: 583ms\n",
            "596:\tlearn: 0.3346018\ttotal: 861ms\tremaining: 582ms\n",
            "597:\tlearn: 0.3343950\ttotal: 863ms\tremaining: 580ms\n",
            "598:\tlearn: 0.3342676\ttotal: 864ms\tremaining: 578ms\n",
            "599:\tlearn: 0.3342308\ttotal: 865ms\tremaining: 577ms\n",
            "600:\tlearn: 0.3341501\ttotal: 866ms\tremaining: 575ms\n",
            "601:\tlearn: 0.3338612\ttotal: 868ms\tremaining: 574ms\n",
            "602:\tlearn: 0.3334194\ttotal: 869ms\tremaining: 572ms\n",
            "603:\tlearn: 0.3331750\ttotal: 870ms\tremaining: 571ms\n",
            "604:\tlearn: 0.3330539\ttotal: 872ms\tremaining: 569ms\n",
            "605:\tlearn: 0.3326945\ttotal: 873ms\tremaining: 568ms\n",
            "606:\tlearn: 0.3325400\ttotal: 874ms\tremaining: 566ms\n",
            "607:\tlearn: 0.3322938\ttotal: 876ms\tremaining: 565ms\n",
            "608:\tlearn: 0.3321425\ttotal: 877ms\tremaining: 563ms\n",
            "609:\tlearn: 0.3320050\ttotal: 878ms\tremaining: 562ms\n",
            "610:\tlearn: 0.3317538\ttotal: 880ms\tremaining: 560ms\n",
            "611:\tlearn: 0.3315738\ttotal: 881ms\tremaining: 559ms\n",
            "612:\tlearn: 0.3314312\ttotal: 883ms\tremaining: 557ms\n",
            "613:\tlearn: 0.3312500\ttotal: 884ms\tremaining: 556ms\n",
            "614:\tlearn: 0.3311033\ttotal: 885ms\tremaining: 554ms\n",
            "615:\tlearn: 0.3309431\ttotal: 887ms\tremaining: 553ms\n",
            "616:\tlearn: 0.3307569\ttotal: 888ms\tremaining: 551ms\n",
            "617:\tlearn: 0.3306140\ttotal: 889ms\tremaining: 550ms\n",
            "618:\tlearn: 0.3303339\ttotal: 891ms\tremaining: 548ms\n",
            "619:\tlearn: 0.3302021\ttotal: 892ms\tremaining: 547ms\n",
            "620:\tlearn: 0.3301012\ttotal: 894ms\tremaining: 546ms\n",
            "621:\tlearn: 0.3300109\ttotal: 896ms\tremaining: 545ms\n",
            "622:\tlearn: 0.3297816\ttotal: 898ms\tremaining: 543ms\n",
            "623:\tlearn: 0.3297484\ttotal: 900ms\tremaining: 542ms\n",
            "624:\tlearn: 0.3296104\ttotal: 901ms\tremaining: 541ms\n",
            "625:\tlearn: 0.3293354\ttotal: 903ms\tremaining: 539ms\n",
            "626:\tlearn: 0.3291403\ttotal: 905ms\tremaining: 538ms\n",
            "627:\tlearn: 0.3288960\ttotal: 907ms\tremaining: 537ms\n",
            "628:\tlearn: 0.3287231\ttotal: 908ms\tremaining: 536ms\n",
            "629:\tlearn: 0.3284106\ttotal: 910ms\tremaining: 534ms\n",
            "630:\tlearn: 0.3282043\ttotal: 911ms\tremaining: 533ms\n",
            "631:\tlearn: 0.3281178\ttotal: 912ms\tremaining: 531ms\n",
            "632:\tlearn: 0.3279854\ttotal: 914ms\tremaining: 530ms\n",
            "633:\tlearn: 0.3276937\ttotal: 915ms\tremaining: 528ms\n",
            "634:\tlearn: 0.3274961\ttotal: 916ms\tremaining: 527ms\n",
            "635:\tlearn: 0.3271973\ttotal: 918ms\tremaining: 525ms\n",
            "636:\tlearn: 0.3270278\ttotal: 919ms\tremaining: 524ms\n",
            "637:\tlearn: 0.3266638\ttotal: 920ms\tremaining: 522ms\n",
            "638:\tlearn: 0.3264075\ttotal: 922ms\tremaining: 521ms\n",
            "639:\tlearn: 0.3262125\ttotal: 923ms\tremaining: 519ms\n",
            "640:\tlearn: 0.3260896\ttotal: 924ms\tremaining: 518ms\n",
            "641:\tlearn: 0.3257180\ttotal: 926ms\tremaining: 516ms\n",
            "642:\tlearn: 0.3253746\ttotal: 927ms\tremaining: 515ms\n",
            "643:\tlearn: 0.3252368\ttotal: 928ms\tremaining: 513ms\n",
            "644:\tlearn: 0.3250143\ttotal: 930ms\tremaining: 512ms\n",
            "645:\tlearn: 0.3249245\ttotal: 931ms\tremaining: 510ms\n",
            "646:\tlearn: 0.3247972\ttotal: 932ms\tremaining: 509ms\n",
            "647:\tlearn: 0.3246314\ttotal: 933ms\tremaining: 507ms\n",
            "648:\tlearn: 0.3245216\ttotal: 935ms\tremaining: 506ms\n",
            "649:\tlearn: 0.3243187\ttotal: 936ms\tremaining: 504ms\n",
            "650:\tlearn: 0.3241456\ttotal: 937ms\tremaining: 503ms\n",
            "651:\tlearn: 0.3238188\ttotal: 939ms\tremaining: 501ms\n",
            "652:\tlearn: 0.3235804\ttotal: 940ms\tremaining: 500ms\n",
            "653:\tlearn: 0.3233808\ttotal: 941ms\tremaining: 498ms\n",
            "654:\tlearn: 0.3232621\ttotal: 943ms\tremaining: 497ms\n",
            "655:\tlearn: 0.3231080\ttotal: 944ms\tremaining: 495ms\n",
            "656:\tlearn: 0.3229194\ttotal: 946ms\tremaining: 494ms\n",
            "657:\tlearn: 0.3227353\ttotal: 948ms\tremaining: 493ms\n",
            "658:\tlearn: 0.3225575\ttotal: 950ms\tremaining: 492ms\n",
            "659:\tlearn: 0.3222815\ttotal: 952ms\tremaining: 490ms\n",
            "660:\tlearn: 0.3220606\ttotal: 953ms\tremaining: 489ms\n",
            "661:\tlearn: 0.3219110\ttotal: 954ms\tremaining: 487ms\n",
            "662:\tlearn: 0.3216644\ttotal: 956ms\tremaining: 486ms\n",
            "663:\tlearn: 0.3214121\ttotal: 957ms\tremaining: 484ms\n",
            "664:\tlearn: 0.3213228\ttotal: 958ms\tremaining: 483ms\n",
            "665:\tlearn: 0.3210825\ttotal: 960ms\tremaining: 481ms\n",
            "666:\tlearn: 0.3209794\ttotal: 961ms\tremaining: 480ms\n",
            "667:\tlearn: 0.3208077\ttotal: 963ms\tremaining: 478ms\n",
            "668:\tlearn: 0.3205635\ttotal: 964ms\tremaining: 477ms\n",
            "669:\tlearn: 0.3202480\ttotal: 965ms\tremaining: 475ms\n",
            "670:\tlearn: 0.3200104\ttotal: 967ms\tremaining: 474ms\n",
            "671:\tlearn: 0.3197839\ttotal: 968ms\tremaining: 472ms\n",
            "672:\tlearn: 0.3195379\ttotal: 969ms\tremaining: 471ms\n",
            "673:\tlearn: 0.3194080\ttotal: 971ms\tremaining: 469ms\n",
            "674:\tlearn: 0.3192717\ttotal: 972ms\tremaining: 468ms\n",
            "675:\tlearn: 0.3191541\ttotal: 973ms\tremaining: 466ms\n",
            "676:\tlearn: 0.3190310\ttotal: 974ms\tremaining: 465ms\n",
            "677:\tlearn: 0.3188438\ttotal: 976ms\tremaining: 463ms\n",
            "678:\tlearn: 0.3186792\ttotal: 977ms\tremaining: 462ms\n",
            "679:\tlearn: 0.3184238\ttotal: 978ms\tremaining: 460ms\n",
            "680:\tlearn: 0.3182102\ttotal: 980ms\tremaining: 459ms\n",
            "681:\tlearn: 0.3181095\ttotal: 981ms\tremaining: 457ms\n",
            "682:\tlearn: 0.3179555\ttotal: 982ms\tremaining: 456ms\n",
            "683:\tlearn: 0.3177712\ttotal: 983ms\tremaining: 454ms\n",
            "684:\tlearn: 0.3175101\ttotal: 984ms\tremaining: 453ms\n",
            "685:\tlearn: 0.3172235\ttotal: 986ms\tremaining: 451ms\n",
            "686:\tlearn: 0.3171631\ttotal: 987ms\tremaining: 450ms\n",
            "687:\tlearn: 0.3169458\ttotal: 989ms\tremaining: 448ms\n",
            "688:\tlearn: 0.3167109\ttotal: 990ms\tremaining: 447ms\n",
            "689:\tlearn: 0.3164772\ttotal: 991ms\tremaining: 445ms\n",
            "690:\tlearn: 0.3162107\ttotal: 993ms\tremaining: 444ms\n",
            "691:\tlearn: 0.3161746\ttotal: 993ms\tremaining: 442ms\n",
            "692:\tlearn: 0.3159760\ttotal: 995ms\tremaining: 441ms\n",
            "693:\tlearn: 0.3158623\ttotal: 996ms\tremaining: 439ms\n",
            "694:\tlearn: 0.3157194\ttotal: 998ms\tremaining: 438ms\n",
            "695:\tlearn: 0.3156219\ttotal: 999ms\tremaining: 436ms\n",
            "696:\tlearn: 0.3153894\ttotal: 1s\tremaining: 435ms\n",
            "697:\tlearn: 0.3153005\ttotal: 1s\tremaining: 433ms\n",
            "698:\tlearn: 0.3152174\ttotal: 1s\tremaining: 432ms\n",
            "699:\tlearn: 0.3148922\ttotal: 1s\tremaining: 430ms\n",
            "700:\tlearn: 0.3147296\ttotal: 1s\tremaining: 429ms\n",
            "701:\tlearn: 0.3146338\ttotal: 1.01s\tremaining: 428ms\n",
            "702:\tlearn: 0.3144530\ttotal: 1.01s\tremaining: 426ms\n",
            "703:\tlearn: 0.3142613\ttotal: 1.01s\tremaining: 425ms\n",
            "704:\tlearn: 0.3141933\ttotal: 1.01s\tremaining: 424ms\n",
            "705:\tlearn: 0.3141283\ttotal: 1.01s\tremaining: 423ms\n",
            "706:\tlearn: 0.3139362\ttotal: 1.02s\tremaining: 422ms\n",
            "707:\tlearn: 0.3137096\ttotal: 1.02s\tremaining: 420ms\n",
            "708:\tlearn: 0.3135528\ttotal: 1.02s\tremaining: 419ms\n",
            "709:\tlearn: 0.3133388\ttotal: 1.02s\tremaining: 418ms\n",
            "710:\tlearn: 0.3132256\ttotal: 1.02s\tremaining: 416ms\n",
            "711:\tlearn: 0.3129425\ttotal: 1.02s\tremaining: 415ms\n",
            "712:\tlearn: 0.3127907\ttotal: 1.03s\tremaining: 413ms\n",
            "713:\tlearn: 0.3126648\ttotal: 1.03s\tremaining: 412ms\n",
            "714:\tlearn: 0.3124103\ttotal: 1.03s\tremaining: 410ms\n",
            "715:\tlearn: 0.3122476\ttotal: 1.03s\tremaining: 409ms\n",
            "716:\tlearn: 0.3121653\ttotal: 1.03s\tremaining: 407ms\n",
            "717:\tlearn: 0.3120558\ttotal: 1.03s\tremaining: 406ms\n",
            "718:\tlearn: 0.3118803\ttotal: 1.03s\tremaining: 404ms\n",
            "719:\tlearn: 0.3115521\ttotal: 1.03s\tremaining: 403ms\n",
            "720:\tlearn: 0.3113558\ttotal: 1.04s\tremaining: 401ms\n",
            "721:\tlearn: 0.3110294\ttotal: 1.04s\tremaining: 400ms\n",
            "722:\tlearn: 0.3108808\ttotal: 1.04s\tremaining: 398ms\n",
            "723:\tlearn: 0.3106052\ttotal: 1.04s\tremaining: 397ms\n",
            "724:\tlearn: 0.3103252\ttotal: 1.04s\tremaining: 395ms\n",
            "725:\tlearn: 0.3102365\ttotal: 1.04s\tremaining: 394ms\n",
            "726:\tlearn: 0.3099476\ttotal: 1.04s\tremaining: 392ms\n",
            "727:\tlearn: 0.3096417\ttotal: 1.05s\tremaining: 391ms\n",
            "728:\tlearn: 0.3093083\ttotal: 1.05s\tremaining: 390ms\n",
            "729:\tlearn: 0.3089457\ttotal: 1.05s\tremaining: 388ms\n",
            "730:\tlearn: 0.3086814\ttotal: 1.05s\tremaining: 387ms\n",
            "731:\tlearn: 0.3085309\ttotal: 1.05s\tremaining: 385ms\n",
            "732:\tlearn: 0.3083961\ttotal: 1.05s\tremaining: 384ms\n",
            "733:\tlearn: 0.3082976\ttotal: 1.06s\tremaining: 383ms\n",
            "734:\tlearn: 0.3082170\ttotal: 1.06s\tremaining: 382ms\n",
            "735:\tlearn: 0.3080837\ttotal: 1.06s\tremaining: 382ms\n",
            "736:\tlearn: 0.3078512\ttotal: 1.07s\tremaining: 381ms\n",
            "737:\tlearn: 0.3077399\ttotal: 1.07s\tremaining: 380ms\n",
            "738:\tlearn: 0.3075654\ttotal: 1.07s\tremaining: 378ms\n",
            "739:\tlearn: 0.3073544\ttotal: 1.07s\tremaining: 377ms\n",
            "740:\tlearn: 0.3072326\ttotal: 1.07s\tremaining: 375ms\n",
            "741:\tlearn: 0.3071182\ttotal: 1.07s\tremaining: 374ms\n",
            "742:\tlearn: 0.3069740\ttotal: 1.08s\tremaining: 372ms\n",
            "743:\tlearn: 0.3067389\ttotal: 1.08s\tremaining: 371ms\n",
            "744:\tlearn: 0.3064574\ttotal: 1.08s\tremaining: 369ms\n",
            "745:\tlearn: 0.3063901\ttotal: 1.08s\tremaining: 368ms\n",
            "746:\tlearn: 0.3061769\ttotal: 1.08s\tremaining: 366ms\n",
            "747:\tlearn: 0.3057912\ttotal: 1.08s\tremaining: 365ms\n",
            "748:\tlearn: 0.3056460\ttotal: 1.08s\tremaining: 363ms\n",
            "749:\tlearn: 0.3052707\ttotal: 1.09s\tremaining: 362ms\n",
            "750:\tlearn: 0.3050630\ttotal: 1.09s\tremaining: 361ms\n",
            "751:\tlearn: 0.3048161\ttotal: 1.09s\tremaining: 360ms\n",
            "752:\tlearn: 0.3047687\ttotal: 1.09s\tremaining: 358ms\n",
            "753:\tlearn: 0.3046297\ttotal: 1.09s\tremaining: 357ms\n",
            "754:\tlearn: 0.3043150\ttotal: 1.1s\tremaining: 356ms\n",
            "755:\tlearn: 0.3041829\ttotal: 1.1s\tremaining: 355ms\n",
            "756:\tlearn: 0.3040480\ttotal: 1.1s\tremaining: 353ms\n",
            "757:\tlearn: 0.3038565\ttotal: 1.1s\tremaining: 352ms\n",
            "758:\tlearn: 0.3036952\ttotal: 1.1s\tremaining: 350ms\n",
            "759:\tlearn: 0.3035321\ttotal: 1.1s\tremaining: 349ms\n",
            "760:\tlearn: 0.3033037\ttotal: 1.1s\tremaining: 347ms\n",
            "761:\tlearn: 0.3028173\ttotal: 1.11s\tremaining: 346ms\n",
            "762:\tlearn: 0.3026439\ttotal: 1.11s\tremaining: 344ms\n",
            "763:\tlearn: 0.3025235\ttotal: 1.11s\tremaining: 343ms\n",
            "764:\tlearn: 0.3024734\ttotal: 1.11s\tremaining: 341ms\n",
            "765:\tlearn: 0.3023018\ttotal: 1.11s\tremaining: 339ms\n",
            "766:\tlearn: 0.3021598\ttotal: 1.11s\tremaining: 338ms\n",
            "767:\tlearn: 0.3019597\ttotal: 1.11s\tremaining: 336ms\n",
            "768:\tlearn: 0.3018274\ttotal: 1.11s\tremaining: 335ms\n",
            "769:\tlearn: 0.3016331\ttotal: 1.12s\tremaining: 334ms\n",
            "770:\tlearn: 0.3012381\ttotal: 1.12s\tremaining: 332ms\n",
            "771:\tlearn: 0.3010412\ttotal: 1.12s\tremaining: 331ms\n",
            "772:\tlearn: 0.3009075\ttotal: 1.12s\tremaining: 329ms\n",
            "773:\tlearn: 0.3008154\ttotal: 1.12s\tremaining: 328ms\n",
            "774:\tlearn: 0.3005777\ttotal: 1.12s\tremaining: 326ms\n",
            "775:\tlearn: 0.3004399\ttotal: 1.12s\tremaining: 325ms\n",
            "776:\tlearn: 0.3003369\ttotal: 1.13s\tremaining: 323ms\n",
            "777:\tlearn: 0.3001716\ttotal: 1.13s\tremaining: 322ms\n",
            "778:\tlearn: 0.3000329\ttotal: 1.13s\tremaining: 320ms\n",
            "779:\tlearn: 0.2997116\ttotal: 1.13s\tremaining: 319ms\n",
            "780:\tlearn: 0.2995761\ttotal: 1.13s\tremaining: 317ms\n",
            "781:\tlearn: 0.2992659\ttotal: 1.13s\tremaining: 316ms\n",
            "782:\tlearn: 0.2991402\ttotal: 1.13s\tremaining: 314ms\n",
            "783:\tlearn: 0.2988895\ttotal: 1.14s\tremaining: 313ms\n",
            "784:\tlearn: 0.2986489\ttotal: 1.14s\tremaining: 312ms\n",
            "785:\tlearn: 0.2985103\ttotal: 1.14s\tremaining: 311ms\n",
            "786:\tlearn: 0.2983212\ttotal: 1.14s\tremaining: 309ms\n",
            "787:\tlearn: 0.2980712\ttotal: 1.14s\tremaining: 308ms\n",
            "788:\tlearn: 0.2976559\ttotal: 1.15s\tremaining: 306ms\n",
            "789:\tlearn: 0.2974744\ttotal: 1.15s\tremaining: 305ms\n",
            "790:\tlearn: 0.2974098\ttotal: 1.15s\tremaining: 303ms\n",
            "791:\tlearn: 0.2972123\ttotal: 1.15s\tremaining: 302ms\n",
            "792:\tlearn: 0.2971581\ttotal: 1.15s\tremaining: 300ms\n",
            "793:\tlearn: 0.2971463\ttotal: 1.15s\tremaining: 299ms\n",
            "794:\tlearn: 0.2969778\ttotal: 1.15s\tremaining: 297ms\n",
            "795:\tlearn: 0.2967173\ttotal: 1.15s\tremaining: 296ms\n",
            "796:\tlearn: 0.2963008\ttotal: 1.16s\tremaining: 294ms\n",
            "797:\tlearn: 0.2962016\ttotal: 1.16s\tremaining: 293ms\n",
            "798:\tlearn: 0.2960740\ttotal: 1.16s\tremaining: 291ms\n",
            "799:\tlearn: 0.2960010\ttotal: 1.16s\tremaining: 290ms\n",
            "800:\tlearn: 0.2957362\ttotal: 1.16s\tremaining: 288ms\n",
            "801:\tlearn: 0.2955920\ttotal: 1.16s\tremaining: 287ms\n",
            "802:\tlearn: 0.2952219\ttotal: 1.16s\tremaining: 285ms\n",
            "803:\tlearn: 0.2952053\ttotal: 1.16s\tremaining: 284ms\n",
            "804:\tlearn: 0.2950541\ttotal: 1.17s\tremaining: 282ms\n",
            "805:\tlearn: 0.2949424\ttotal: 1.17s\tremaining: 281ms\n",
            "806:\tlearn: 0.2948200\ttotal: 1.17s\tremaining: 279ms\n",
            "807:\tlearn: 0.2946830\ttotal: 1.17s\tremaining: 278ms\n",
            "808:\tlearn: 0.2944564\ttotal: 1.17s\tremaining: 276ms\n",
            "809:\tlearn: 0.2940199\ttotal: 1.17s\tremaining: 275ms\n",
            "810:\tlearn: 0.2938232\ttotal: 1.17s\tremaining: 273ms\n",
            "811:\tlearn: 0.2936968\ttotal: 1.17s\tremaining: 272ms\n",
            "812:\tlearn: 0.2934379\ttotal: 1.18s\tremaining: 270ms\n",
            "813:\tlearn: 0.2932832\ttotal: 1.18s\tremaining: 269ms\n",
            "814:\tlearn: 0.2931515\ttotal: 1.18s\tremaining: 267ms\n",
            "815:\tlearn: 0.2930702\ttotal: 1.18s\tremaining: 266ms\n",
            "816:\tlearn: 0.2929711\ttotal: 1.18s\tremaining: 265ms\n",
            "817:\tlearn: 0.2926221\ttotal: 1.18s\tremaining: 263ms\n",
            "818:\tlearn: 0.2924087\ttotal: 1.18s\tremaining: 262ms\n",
            "819:\tlearn: 0.2920943\ttotal: 1.18s\tremaining: 260ms\n",
            "820:\tlearn: 0.2918056\ttotal: 1.19s\tremaining: 259ms\n",
            "821:\tlearn: 0.2916788\ttotal: 1.19s\tremaining: 257ms\n",
            "822:\tlearn: 0.2914886\ttotal: 1.19s\tremaining: 256ms\n",
            "823:\tlearn: 0.2913118\ttotal: 1.19s\tremaining: 254ms\n",
            "824:\tlearn: 0.2910679\ttotal: 1.19s\tremaining: 253ms\n",
            "825:\tlearn: 0.2909064\ttotal: 1.19s\tremaining: 251ms\n",
            "826:\tlearn: 0.2907891\ttotal: 1.19s\tremaining: 250ms\n",
            "827:\tlearn: 0.2906863\ttotal: 1.2s\tremaining: 248ms\n",
            "828:\tlearn: 0.2904511\ttotal: 1.2s\tremaining: 247ms\n",
            "829:\tlearn: 0.2901988\ttotal: 1.2s\tremaining: 245ms\n",
            "830:\tlearn: 0.2900166\ttotal: 1.2s\tremaining: 244ms\n",
            "831:\tlearn: 0.2898479\ttotal: 1.2s\tremaining: 243ms\n",
            "832:\tlearn: 0.2896478\ttotal: 1.2s\tremaining: 241ms\n",
            "833:\tlearn: 0.2893635\ttotal: 1.2s\tremaining: 240ms\n",
            "834:\tlearn: 0.2890780\ttotal: 1.2s\tremaining: 238ms\n",
            "835:\tlearn: 0.2889663\ttotal: 1.21s\tremaining: 237ms\n",
            "836:\tlearn: 0.2888110\ttotal: 1.21s\tremaining: 235ms\n",
            "837:\tlearn: 0.2886058\ttotal: 1.21s\tremaining: 234ms\n",
            "838:\tlearn: 0.2884834\ttotal: 1.21s\tremaining: 232ms\n",
            "839:\tlearn: 0.2883565\ttotal: 1.21s\tremaining: 231ms\n",
            "840:\tlearn: 0.2883062\ttotal: 1.21s\tremaining: 229ms\n",
            "841:\tlearn: 0.2881362\ttotal: 1.21s\tremaining: 228ms\n",
            "842:\tlearn: 0.2880461\ttotal: 1.22s\tremaining: 226ms\n",
            "843:\tlearn: 0.2879594\ttotal: 1.22s\tremaining: 225ms\n",
            "844:\tlearn: 0.2878493\ttotal: 1.22s\tremaining: 223ms\n",
            "845:\tlearn: 0.2877564\ttotal: 1.22s\tremaining: 222ms\n",
            "846:\tlearn: 0.2876941\ttotal: 1.22s\tremaining: 221ms\n",
            "847:\tlearn: 0.2875306\ttotal: 1.22s\tremaining: 219ms\n",
            "848:\tlearn: 0.2873840\ttotal: 1.22s\tremaining: 218ms\n",
            "849:\tlearn: 0.2872068\ttotal: 1.23s\tremaining: 216ms\n",
            "850:\tlearn: 0.2867964\ttotal: 1.23s\tremaining: 215ms\n",
            "851:\tlearn: 0.2865908\ttotal: 1.23s\tremaining: 213ms\n",
            "852:\tlearn: 0.2863376\ttotal: 1.23s\tremaining: 212ms\n",
            "853:\tlearn: 0.2861609\ttotal: 1.23s\tremaining: 210ms\n",
            "854:\tlearn: 0.2860701\ttotal: 1.23s\tremaining: 209ms\n",
            "855:\tlearn: 0.2859455\ttotal: 1.23s\tremaining: 207ms\n",
            "856:\tlearn: 0.2858522\ttotal: 1.23s\tremaining: 206ms\n",
            "857:\tlearn: 0.2857507\ttotal: 1.24s\tremaining: 205ms\n",
            "858:\tlearn: 0.2856630\ttotal: 1.24s\tremaining: 203ms\n",
            "859:\tlearn: 0.2854916\ttotal: 1.24s\tremaining: 202ms\n",
            "860:\tlearn: 0.2853809\ttotal: 1.24s\tremaining: 200ms\n",
            "861:\tlearn: 0.2851202\ttotal: 1.24s\tremaining: 199ms\n",
            "862:\tlearn: 0.2849786\ttotal: 1.24s\tremaining: 197ms\n",
            "863:\tlearn: 0.2847927\ttotal: 1.24s\tremaining: 196ms\n",
            "864:\tlearn: 0.2844995\ttotal: 1.25s\tremaining: 194ms\n",
            "865:\tlearn: 0.2843704\ttotal: 1.25s\tremaining: 193ms\n",
            "866:\tlearn: 0.2840724\ttotal: 1.25s\tremaining: 192ms\n",
            "867:\tlearn: 0.2838757\ttotal: 1.25s\tremaining: 190ms\n",
            "868:\tlearn: 0.2835995\ttotal: 1.25s\tremaining: 189ms\n",
            "869:\tlearn: 0.2833809\ttotal: 1.25s\tremaining: 187ms\n",
            "870:\tlearn: 0.2832386\ttotal: 1.25s\tremaining: 186ms\n",
            "871:\tlearn: 0.2829471\ttotal: 1.25s\tremaining: 184ms\n",
            "872:\tlearn: 0.2825681\ttotal: 1.26s\tremaining: 183ms\n",
            "873:\tlearn: 0.2824145\ttotal: 1.26s\tremaining: 181ms\n",
            "874:\tlearn: 0.2823222\ttotal: 1.26s\tremaining: 180ms\n",
            "875:\tlearn: 0.2822693\ttotal: 1.26s\tremaining: 178ms\n",
            "876:\tlearn: 0.2819900\ttotal: 1.26s\tremaining: 177ms\n",
            "877:\tlearn: 0.2818827\ttotal: 1.26s\tremaining: 176ms\n",
            "878:\tlearn: 0.2816098\ttotal: 1.26s\tremaining: 174ms\n",
            "879:\tlearn: 0.2814721\ttotal: 1.27s\tremaining: 173ms\n",
            "880:\tlearn: 0.2812920\ttotal: 1.27s\tremaining: 171ms\n",
            "881:\tlearn: 0.2810469\ttotal: 1.27s\tremaining: 170ms\n",
            "882:\tlearn: 0.2808815\ttotal: 1.27s\tremaining: 168ms\n",
            "883:\tlearn: 0.2807731\ttotal: 1.27s\tremaining: 167ms\n",
            "884:\tlearn: 0.2805942\ttotal: 1.27s\tremaining: 165ms\n",
            "885:\tlearn: 0.2804573\ttotal: 1.27s\tremaining: 164ms\n",
            "886:\tlearn: 0.2803245\ttotal: 1.28s\tremaining: 163ms\n",
            "887:\tlearn: 0.2801862\ttotal: 1.28s\tremaining: 161ms\n",
            "888:\tlearn: 0.2801111\ttotal: 1.28s\tremaining: 160ms\n",
            "889:\tlearn: 0.2800287\ttotal: 1.28s\tremaining: 159ms\n",
            "890:\tlearn: 0.2798370\ttotal: 1.28s\tremaining: 157ms\n",
            "891:\tlearn: 0.2796127\ttotal: 1.29s\tremaining: 156ms\n",
            "892:\tlearn: 0.2793934\ttotal: 1.29s\tremaining: 154ms\n",
            "893:\tlearn: 0.2792708\ttotal: 1.29s\tremaining: 153ms\n",
            "894:\tlearn: 0.2791146\ttotal: 1.29s\tremaining: 152ms\n",
            "895:\tlearn: 0.2790123\ttotal: 1.29s\tremaining: 150ms\n",
            "896:\tlearn: 0.2788266\ttotal: 1.29s\tremaining: 149ms\n",
            "897:\tlearn: 0.2787247\ttotal: 1.3s\tremaining: 147ms\n",
            "898:\tlearn: 0.2786717\ttotal: 1.3s\tremaining: 146ms\n",
            "899:\tlearn: 0.2785501\ttotal: 1.3s\tremaining: 144ms\n",
            "900:\tlearn: 0.2783916\ttotal: 1.3s\tremaining: 143ms\n",
            "901:\tlearn: 0.2783048\ttotal: 1.3s\tremaining: 141ms\n",
            "902:\tlearn: 0.2781073\ttotal: 1.3s\tremaining: 140ms\n",
            "903:\tlearn: 0.2778695\ttotal: 1.3s\tremaining: 139ms\n",
            "904:\tlearn: 0.2777985\ttotal: 1.3s\tremaining: 137ms\n",
            "905:\tlearn: 0.2776311\ttotal: 1.31s\tremaining: 136ms\n",
            "906:\tlearn: 0.2774705\ttotal: 1.31s\tremaining: 134ms\n",
            "907:\tlearn: 0.2772546\ttotal: 1.31s\tremaining: 133ms\n",
            "908:\tlearn: 0.2771266\ttotal: 1.31s\tremaining: 131ms\n",
            "909:\tlearn: 0.2769332\ttotal: 1.31s\tremaining: 130ms\n",
            "910:\tlearn: 0.2768204\ttotal: 1.31s\tremaining: 128ms\n",
            "911:\tlearn: 0.2766503\ttotal: 1.31s\tremaining: 127ms\n",
            "912:\tlearn: 0.2764458\ttotal: 1.32s\tremaining: 125ms\n",
            "913:\tlearn: 0.2762511\ttotal: 1.32s\tremaining: 124ms\n",
            "914:\tlearn: 0.2760254\ttotal: 1.32s\tremaining: 123ms\n",
            "915:\tlearn: 0.2758496\ttotal: 1.32s\tremaining: 121ms\n",
            "916:\tlearn: 0.2756899\ttotal: 1.32s\tremaining: 120ms\n",
            "917:\tlearn: 0.2755744\ttotal: 1.32s\tremaining: 118ms\n",
            "918:\tlearn: 0.2754512\ttotal: 1.32s\tremaining: 117ms\n",
            "919:\tlearn: 0.2752716\ttotal: 1.33s\tremaining: 115ms\n",
            "920:\tlearn: 0.2750525\ttotal: 1.33s\tremaining: 114ms\n",
            "921:\tlearn: 0.2749779\ttotal: 1.33s\tremaining: 113ms\n",
            "922:\tlearn: 0.2748659\ttotal: 1.33s\tremaining: 111ms\n",
            "923:\tlearn: 0.2746859\ttotal: 1.33s\tremaining: 110ms\n",
            "924:\tlearn: 0.2745615\ttotal: 1.34s\tremaining: 108ms\n",
            "925:\tlearn: 0.2743005\ttotal: 1.34s\tremaining: 107ms\n",
            "926:\tlearn: 0.2741596\ttotal: 1.34s\tremaining: 105ms\n",
            "927:\tlearn: 0.2739840\ttotal: 1.34s\tremaining: 104ms\n",
            "928:\tlearn: 0.2738738\ttotal: 1.34s\tremaining: 103ms\n",
            "929:\tlearn: 0.2738131\ttotal: 1.34s\tremaining: 101ms\n",
            "930:\tlearn: 0.2736533\ttotal: 1.34s\tremaining: 99.6ms\n",
            "931:\tlearn: 0.2735606\ttotal: 1.34s\tremaining: 98.2ms\n",
            "932:\tlearn: 0.2733554\ttotal: 1.35s\tremaining: 96.8ms\n",
            "933:\tlearn: 0.2732123\ttotal: 1.35s\tremaining: 95.3ms\n",
            "934:\tlearn: 0.2729806\ttotal: 1.35s\tremaining: 93.9ms\n",
            "935:\tlearn: 0.2728763\ttotal: 1.35s\tremaining: 92.4ms\n",
            "936:\tlearn: 0.2728003\ttotal: 1.35s\tremaining: 91ms\n",
            "937:\tlearn: 0.2726642\ttotal: 1.35s\tremaining: 89.5ms\n",
            "938:\tlearn: 0.2724193\ttotal: 1.35s\tremaining: 88.1ms\n",
            "939:\tlearn: 0.2721446\ttotal: 1.36s\tremaining: 86.6ms\n",
            "940:\tlearn: 0.2719508\ttotal: 1.36s\tremaining: 85.2ms\n",
            "941:\tlearn: 0.2717928\ttotal: 1.36s\tremaining: 83.7ms\n",
            "942:\tlearn: 0.2716914\ttotal: 1.36s\tremaining: 82.3ms\n",
            "943:\tlearn: 0.2714316\ttotal: 1.36s\tremaining: 80.8ms\n",
            "944:\tlearn: 0.2713608\ttotal: 1.36s\tremaining: 79.4ms\n",
            "945:\tlearn: 0.2711602\ttotal: 1.36s\tremaining: 77.9ms\n",
            "946:\tlearn: 0.2709318\ttotal: 1.37s\tremaining: 76.5ms\n",
            "947:\tlearn: 0.2708679\ttotal: 1.37s\tremaining: 75ms\n",
            "948:\tlearn: 0.2705899\ttotal: 1.37s\tremaining: 73.6ms\n",
            "949:\tlearn: 0.2704843\ttotal: 1.37s\tremaining: 72.1ms\n",
            "950:\tlearn: 0.2704142\ttotal: 1.37s\tremaining: 70.7ms\n",
            "951:\tlearn: 0.2702505\ttotal: 1.37s\tremaining: 69.3ms\n",
            "952:\tlearn: 0.2701301\ttotal: 1.37s\tremaining: 67.8ms\n",
            "953:\tlearn: 0.2699043\ttotal: 1.38s\tremaining: 66.4ms\n",
            "954:\tlearn: 0.2697563\ttotal: 1.38s\tremaining: 64.9ms\n",
            "955:\tlearn: 0.2696072\ttotal: 1.38s\tremaining: 63.5ms\n",
            "956:\tlearn: 0.2694220\ttotal: 1.38s\tremaining: 62ms\n",
            "957:\tlearn: 0.2692647\ttotal: 1.38s\tremaining: 60.6ms\n",
            "958:\tlearn: 0.2690632\ttotal: 1.38s\tremaining: 59.2ms\n",
            "959:\tlearn: 0.2689561\ttotal: 1.39s\tremaining: 57.7ms\n",
            "960:\tlearn: 0.2686286\ttotal: 1.39s\tremaining: 56.3ms\n",
            "961:\tlearn: 0.2684236\ttotal: 1.39s\tremaining: 55ms\n",
            "962:\tlearn: 0.2682183\ttotal: 1.39s\tremaining: 53.5ms\n",
            "963:\tlearn: 0.2681170\ttotal: 1.39s\tremaining: 52.1ms\n",
            "964:\tlearn: 0.2679075\ttotal: 1.4s\tremaining: 50.6ms\n",
            "965:\tlearn: 0.2677363\ttotal: 1.4s\tremaining: 49.2ms\n",
            "966:\tlearn: 0.2675577\ttotal: 1.4s\tremaining: 47.7ms\n",
            "967:\tlearn: 0.2673599\ttotal: 1.4s\tremaining: 46.3ms\n",
            "968:\tlearn: 0.2672593\ttotal: 1.4s\tremaining: 44.8ms\n",
            "969:\tlearn: 0.2670997\ttotal: 1.4s\tremaining: 43.4ms\n",
            "970:\tlearn: 0.2670552\ttotal: 1.4s\tremaining: 41.9ms\n",
            "971:\tlearn: 0.2669171\ttotal: 1.41s\tremaining: 40.5ms\n",
            "972:\tlearn: 0.2668228\ttotal: 1.41s\tremaining: 39ms\n",
            "973:\tlearn: 0.2665953\ttotal: 1.41s\tremaining: 37.6ms\n",
            "974:\tlearn: 0.2664254\ttotal: 1.41s\tremaining: 36.1ms\n",
            "975:\tlearn: 0.2662588\ttotal: 1.41s\tremaining: 34.7ms\n",
            "976:\tlearn: 0.2661612\ttotal: 1.41s\tremaining: 33.2ms\n",
            "977:\tlearn: 0.2660152\ttotal: 1.41s\tremaining: 31.8ms\n",
            "978:\tlearn: 0.2657712\ttotal: 1.42s\tremaining: 30.4ms\n",
            "979:\tlearn: 0.2655026\ttotal: 1.42s\tremaining: 28.9ms\n",
            "980:\tlearn: 0.2654025\ttotal: 1.42s\tremaining: 27.5ms\n",
            "981:\tlearn: 0.2651935\ttotal: 1.42s\tremaining: 26ms\n",
            "982:\tlearn: 0.2650073\ttotal: 1.42s\tremaining: 24.6ms\n",
            "983:\tlearn: 0.2648720\ttotal: 1.42s\tremaining: 23.1ms\n",
            "984:\tlearn: 0.2647666\ttotal: 1.42s\tremaining: 21.7ms\n",
            "985:\tlearn: 0.2645571\ttotal: 1.42s\tremaining: 20.2ms\n",
            "986:\tlearn: 0.2644713\ttotal: 1.43s\tremaining: 18.8ms\n",
            "987:\tlearn: 0.2642635\ttotal: 1.43s\tremaining: 17.3ms\n",
            "988:\tlearn: 0.2641904\ttotal: 1.43s\tremaining: 15.9ms\n",
            "989:\tlearn: 0.2640152\ttotal: 1.43s\tremaining: 14.5ms\n",
            "990:\tlearn: 0.2639384\ttotal: 1.43s\tremaining: 13ms\n",
            "991:\tlearn: 0.2637812\ttotal: 1.43s\tremaining: 11.6ms\n",
            "992:\tlearn: 0.2636568\ttotal: 1.44s\tremaining: 10.1ms\n",
            "993:\tlearn: 0.2635737\ttotal: 1.44s\tremaining: 8.67ms\n",
            "994:\tlearn: 0.2634378\ttotal: 1.44s\tremaining: 7.23ms\n",
            "995:\tlearn: 0.2633001\ttotal: 1.44s\tremaining: 5.78ms\n",
            "996:\tlearn: 0.2632481\ttotal: 1.44s\tremaining: 4.33ms\n",
            "997:\tlearn: 0.2632303\ttotal: 1.44s\tremaining: 2.89ms\n",
            "998:\tlearn: 0.2630494\ttotal: 1.44s\tremaining: 1.44ms\n",
            "999:\tlearn: 0.2629250\ttotal: 1.44s\tremaining: 0us\n",
            "Learning rate set to 0.00421\n",
            "0:\tlearn: 0.6909604\ttotal: 1.07ms\tremaining: 1.07s\n",
            "1:\tlearn: 0.6887233\ttotal: 1.9ms\tremaining: 948ms\n",
            "2:\tlearn: 0.6865613\ttotal: 2.7ms\tremaining: 897ms\n",
            "3:\tlearn: 0.6847893\ttotal: 3.48ms\tremaining: 867ms\n",
            "4:\tlearn: 0.6829026\ttotal: 4.25ms\tremaining: 845ms\n",
            "5:\tlearn: 0.6812562\ttotal: 4.96ms\tremaining: 822ms\n",
            "6:\tlearn: 0.6798473\ttotal: 5.61ms\tremaining: 795ms\n",
            "7:\tlearn: 0.6783426\ttotal: 6.35ms\tremaining: 787ms\n",
            "8:\tlearn: 0.6763381\ttotal: 7.12ms\tremaining: 784ms\n",
            "9:\tlearn: 0.6746919\ttotal: 7.94ms\tremaining: 786ms\n",
            "10:\tlearn: 0.6733433\ttotal: 8.43ms\tremaining: 758ms\n",
            "11:\tlearn: 0.6715876\ttotal: 9.18ms\tremaining: 756ms\n",
            "12:\tlearn: 0.6699389\ttotal: 10.1ms\tremaining: 768ms\n",
            "13:\tlearn: 0.6678142\ttotal: 11ms\tremaining: 774ms\n",
            "14:\tlearn: 0.6657083\ttotal: 11.8ms\tremaining: 775ms\n",
            "15:\tlearn: 0.6635035\ttotal: 12.6ms\tremaining: 774ms\n",
            "16:\tlearn: 0.6616519\ttotal: 13.4ms\tremaining: 774ms\n",
            "17:\tlearn: 0.6594167\ttotal: 14.2ms\tremaining: 773ms\n",
            "18:\tlearn: 0.6577898\ttotal: 14.8ms\tremaining: 762ms\n",
            "19:\tlearn: 0.6556997\ttotal: 15.5ms\tremaining: 762ms\n",
            "20:\tlearn: 0.6544803\ttotal: 16.1ms\tremaining: 751ms\n",
            "21:\tlearn: 0.6531910\ttotal: 16.6ms\tremaining: 737ms\n",
            "22:\tlearn: 0.6510400\ttotal: 17.3ms\tremaining: 736ms\n",
            "23:\tlearn: 0.6486766\ttotal: 18.1ms\tremaining: 737ms\n",
            "24:\tlearn: 0.6468230\ttotal: 18.9ms\tremaining: 739ms\n",
            "25:\tlearn: 0.6446482\ttotal: 19.7ms\tremaining: 739ms\n",
            "26:\tlearn: 0.6440701\ttotal: 20.2ms\tremaining: 727ms\n",
            "27:\tlearn: 0.6422534\ttotal: 20.9ms\tremaining: 725ms\n",
            "28:\tlearn: 0.6406101\ttotal: 21.7ms\tremaining: 727ms\n",
            "29:\tlearn: 0.6386726\ttotal: 22.5ms\tremaining: 726ms\n",
            "30:\tlearn: 0.6373595\ttotal: 24.8ms\tremaining: 776ms\n",
            "31:\tlearn: 0.6358526\ttotal: 25.8ms\tremaining: 779ms\n",
            "32:\tlearn: 0.6343923\ttotal: 27.1ms\tremaining: 793ms\n",
            "33:\tlearn: 0.6324463\ttotal: 27.8ms\tremaining: 789ms\n",
            "34:\tlearn: 0.6314556\ttotal: 28.6ms\tremaining: 788ms\n",
            "35:\tlearn: 0.6294088\ttotal: 29.3ms\tremaining: 785ms\n",
            "36:\tlearn: 0.6272224\ttotal: 30.2ms\tremaining: 785ms\n",
            "37:\tlearn: 0.6251986\ttotal: 30.9ms\tremaining: 783ms\n",
            "38:\tlearn: 0.6234614\ttotal: 31.8ms\tremaining: 783ms\n",
            "39:\tlearn: 0.6215238\ttotal: 32.6ms\tremaining: 782ms\n",
            "40:\tlearn: 0.6201026\ttotal: 33.3ms\tremaining: 778ms\n",
            "41:\tlearn: 0.6184936\ttotal: 34ms\tremaining: 775ms\n",
            "42:\tlearn: 0.6164827\ttotal: 34.7ms\tremaining: 772ms\n",
            "43:\tlearn: 0.6144091\ttotal: 35.5ms\tremaining: 771ms\n",
            "44:\tlearn: 0.6134427\ttotal: 36.2ms\tremaining: 769ms\n",
            "45:\tlearn: 0.6119801\ttotal: 37.1ms\tremaining: 769ms\n",
            "46:\tlearn: 0.6106368\ttotal: 37.8ms\tremaining: 767ms\n",
            "47:\tlearn: 0.6090495\ttotal: 38.6ms\tremaining: 766ms\n",
            "48:\tlearn: 0.6077262\ttotal: 39.4ms\tremaining: 764ms\n",
            "49:\tlearn: 0.6064724\ttotal: 40.1ms\tremaining: 761ms\n",
            "50:\tlearn: 0.6058289\ttotal: 40.6ms\tremaining: 755ms\n",
            "51:\tlearn: 0.6044981\ttotal: 41.3ms\tremaining: 753ms\n",
            "52:\tlearn: 0.6031068\ttotal: 42.1ms\tremaining: 753ms\n",
            "53:\tlearn: 0.6021693\ttotal: 42.9ms\tremaining: 751ms\n",
            "54:\tlearn: 0.6013361\ttotal: 43.6ms\tremaining: 749ms\n",
            "55:\tlearn: 0.5998882\ttotal: 44.3ms\tremaining: 746ms\n",
            "56:\tlearn: 0.5985323\ttotal: 45.1ms\tremaining: 746ms\n",
            "57:\tlearn: 0.5969197\ttotal: 45.8ms\tremaining: 745ms\n",
            "58:\tlearn: 0.5957718\ttotal: 46.6ms\tremaining: 744ms\n",
            "59:\tlearn: 0.5938719\ttotal: 47.4ms\tremaining: 743ms\n",
            "60:\tlearn: 0.5926002\ttotal: 48.3ms\tremaining: 743ms\n",
            "61:\tlearn: 0.5914776\ttotal: 49.1ms\tremaining: 742ms\n",
            "62:\tlearn: 0.5902093\ttotal: 49.8ms\tremaining: 741ms\n",
            "63:\tlearn: 0.5887765\ttotal: 50.6ms\tremaining: 741ms\n",
            "64:\tlearn: 0.5872582\ttotal: 51.4ms\tremaining: 739ms\n",
            "65:\tlearn: 0.5856328\ttotal: 52.2ms\tremaining: 738ms\n",
            "66:\tlearn: 0.5837375\ttotal: 52.9ms\tremaining: 737ms\n",
            "67:\tlearn: 0.5823523\ttotal: 53.7ms\tremaining: 736ms\n",
            "68:\tlearn: 0.5809438\ttotal: 54.5ms\tremaining: 735ms\n",
            "69:\tlearn: 0.5792896\ttotal: 55.3ms\tremaining: 734ms\n",
            "70:\tlearn: 0.5782388\ttotal: 56ms\tremaining: 733ms\n",
            "71:\tlearn: 0.5766523\ttotal: 56.7ms\tremaining: 731ms\n",
            "72:\tlearn: 0.5751709\ttotal: 57.4ms\tremaining: 729ms\n",
            "73:\tlearn: 0.5740525\ttotal: 58.1ms\tremaining: 728ms\n",
            "74:\tlearn: 0.5725307\ttotal: 58.9ms\tremaining: 727ms\n",
            "75:\tlearn: 0.5711358\ttotal: 59.8ms\tremaining: 727ms\n",
            "76:\tlearn: 0.5700358\ttotal: 60.5ms\tremaining: 725ms\n",
            "77:\tlearn: 0.5686908\ttotal: 61.3ms\tremaining: 724ms\n",
            "78:\tlearn: 0.5670510\ttotal: 62ms\tremaining: 723ms\n",
            "79:\tlearn: 0.5657536\ttotal: 62.8ms\tremaining: 722ms\n",
            "80:\tlearn: 0.5644171\ttotal: 63.6ms\tremaining: 721ms\n",
            "81:\tlearn: 0.5630023\ttotal: 64.3ms\tremaining: 720ms\n",
            "82:\tlearn: 0.5614064\ttotal: 65.1ms\tremaining: 719ms\n",
            "83:\tlearn: 0.5598634\ttotal: 65.8ms\tremaining: 718ms\n",
            "84:\tlearn: 0.5586397\ttotal: 66.5ms\tremaining: 716ms\n",
            "85:\tlearn: 0.5573839\ttotal: 67.2ms\tremaining: 714ms\n",
            "86:\tlearn: 0.5563553\ttotal: 67.9ms\tremaining: 712ms\n",
            "87:\tlearn: 0.5552882\ttotal: 68.6ms\tremaining: 711ms\n",
            "88:\tlearn: 0.5536613\ttotal: 69.3ms\tremaining: 710ms\n",
            "89:\tlearn: 0.5520825\ttotal: 70.1ms\tremaining: 709ms\n",
            "90:\tlearn: 0.5510198\ttotal: 70.8ms\tremaining: 707ms\n",
            "91:\tlearn: 0.5498919\ttotal: 71.6ms\tremaining: 706ms\n",
            "92:\tlearn: 0.5492407\ttotal: 72ms\tremaining: 702ms\n",
            "93:\tlearn: 0.5481299\ttotal: 72.8ms\tremaining: 701ms\n",
            "94:\tlearn: 0.5474018\ttotal: 73.3ms\tremaining: 698ms\n",
            "95:\tlearn: 0.5464398\ttotal: 74ms\tremaining: 697ms\n",
            "96:\tlearn: 0.5459188\ttotal: 74.7ms\tremaining: 695ms\n",
            "97:\tlearn: 0.5452328\ttotal: 75.1ms\tremaining: 691ms\n",
            "98:\tlearn: 0.5438905\ttotal: 75.7ms\tremaining: 689ms\n",
            "99:\tlearn: 0.5422249\ttotal: 76.4ms\tremaining: 688ms\n",
            "100:\tlearn: 0.5409159\ttotal: 77.2ms\tremaining: 687ms\n",
            "101:\tlearn: 0.5405787\ttotal: 77.5ms\tremaining: 682ms\n",
            "102:\tlearn: 0.5395811\ttotal: 78.2ms\tremaining: 681ms\n",
            "103:\tlearn: 0.5388539\ttotal: 79ms\tremaining: 680ms\n",
            "104:\tlearn: 0.5375826\ttotal: 79.7ms\tremaining: 680ms\n",
            "105:\tlearn: 0.5364113\ttotal: 80.6ms\tremaining: 679ms\n",
            "106:\tlearn: 0.5353409\ttotal: 81.3ms\tremaining: 679ms\n",
            "107:\tlearn: 0.5342072\ttotal: 82.1ms\tremaining: 678ms\n",
            "108:\tlearn: 0.5331511\ttotal: 82.8ms\tremaining: 677ms\n",
            "109:\tlearn: 0.5318971\ttotal: 83.5ms\tremaining: 675ms\n",
            "110:\tlearn: 0.5308674\ttotal: 84.1ms\tremaining: 674ms\n",
            "111:\tlearn: 0.5304140\ttotal: 84.6ms\tremaining: 671ms\n",
            "112:\tlearn: 0.5297496\ttotal: 85.2ms\tremaining: 669ms\n",
            "113:\tlearn: 0.5290610\ttotal: 85.9ms\tremaining: 667ms\n",
            "114:\tlearn: 0.5280323\ttotal: 86.6ms\tremaining: 666ms\n",
            "115:\tlearn: 0.5268344\ttotal: 87.4ms\tremaining: 666ms\n",
            "116:\tlearn: 0.5258155\ttotal: 88.1ms\tremaining: 665ms\n",
            "117:\tlearn: 0.5246273\ttotal: 88.8ms\tremaining: 664ms\n",
            "118:\tlearn: 0.5240827\ttotal: 89.6ms\tremaining: 664ms\n",
            "119:\tlearn: 0.5237455\ttotal: 90.2ms\tremaining: 661ms\n",
            "120:\tlearn: 0.5226977\ttotal: 90.9ms\tremaining: 660ms\n",
            "121:\tlearn: 0.5217318\ttotal: 91.8ms\tremaining: 661ms\n",
            "122:\tlearn: 0.5206317\ttotal: 92.6ms\tremaining: 660ms\n",
            "123:\tlearn: 0.5197052\ttotal: 93.4ms\tremaining: 660ms\n",
            "124:\tlearn: 0.5190064\ttotal: 94.2ms\tremaining: 659ms\n",
            "125:\tlearn: 0.5182638\ttotal: 95ms\tremaining: 659ms\n",
            "126:\tlearn: 0.5171628\ttotal: 95.7ms\tremaining: 658ms\n",
            "127:\tlearn: 0.5158562\ttotal: 96.5ms\tremaining: 658ms\n",
            "128:\tlearn: 0.5153539\ttotal: 97.4ms\tremaining: 657ms\n",
            "129:\tlearn: 0.5143721\ttotal: 98.2ms\tremaining: 657ms\n",
            "130:\tlearn: 0.5135334\ttotal: 99ms\tremaining: 657ms\n",
            "131:\tlearn: 0.5122839\ttotal: 99.8ms\tremaining: 656ms\n",
            "132:\tlearn: 0.5113240\ttotal: 101ms\tremaining: 656ms\n",
            "133:\tlearn: 0.5104539\ttotal: 101ms\tremaining: 655ms\n",
            "134:\tlearn: 0.5095600\ttotal: 102ms\tremaining: 654ms\n",
            "135:\tlearn: 0.5085333\ttotal: 103ms\tremaining: 654ms\n",
            "136:\tlearn: 0.5075805\ttotal: 104ms\tremaining: 653ms\n",
            "137:\tlearn: 0.5067516\ttotal: 104ms\tremaining: 652ms\n",
            "138:\tlearn: 0.5059698\ttotal: 105ms\tremaining: 652ms\n",
            "139:\tlearn: 0.5057261\ttotal: 106ms\tremaining: 649ms\n",
            "140:\tlearn: 0.5049291\ttotal: 106ms\tremaining: 648ms\n",
            "141:\tlearn: 0.5043259\ttotal: 107ms\tremaining: 647ms\n",
            "142:\tlearn: 0.5035401\ttotal: 108ms\tremaining: 647ms\n",
            "143:\tlearn: 0.5022055\ttotal: 109ms\tremaining: 646ms\n",
            "144:\tlearn: 0.5012762\ttotal: 110ms\tremaining: 646ms\n",
            "145:\tlearn: 0.5005689\ttotal: 110ms\tremaining: 645ms\n",
            "146:\tlearn: 0.4994479\ttotal: 111ms\tremaining: 645ms\n",
            "147:\tlearn: 0.4985276\ttotal: 112ms\tremaining: 644ms\n",
            "148:\tlearn: 0.4978411\ttotal: 113ms\tremaining: 644ms\n",
            "149:\tlearn: 0.4969636\ttotal: 114ms\tremaining: 643ms\n",
            "150:\tlearn: 0.4961743\ttotal: 114ms\tremaining: 643ms\n",
            "151:\tlearn: 0.4954505\ttotal: 115ms\tremaining: 643ms\n",
            "152:\tlearn: 0.4947269\ttotal: 116ms\tremaining: 642ms\n",
            "153:\tlearn: 0.4942996\ttotal: 116ms\tremaining: 640ms\n",
            "154:\tlearn: 0.4933167\ttotal: 117ms\tremaining: 639ms\n",
            "155:\tlearn: 0.4923970\ttotal: 118ms\tremaining: 638ms\n",
            "156:\tlearn: 0.4912890\ttotal: 119ms\tremaining: 638ms\n",
            "157:\tlearn: 0.4908084\ttotal: 120ms\tremaining: 637ms\n",
            "158:\tlearn: 0.4901730\ttotal: 120ms\tremaining: 637ms\n",
            "159:\tlearn: 0.4894366\ttotal: 121ms\tremaining: 636ms\n",
            "160:\tlearn: 0.4887836\ttotal: 122ms\tremaining: 635ms\n",
            "161:\tlearn: 0.4880944\ttotal: 123ms\tremaining: 634ms\n",
            "162:\tlearn: 0.4870660\ttotal: 123ms\tremaining: 634ms\n",
            "163:\tlearn: 0.4865461\ttotal: 124ms\tremaining: 633ms\n",
            "164:\tlearn: 0.4856532\ttotal: 125ms\tremaining: 633ms\n",
            "165:\tlearn: 0.4848126\ttotal: 126ms\tremaining: 632ms\n",
            "166:\tlearn: 0.4840295\ttotal: 127ms\tremaining: 631ms\n",
            "167:\tlearn: 0.4831828\ttotal: 127ms\tremaining: 631ms\n",
            "168:\tlearn: 0.4830193\ttotal: 128ms\tremaining: 628ms\n",
            "169:\tlearn: 0.4820352\ttotal: 128ms\tremaining: 627ms\n",
            "170:\tlearn: 0.4814748\ttotal: 129ms\tremaining: 626ms\n",
            "171:\tlearn: 0.4806605\ttotal: 130ms\tremaining: 625ms\n",
            "172:\tlearn: 0.4794763\ttotal: 131ms\tremaining: 624ms\n",
            "173:\tlearn: 0.4787989\ttotal: 131ms\tremaining: 623ms\n",
            "174:\tlearn: 0.4778969\ttotal: 132ms\tremaining: 623ms\n",
            "175:\tlearn: 0.4771245\ttotal: 133ms\tremaining: 622ms\n",
            "176:\tlearn: 0.4767169\ttotal: 134ms\tremaining: 622ms\n",
            "177:\tlearn: 0.4758633\ttotal: 135ms\tremaining: 621ms\n",
            "178:\tlearn: 0.4752972\ttotal: 135ms\tremaining: 621ms\n",
            "179:\tlearn: 0.4744927\ttotal: 136ms\tremaining: 620ms\n",
            "180:\tlearn: 0.4740209\ttotal: 137ms\tremaining: 620ms\n",
            "181:\tlearn: 0.4733363\ttotal: 138ms\tremaining: 619ms\n",
            "182:\tlearn: 0.4724207\ttotal: 138ms\tremaining: 618ms\n",
            "183:\tlearn: 0.4716752\ttotal: 139ms\tremaining: 617ms\n",
            "184:\tlearn: 0.4709617\ttotal: 140ms\tremaining: 617ms\n",
            "185:\tlearn: 0.4699422\ttotal: 141ms\tremaining: 616ms\n",
            "186:\tlearn: 0.4691934\ttotal: 142ms\tremaining: 615ms\n",
            "187:\tlearn: 0.4681753\ttotal: 142ms\tremaining: 615ms\n",
            "188:\tlearn: 0.4677537\ttotal: 143ms\tremaining: 614ms\n",
            "189:\tlearn: 0.4669218\ttotal: 144ms\tremaining: 614ms\n",
            "190:\tlearn: 0.4660478\ttotal: 145ms\tremaining: 613ms\n",
            "191:\tlearn: 0.4651434\ttotal: 145ms\tremaining: 612ms\n",
            "192:\tlearn: 0.4645534\ttotal: 146ms\tremaining: 611ms\n",
            "193:\tlearn: 0.4642395\ttotal: 147ms\tremaining: 609ms\n",
            "194:\tlearn: 0.4637059\ttotal: 147ms\tremaining: 608ms\n",
            "195:\tlearn: 0.4627901\ttotal: 148ms\tremaining: 608ms\n",
            "196:\tlearn: 0.4619231\ttotal: 149ms\tremaining: 607ms\n",
            "197:\tlearn: 0.4611545\ttotal: 150ms\tremaining: 606ms\n",
            "198:\tlearn: 0.4604178\ttotal: 150ms\tremaining: 605ms\n",
            "199:\tlearn: 0.4598926\ttotal: 151ms\tremaining: 605ms\n",
            "200:\tlearn: 0.4592251\ttotal: 152ms\tremaining: 604ms\n",
            "201:\tlearn: 0.4586555\ttotal: 153ms\tremaining: 604ms\n",
            "202:\tlearn: 0.4580796\ttotal: 154ms\tremaining: 603ms\n",
            "203:\tlearn: 0.4575941\ttotal: 154ms\tremaining: 602ms\n",
            "204:\tlearn: 0.4569770\ttotal: 155ms\tremaining: 601ms\n",
            "205:\tlearn: 0.4563707\ttotal: 156ms\tremaining: 601ms\n",
            "206:\tlearn: 0.4555149\ttotal: 157ms\tremaining: 600ms\n",
            "207:\tlearn: 0.4548209\ttotal: 157ms\tremaining: 599ms\n",
            "208:\tlearn: 0.4543824\ttotal: 158ms\tremaining: 598ms\n",
            "209:\tlearn: 0.4533863\ttotal: 159ms\tremaining: 597ms\n",
            "210:\tlearn: 0.4524290\ttotal: 160ms\tremaining: 597ms\n",
            "211:\tlearn: 0.4519120\ttotal: 160ms\tremaining: 596ms\n",
            "212:\tlearn: 0.4512051\ttotal: 161ms\tremaining: 595ms\n",
            "213:\tlearn: 0.4506764\ttotal: 162ms\tremaining: 595ms\n",
            "214:\tlearn: 0.4503989\ttotal: 162ms\tremaining: 593ms\n",
            "215:\tlearn: 0.4494571\ttotal: 163ms\tremaining: 592ms\n",
            "216:\tlearn: 0.4491103\ttotal: 164ms\tremaining: 591ms\n",
            "217:\tlearn: 0.4480275\ttotal: 165ms\tremaining: 590ms\n",
            "218:\tlearn: 0.4476258\ttotal: 165ms\tremaining: 590ms\n",
            "219:\tlearn: 0.4470321\ttotal: 166ms\tremaining: 589ms\n",
            "220:\tlearn: 0.4465859\ttotal: 167ms\tremaining: 588ms\n",
            "221:\tlearn: 0.4462091\ttotal: 168ms\tremaining: 588ms\n",
            "222:\tlearn: 0.4455601\ttotal: 168ms\tremaining: 586ms\n",
            "223:\tlearn: 0.4447689\ttotal: 169ms\tremaining: 586ms\n",
            "224:\tlearn: 0.4441640\ttotal: 170ms\tremaining: 585ms\n",
            "225:\tlearn: 0.4432242\ttotal: 171ms\tremaining: 584ms\n",
            "226:\tlearn: 0.4430082\ttotal: 171ms\tremaining: 584ms\n",
            "227:\tlearn: 0.4425435\ttotal: 172ms\tremaining: 583ms\n",
            "228:\tlearn: 0.4422357\ttotal: 173ms\tremaining: 582ms\n",
            "229:\tlearn: 0.4415085\ttotal: 174ms\tremaining: 581ms\n",
            "230:\tlearn: 0.4408942\ttotal: 181ms\tremaining: 604ms\n",
            "231:\tlearn: 0.4402202\ttotal: 182ms\tremaining: 604ms\n",
            "232:\tlearn: 0.4394644\ttotal: 183ms\tremaining: 603ms\n",
            "233:\tlearn: 0.4391554\ttotal: 184ms\tremaining: 602ms\n",
            "234:\tlearn: 0.4387569\ttotal: 185ms\tremaining: 601ms\n",
            "235:\tlearn: 0.4382760\ttotal: 185ms\tremaining: 600ms\n",
            "236:\tlearn: 0.4376240\ttotal: 186ms\tremaining: 599ms\n",
            "237:\tlearn: 0.4369970\ttotal: 187ms\tremaining: 599ms\n",
            "238:\tlearn: 0.4362964\ttotal: 188ms\tremaining: 598ms\n",
            "239:\tlearn: 0.4358023\ttotal: 189ms\tremaining: 597ms\n",
            "240:\tlearn: 0.4352078\ttotal: 189ms\tremaining: 597ms\n",
            "241:\tlearn: 0.4342960\ttotal: 190ms\tremaining: 596ms\n",
            "242:\tlearn: 0.4335419\ttotal: 191ms\tremaining: 595ms\n",
            "243:\tlearn: 0.4329776\ttotal: 192ms\tremaining: 594ms\n",
            "244:\tlearn: 0.4328538\ttotal: 192ms\tremaining: 592ms\n",
            "245:\tlearn: 0.4324373\ttotal: 193ms\tremaining: 591ms\n",
            "246:\tlearn: 0.4317970\ttotal: 194ms\tremaining: 590ms\n",
            "247:\tlearn: 0.4313054\ttotal: 194ms\tremaining: 590ms\n",
            "248:\tlearn: 0.4311820\ttotal: 195ms\tremaining: 588ms\n",
            "249:\tlearn: 0.4306850\ttotal: 196ms\tremaining: 587ms\n",
            "250:\tlearn: 0.4305876\ttotal: 196ms\tremaining: 585ms\n",
            "251:\tlearn: 0.4296525\ttotal: 197ms\tremaining: 584ms\n",
            "252:\tlearn: 0.4290521\ttotal: 198ms\tremaining: 583ms\n",
            "253:\tlearn: 0.4287567\ttotal: 198ms\tremaining: 583ms\n",
            "254:\tlearn: 0.4281365\ttotal: 199ms\tremaining: 582ms\n",
            "255:\tlearn: 0.4275386\ttotal: 200ms\tremaining: 581ms\n",
            "256:\tlearn: 0.4273094\ttotal: 201ms\tremaining: 582ms\n",
            "257:\tlearn: 0.4266676\ttotal: 202ms\tremaining: 582ms\n",
            "258:\tlearn: 0.4260985\ttotal: 204ms\tremaining: 583ms\n",
            "259:\tlearn: 0.4255627\ttotal: 205ms\tremaining: 583ms\n",
            "260:\tlearn: 0.4251199\ttotal: 206ms\tremaining: 582ms\n",
            "261:\tlearn: 0.4244092\ttotal: 206ms\tremaining: 581ms\n",
            "262:\tlearn: 0.4239411\ttotal: 207ms\tremaining: 580ms\n",
            "263:\tlearn: 0.4234454\ttotal: 208ms\tremaining: 579ms\n",
            "264:\tlearn: 0.4230645\ttotal: 209ms\tremaining: 579ms\n",
            "265:\tlearn: 0.4224173\ttotal: 209ms\tremaining: 578ms\n",
            "266:\tlearn: 0.4218699\ttotal: 210ms\tremaining: 577ms\n",
            "267:\tlearn: 0.4211332\ttotal: 211ms\tremaining: 576ms\n",
            "268:\tlearn: 0.4208189\ttotal: 212ms\tremaining: 575ms\n",
            "269:\tlearn: 0.4205736\ttotal: 212ms\tremaining: 574ms\n",
            "270:\tlearn: 0.4201035\ttotal: 213ms\tremaining: 573ms\n",
            "271:\tlearn: 0.4194612\ttotal: 214ms\tremaining: 572ms\n",
            "272:\tlearn: 0.4187892\ttotal: 214ms\tremaining: 571ms\n",
            "273:\tlearn: 0.4183701\ttotal: 215ms\tremaining: 570ms\n",
            "274:\tlearn: 0.4178957\ttotal: 216ms\tremaining: 569ms\n",
            "275:\tlearn: 0.4174081\ttotal: 217ms\tremaining: 568ms\n",
            "276:\tlearn: 0.4171202\ttotal: 217ms\tremaining: 567ms\n",
            "277:\tlearn: 0.4165272\ttotal: 218ms\tremaining: 566ms\n",
            "278:\tlearn: 0.4161527\ttotal: 219ms\tremaining: 565ms\n",
            "279:\tlearn: 0.4157874\ttotal: 220ms\tremaining: 565ms\n",
            "280:\tlearn: 0.4151968\ttotal: 220ms\tremaining: 564ms\n",
            "281:\tlearn: 0.4149140\ttotal: 221ms\tremaining: 563ms\n",
            "282:\tlearn: 0.4141547\ttotal: 222ms\tremaining: 562ms\n",
            "283:\tlearn: 0.4137057\ttotal: 223ms\tremaining: 562ms\n",
            "284:\tlearn: 0.4132387\ttotal: 224ms\tremaining: 561ms\n",
            "285:\tlearn: 0.4128031\ttotal: 224ms\tremaining: 560ms\n",
            "286:\tlearn: 0.4123224\ttotal: 225ms\tremaining: 559ms\n",
            "287:\tlearn: 0.4116636\ttotal: 226ms\tremaining: 558ms\n",
            "288:\tlearn: 0.4114922\ttotal: 226ms\tremaining: 557ms\n",
            "289:\tlearn: 0.4108911\ttotal: 227ms\tremaining: 556ms\n",
            "290:\tlearn: 0.4100748\ttotal: 228ms\tremaining: 555ms\n",
            "291:\tlearn: 0.4096056\ttotal: 229ms\tremaining: 554ms\n",
            "292:\tlearn: 0.4092088\ttotal: 229ms\tremaining: 554ms\n",
            "293:\tlearn: 0.4088056\ttotal: 230ms\tremaining: 553ms\n",
            "294:\tlearn: 0.4083470\ttotal: 231ms\tremaining: 552ms\n",
            "295:\tlearn: 0.4077220\ttotal: 232ms\tremaining: 551ms\n",
            "296:\tlearn: 0.4071259\ttotal: 233ms\tremaining: 551ms\n",
            "297:\tlearn: 0.4066973\ttotal: 233ms\tremaining: 550ms\n",
            "298:\tlearn: 0.4061685\ttotal: 234ms\tremaining: 549ms\n",
            "299:\tlearn: 0.4057012\ttotal: 235ms\tremaining: 548ms\n",
            "300:\tlearn: 0.4050072\ttotal: 236ms\tremaining: 547ms\n",
            "301:\tlearn: 0.4048155\ttotal: 236ms\tremaining: 546ms\n",
            "302:\tlearn: 0.4044006\ttotal: 237ms\tremaining: 545ms\n",
            "303:\tlearn: 0.4040709\ttotal: 238ms\tremaining: 544ms\n",
            "304:\tlearn: 0.4034868\ttotal: 239ms\tremaining: 544ms\n",
            "305:\tlearn: 0.4027688\ttotal: 239ms\tremaining: 543ms\n",
            "306:\tlearn: 0.4021811\ttotal: 240ms\tremaining: 542ms\n",
            "307:\tlearn: 0.4014569\ttotal: 241ms\tremaining: 541ms\n",
            "308:\tlearn: 0.4010052\ttotal: 242ms\tremaining: 540ms\n",
            "309:\tlearn: 0.4005226\ttotal: 242ms\tremaining: 539ms\n",
            "310:\tlearn: 0.4000806\ttotal: 243ms\tremaining: 539ms\n",
            "311:\tlearn: 0.3995568\ttotal: 244ms\tremaining: 538ms\n",
            "312:\tlearn: 0.3992304\ttotal: 245ms\tremaining: 537ms\n",
            "313:\tlearn: 0.3986689\ttotal: 245ms\tremaining: 536ms\n",
            "314:\tlearn: 0.3981501\ttotal: 246ms\tremaining: 535ms\n",
            "315:\tlearn: 0.3977357\ttotal: 247ms\tremaining: 534ms\n",
            "316:\tlearn: 0.3972961\ttotal: 248ms\tremaining: 534ms\n",
            "317:\tlearn: 0.3968575\ttotal: 248ms\tremaining: 532ms\n",
            "318:\tlearn: 0.3966011\ttotal: 249ms\tremaining: 531ms\n",
            "319:\tlearn: 0.3962340\ttotal: 250ms\tremaining: 530ms\n",
            "320:\tlearn: 0.3958912\ttotal: 250ms\tremaining: 530ms\n",
            "321:\tlearn: 0.3955433\ttotal: 251ms\tremaining: 529ms\n",
            "322:\tlearn: 0.3949312\ttotal: 252ms\tremaining: 528ms\n",
            "323:\tlearn: 0.3946104\ttotal: 253ms\tremaining: 527ms\n",
            "324:\tlearn: 0.3941005\ttotal: 254ms\tremaining: 527ms\n",
            "325:\tlearn: 0.3936014\ttotal: 254ms\tremaining: 526ms\n",
            "326:\tlearn: 0.3930845\ttotal: 255ms\tremaining: 525ms\n",
            "327:\tlearn: 0.3925958\ttotal: 256ms\tremaining: 524ms\n",
            "328:\tlearn: 0.3921990\ttotal: 257ms\tremaining: 524ms\n",
            "329:\tlearn: 0.3914188\ttotal: 258ms\tremaining: 523ms\n",
            "330:\tlearn: 0.3909978\ttotal: 258ms\tremaining: 522ms\n",
            "331:\tlearn: 0.3905779\ttotal: 259ms\tremaining: 521ms\n",
            "332:\tlearn: 0.3902206\ttotal: 260ms\tremaining: 520ms\n",
            "333:\tlearn: 0.3895661\ttotal: 261ms\tremaining: 520ms\n",
            "334:\tlearn: 0.3890774\ttotal: 261ms\tremaining: 519ms\n",
            "335:\tlearn: 0.3886979\ttotal: 262ms\tremaining: 518ms\n",
            "336:\tlearn: 0.3882839\ttotal: 263ms\tremaining: 517ms\n",
            "337:\tlearn: 0.3879057\ttotal: 264ms\tremaining: 516ms\n",
            "338:\tlearn: 0.3871956\ttotal: 265ms\tremaining: 516ms\n",
            "339:\tlearn: 0.3865789\ttotal: 265ms\tremaining: 515ms\n",
            "340:\tlearn: 0.3861452\ttotal: 266ms\tremaining: 514ms\n",
            "341:\tlearn: 0.3854593\ttotal: 267ms\tremaining: 514ms\n",
            "342:\tlearn: 0.3847350\ttotal: 268ms\tremaining: 513ms\n",
            "343:\tlearn: 0.3843229\ttotal: 269ms\tremaining: 512ms\n",
            "344:\tlearn: 0.3838856\ttotal: 269ms\tremaining: 511ms\n",
            "345:\tlearn: 0.3835480\ttotal: 270ms\tremaining: 510ms\n",
            "346:\tlearn: 0.3831509\ttotal: 271ms\tremaining: 510ms\n",
            "347:\tlearn: 0.3826993\ttotal: 272ms\tremaining: 509ms\n",
            "348:\tlearn: 0.3818748\ttotal: 272ms\tremaining: 508ms\n",
            "349:\tlearn: 0.3815643\ttotal: 273ms\tremaining: 507ms\n",
            "350:\tlearn: 0.3811019\ttotal: 274ms\tremaining: 506ms\n",
            "351:\tlearn: 0.3808972\ttotal: 275ms\tremaining: 505ms\n",
            "352:\tlearn: 0.3805761\ttotal: 275ms\tremaining: 505ms\n",
            "353:\tlearn: 0.3798387\ttotal: 276ms\tremaining: 504ms\n",
            "354:\tlearn: 0.3794515\ttotal: 277ms\tremaining: 503ms\n",
            "355:\tlearn: 0.3788775\ttotal: 277ms\tremaining: 502ms\n",
            "356:\tlearn: 0.3785900\ttotal: 278ms\tremaining: 501ms\n",
            "357:\tlearn: 0.3781368\ttotal: 279ms\tremaining: 500ms\n",
            "358:\tlearn: 0.3780559\ttotal: 279ms\tremaining: 499ms\n",
            "359:\tlearn: 0.3774067\ttotal: 280ms\tremaining: 498ms\n",
            "360:\tlearn: 0.3767764\ttotal: 281ms\tremaining: 497ms\n",
            "361:\tlearn: 0.3762783\ttotal: 282ms\tremaining: 496ms\n",
            "362:\tlearn: 0.3757419\ttotal: 283ms\tremaining: 496ms\n",
            "363:\tlearn: 0.3752775\ttotal: 283ms\tremaining: 495ms\n",
            "364:\tlearn: 0.3746630\ttotal: 284ms\tremaining: 494ms\n",
            "365:\tlearn: 0.3741464\ttotal: 285ms\tremaining: 494ms\n",
            "366:\tlearn: 0.3739386\ttotal: 286ms\tremaining: 493ms\n",
            "367:\tlearn: 0.3735021\ttotal: 286ms\tremaining: 492ms\n",
            "368:\tlearn: 0.3729159\ttotal: 287ms\tremaining: 491ms\n",
            "369:\tlearn: 0.3722179\ttotal: 288ms\tremaining: 490ms\n",
            "370:\tlearn: 0.3719812\ttotal: 289ms\tremaining: 490ms\n",
            "371:\tlearn: 0.3716835\ttotal: 290ms\tremaining: 489ms\n",
            "372:\tlearn: 0.3714212\ttotal: 290ms\tremaining: 488ms\n",
            "373:\tlearn: 0.3708830\ttotal: 291ms\tremaining: 487ms\n",
            "374:\tlearn: 0.3705529\ttotal: 292ms\tremaining: 487ms\n",
            "375:\tlearn: 0.3702609\ttotal: 293ms\tremaining: 486ms\n",
            "376:\tlearn: 0.3700372\ttotal: 294ms\tremaining: 485ms\n",
            "377:\tlearn: 0.3696035\ttotal: 294ms\tremaining: 485ms\n",
            "378:\tlearn: 0.3692445\ttotal: 295ms\tremaining: 484ms\n",
            "379:\tlearn: 0.3690347\ttotal: 296ms\tremaining: 483ms\n",
            "380:\tlearn: 0.3687800\ttotal: 297ms\tremaining: 482ms\n",
            "381:\tlearn: 0.3681005\ttotal: 298ms\tremaining: 481ms\n",
            "382:\tlearn: 0.3676323\ttotal: 298ms\tremaining: 481ms\n",
            "383:\tlearn: 0.3671476\ttotal: 299ms\tremaining: 480ms\n",
            "384:\tlearn: 0.3668762\ttotal: 300ms\tremaining: 479ms\n",
            "385:\tlearn: 0.3663524\ttotal: 301ms\tremaining: 478ms\n",
            "386:\tlearn: 0.3656620\ttotal: 301ms\tremaining: 478ms\n",
            "387:\tlearn: 0.3653833\ttotal: 302ms\tremaining: 476ms\n",
            "388:\tlearn: 0.3649460\ttotal: 303ms\tremaining: 476ms\n",
            "389:\tlearn: 0.3644096\ttotal: 303ms\tremaining: 475ms\n",
            "390:\tlearn: 0.3641629\ttotal: 304ms\tremaining: 474ms\n",
            "391:\tlearn: 0.3638881\ttotal: 305ms\tremaining: 473ms\n",
            "392:\tlearn: 0.3634721\ttotal: 306ms\tremaining: 472ms\n",
            "393:\tlearn: 0.3631725\ttotal: 306ms\tremaining: 471ms\n",
            "394:\tlearn: 0.3626432\ttotal: 307ms\tremaining: 471ms\n",
            "395:\tlearn: 0.3619181\ttotal: 308ms\tremaining: 470ms\n",
            "396:\tlearn: 0.3614539\ttotal: 309ms\tremaining: 469ms\n",
            "397:\tlearn: 0.3611172\ttotal: 310ms\tremaining: 468ms\n",
            "398:\tlearn: 0.3607319\ttotal: 310ms\tremaining: 468ms\n",
            "399:\tlearn: 0.3606044\ttotal: 311ms\tremaining: 466ms\n",
            "400:\tlearn: 0.3600970\ttotal: 312ms\tremaining: 466ms\n",
            "401:\tlearn: 0.3598131\ttotal: 312ms\tremaining: 465ms\n",
            "402:\tlearn: 0.3594763\ttotal: 313ms\tremaining: 464ms\n",
            "403:\tlearn: 0.3592964\ttotal: 314ms\tremaining: 463ms\n",
            "404:\tlearn: 0.3588437\ttotal: 315ms\tremaining: 463ms\n",
            "405:\tlearn: 0.3586169\ttotal: 316ms\tremaining: 462ms\n",
            "406:\tlearn: 0.3580535\ttotal: 316ms\tremaining: 461ms\n",
            "407:\tlearn: 0.3575510\ttotal: 317ms\tremaining: 460ms\n",
            "408:\tlearn: 0.3571107\ttotal: 318ms\tremaining: 460ms\n",
            "409:\tlearn: 0.3569036\ttotal: 319ms\tremaining: 459ms\n",
            "410:\tlearn: 0.3565486\ttotal: 320ms\tremaining: 458ms\n",
            "411:\tlearn: 0.3561763\ttotal: 320ms\tremaining: 457ms\n",
            "412:\tlearn: 0.3558850\ttotal: 321ms\tremaining: 456ms\n",
            "413:\tlearn: 0.3556807\ttotal: 322ms\tremaining: 456ms\n",
            "414:\tlearn: 0.3554200\ttotal: 323ms\tremaining: 455ms\n",
            "415:\tlearn: 0.3549151\ttotal: 323ms\tremaining: 454ms\n",
            "416:\tlearn: 0.3548242\ttotal: 324ms\tremaining: 453ms\n",
            "417:\tlearn: 0.3544160\ttotal: 325ms\tremaining: 452ms\n",
            "418:\tlearn: 0.3539861\ttotal: 326ms\tremaining: 451ms\n",
            "419:\tlearn: 0.3537470\ttotal: 326ms\tremaining: 451ms\n",
            "420:\tlearn: 0.3529603\ttotal: 327ms\tremaining: 450ms\n",
            "421:\tlearn: 0.3523533\ttotal: 328ms\tremaining: 449ms\n",
            "422:\tlearn: 0.3520453\ttotal: 329ms\tremaining: 448ms\n",
            "423:\tlearn: 0.3515269\ttotal: 329ms\tremaining: 448ms\n",
            "424:\tlearn: 0.3511700\ttotal: 330ms\tremaining: 447ms\n",
            "425:\tlearn: 0.3509284\ttotal: 331ms\tremaining: 446ms\n",
            "426:\tlearn: 0.3505882\ttotal: 332ms\tremaining: 445ms\n",
            "427:\tlearn: 0.3502208\ttotal: 333ms\tremaining: 444ms\n",
            "428:\tlearn: 0.3496602\ttotal: 333ms\tremaining: 444ms\n",
            "429:\tlearn: 0.3491268\ttotal: 334ms\tremaining: 443ms\n",
            "430:\tlearn: 0.3487718\ttotal: 336ms\tremaining: 443ms\n",
            "431:\tlearn: 0.3482089\ttotal: 336ms\tremaining: 442ms\n",
            "432:\tlearn: 0.3479036\ttotal: 337ms\tremaining: 442ms\n",
            "433:\tlearn: 0.3475789\ttotal: 338ms\tremaining: 441ms\n",
            "434:\tlearn: 0.3472268\ttotal: 339ms\tremaining: 440ms\n",
            "435:\tlearn: 0.3468019\ttotal: 339ms\tremaining: 439ms\n",
            "436:\tlearn: 0.3463590\ttotal: 341ms\tremaining: 439ms\n",
            "437:\tlearn: 0.3460489\ttotal: 342ms\tremaining: 439ms\n",
            "438:\tlearn: 0.3456879\ttotal: 343ms\tremaining: 438ms\n",
            "439:\tlearn: 0.3452850\ttotal: 344ms\tremaining: 437ms\n",
            "440:\tlearn: 0.3449272\ttotal: 344ms\tremaining: 437ms\n",
            "441:\tlearn: 0.3442553\ttotal: 345ms\tremaining: 436ms\n",
            "442:\tlearn: 0.3438320\ttotal: 346ms\tremaining: 435ms\n",
            "443:\tlearn: 0.3436970\ttotal: 347ms\tremaining: 434ms\n",
            "444:\tlearn: 0.3434151\ttotal: 347ms\tremaining: 433ms\n",
            "445:\tlearn: 0.3431960\ttotal: 348ms\tremaining: 432ms\n",
            "446:\tlearn: 0.3428371\ttotal: 349ms\tremaining: 432ms\n",
            "447:\tlearn: 0.3426831\ttotal: 350ms\tremaining: 432ms\n",
            "448:\tlearn: 0.3421473\ttotal: 352ms\tremaining: 432ms\n",
            "449:\tlearn: 0.3419307\ttotal: 353ms\tremaining: 431ms\n",
            "450:\tlearn: 0.3415663\ttotal: 354ms\tremaining: 431ms\n",
            "451:\tlearn: 0.3412751\ttotal: 356ms\tremaining: 431ms\n",
            "452:\tlearn: 0.3409589\ttotal: 358ms\tremaining: 433ms\n",
            "453:\tlearn: 0.3406653\ttotal: 360ms\tremaining: 433ms\n",
            "454:\tlearn: 0.3402819\ttotal: 361ms\tremaining: 432ms\n",
            "455:\tlearn: 0.3399179\ttotal: 363ms\tremaining: 433ms\n",
            "456:\tlearn: 0.3397281\ttotal: 363ms\tremaining: 432ms\n",
            "457:\tlearn: 0.3394036\ttotal: 365ms\tremaining: 432ms\n",
            "458:\tlearn: 0.3392475\ttotal: 366ms\tremaining: 431ms\n",
            "459:\tlearn: 0.3390873\ttotal: 367ms\tremaining: 431ms\n",
            "460:\tlearn: 0.3387414\ttotal: 368ms\tremaining: 431ms\n",
            "461:\tlearn: 0.3382820\ttotal: 370ms\tremaining: 430ms\n",
            "462:\tlearn: 0.3381813\ttotal: 371ms\tremaining: 430ms\n",
            "463:\tlearn: 0.3378118\ttotal: 373ms\tremaining: 430ms\n",
            "464:\tlearn: 0.3376131\ttotal: 374ms\tremaining: 430ms\n",
            "465:\tlearn: 0.3373316\ttotal: 375ms\tremaining: 430ms\n",
            "466:\tlearn: 0.3369670\ttotal: 378ms\tremaining: 431ms\n",
            "467:\tlearn: 0.3365673\ttotal: 383ms\tremaining: 435ms\n",
            "468:\tlearn: 0.3363282\ttotal: 385ms\tremaining: 435ms\n",
            "469:\tlearn: 0.3359696\ttotal: 386ms\tremaining: 435ms\n",
            "470:\tlearn: 0.3357577\ttotal: 388ms\tremaining: 436ms\n",
            "471:\tlearn: 0.3354391\ttotal: 389ms\tremaining: 436ms\n",
            "472:\tlearn: 0.3349874\ttotal: 391ms\tremaining: 435ms\n",
            "473:\tlearn: 0.3346857\ttotal: 392ms\tremaining: 435ms\n",
            "474:\tlearn: 0.3343440\ttotal: 395ms\tremaining: 437ms\n",
            "475:\tlearn: 0.3340261\ttotal: 397ms\tremaining: 437ms\n",
            "476:\tlearn: 0.3335231\ttotal: 399ms\tremaining: 438ms\n",
            "477:\tlearn: 0.3331341\ttotal: 401ms\tremaining: 438ms\n",
            "478:\tlearn: 0.3329373\ttotal: 403ms\tremaining: 438ms\n",
            "479:\tlearn: 0.3325870\ttotal: 405ms\tremaining: 439ms\n",
            "480:\tlearn: 0.3322250\ttotal: 407ms\tremaining: 439ms\n",
            "481:\tlearn: 0.3318166\ttotal: 409ms\tremaining: 439ms\n",
            "482:\tlearn: 0.3312832\ttotal: 411ms\tremaining: 440ms\n",
            "483:\tlearn: 0.3311150\ttotal: 412ms\tremaining: 440ms\n",
            "484:\tlearn: 0.3307673\ttotal: 414ms\tremaining: 439ms\n",
            "485:\tlearn: 0.3304412\ttotal: 415ms\tremaining: 439ms\n",
            "486:\tlearn: 0.3302022\ttotal: 417ms\tremaining: 439ms\n",
            "487:\tlearn: 0.3301201\ttotal: 418ms\tremaining: 439ms\n",
            "488:\tlearn: 0.3297850\ttotal: 420ms\tremaining: 439ms\n",
            "489:\tlearn: 0.3295647\ttotal: 422ms\tremaining: 439ms\n",
            "490:\tlearn: 0.3292787\ttotal: 425ms\tremaining: 441ms\n",
            "491:\tlearn: 0.3290548\ttotal: 427ms\tremaining: 441ms\n",
            "492:\tlearn: 0.3287603\ttotal: 428ms\tremaining: 440ms\n",
            "493:\tlearn: 0.3287357\ttotal: 429ms\tremaining: 439ms\n",
            "494:\tlearn: 0.3284884\ttotal: 430ms\tremaining: 438ms\n",
            "495:\tlearn: 0.3282530\ttotal: 430ms\tremaining: 437ms\n",
            "496:\tlearn: 0.3277764\ttotal: 431ms\tremaining: 436ms\n",
            "497:\tlearn: 0.3275944\ttotal: 432ms\tremaining: 435ms\n",
            "498:\tlearn: 0.3272614\ttotal: 433ms\tremaining: 435ms\n",
            "499:\tlearn: 0.3270183\ttotal: 434ms\tremaining: 434ms\n",
            "500:\tlearn: 0.3265825\ttotal: 434ms\tremaining: 433ms\n",
            "501:\tlearn: 0.3264234\ttotal: 435ms\tremaining: 432ms\n",
            "502:\tlearn: 0.3260221\ttotal: 436ms\tremaining: 431ms\n",
            "503:\tlearn: 0.3256068\ttotal: 437ms\tremaining: 430ms\n",
            "504:\tlearn: 0.3255442\ttotal: 437ms\tremaining: 428ms\n",
            "505:\tlearn: 0.3252102\ttotal: 438ms\tremaining: 427ms\n",
            "506:\tlearn: 0.3250436\ttotal: 439ms\tremaining: 427ms\n",
            "507:\tlearn: 0.3246325\ttotal: 439ms\tremaining: 426ms\n",
            "508:\tlearn: 0.3243863\ttotal: 440ms\tremaining: 425ms\n",
            "509:\tlearn: 0.3239538\ttotal: 441ms\tremaining: 424ms\n",
            "510:\tlearn: 0.3236442\ttotal: 442ms\tremaining: 423ms\n",
            "511:\tlearn: 0.3231489\ttotal: 443ms\tremaining: 422ms\n",
            "512:\tlearn: 0.3229303\ttotal: 443ms\tremaining: 421ms\n",
            "513:\tlearn: 0.3225284\ttotal: 444ms\tremaining: 420ms\n",
            "514:\tlearn: 0.3224032\ttotal: 445ms\tremaining: 419ms\n",
            "515:\tlearn: 0.3219107\ttotal: 446ms\tremaining: 418ms\n",
            "516:\tlearn: 0.3215360\ttotal: 446ms\tremaining: 417ms\n",
            "517:\tlearn: 0.3212440\ttotal: 447ms\tremaining: 416ms\n",
            "518:\tlearn: 0.3209004\ttotal: 448ms\tremaining: 415ms\n",
            "519:\tlearn: 0.3205929\ttotal: 449ms\tremaining: 414ms\n",
            "520:\tlearn: 0.3203902\ttotal: 449ms\tremaining: 413ms\n",
            "521:\tlearn: 0.3200247\ttotal: 450ms\tremaining: 412ms\n",
            "522:\tlearn: 0.3198157\ttotal: 451ms\tremaining: 411ms\n",
            "523:\tlearn: 0.3194807\ttotal: 452ms\tremaining: 410ms\n",
            "524:\tlearn: 0.3191599\ttotal: 453ms\tremaining: 410ms\n",
            "525:\tlearn: 0.3190105\ttotal: 453ms\tremaining: 409ms\n",
            "526:\tlearn: 0.3184992\ttotal: 454ms\tremaining: 408ms\n",
            "527:\tlearn: 0.3182245\ttotal: 455ms\tremaining: 407ms\n",
            "528:\tlearn: 0.3180101\ttotal: 456ms\tremaining: 406ms\n",
            "529:\tlearn: 0.3177352\ttotal: 456ms\tremaining: 405ms\n",
            "530:\tlearn: 0.3174031\ttotal: 457ms\tremaining: 404ms\n",
            "531:\tlearn: 0.3169237\ttotal: 458ms\tremaining: 403ms\n",
            "532:\tlearn: 0.3163588\ttotal: 459ms\tremaining: 402ms\n",
            "533:\tlearn: 0.3159031\ttotal: 460ms\tremaining: 401ms\n",
            "534:\tlearn: 0.3156032\ttotal: 460ms\tremaining: 400ms\n",
            "535:\tlearn: 0.3151981\ttotal: 461ms\tremaining: 399ms\n",
            "536:\tlearn: 0.3149114\ttotal: 462ms\tremaining: 398ms\n",
            "537:\tlearn: 0.3148108\ttotal: 462ms\tremaining: 397ms\n",
            "538:\tlearn: 0.3146551\ttotal: 463ms\tremaining: 396ms\n",
            "539:\tlearn: 0.3144088\ttotal: 464ms\tremaining: 395ms\n",
            "540:\tlearn: 0.3140370\ttotal: 464ms\tremaining: 394ms\n",
            "541:\tlearn: 0.3138657\ttotal: 465ms\tremaining: 393ms\n",
            "542:\tlearn: 0.3135019\ttotal: 466ms\tremaining: 393ms\n",
            "543:\tlearn: 0.3132903\ttotal: 467ms\tremaining: 392ms\n",
            "544:\tlearn: 0.3131172\ttotal: 468ms\tremaining: 391ms\n",
            "545:\tlearn: 0.3129669\ttotal: 469ms\tremaining: 390ms\n",
            "546:\tlearn: 0.3126531\ttotal: 470ms\tremaining: 389ms\n",
            "547:\tlearn: 0.3122560\ttotal: 470ms\tremaining: 388ms\n",
            "548:\tlearn: 0.3119326\ttotal: 471ms\tremaining: 387ms\n",
            "549:\tlearn: 0.3117166\ttotal: 472ms\tremaining: 386ms\n",
            "550:\tlearn: 0.3113414\ttotal: 473ms\tremaining: 385ms\n",
            "551:\tlearn: 0.3109561\ttotal: 473ms\tremaining: 384ms\n",
            "552:\tlearn: 0.3106639\ttotal: 474ms\tremaining: 383ms\n",
            "553:\tlearn: 0.3104408\ttotal: 479ms\tremaining: 385ms\n",
            "554:\tlearn: 0.3099109\ttotal: 480ms\tremaining: 385ms\n",
            "555:\tlearn: 0.3095515\ttotal: 481ms\tremaining: 384ms\n",
            "556:\tlearn: 0.3091553\ttotal: 482ms\tremaining: 383ms\n",
            "557:\tlearn: 0.3089423\ttotal: 483ms\tremaining: 382ms\n",
            "558:\tlearn: 0.3085328\ttotal: 484ms\tremaining: 382ms\n",
            "559:\tlearn: 0.3083508\ttotal: 485ms\tremaining: 381ms\n",
            "560:\tlearn: 0.3083152\ttotal: 486ms\tremaining: 380ms\n",
            "561:\tlearn: 0.3080749\ttotal: 487ms\tremaining: 379ms\n",
            "562:\tlearn: 0.3078814\ttotal: 488ms\tremaining: 378ms\n",
            "563:\tlearn: 0.3073707\ttotal: 488ms\tremaining: 377ms\n",
            "564:\tlearn: 0.3070574\ttotal: 489ms\tremaining: 377ms\n",
            "565:\tlearn: 0.3066527\ttotal: 490ms\tremaining: 376ms\n",
            "566:\tlearn: 0.3062759\ttotal: 491ms\tremaining: 375ms\n",
            "567:\tlearn: 0.3058373\ttotal: 491ms\tremaining: 374ms\n",
            "568:\tlearn: 0.3052766\ttotal: 492ms\tremaining: 373ms\n",
            "569:\tlearn: 0.3051674\ttotal: 493ms\tremaining: 372ms\n",
            "570:\tlearn: 0.3050018\ttotal: 494ms\tremaining: 371ms\n",
            "571:\tlearn: 0.3046139\ttotal: 494ms\tremaining: 370ms\n",
            "572:\tlearn: 0.3043182\ttotal: 495ms\tremaining: 369ms\n",
            "573:\tlearn: 0.3039052\ttotal: 496ms\tremaining: 368ms\n",
            "574:\tlearn: 0.3036068\ttotal: 497ms\tremaining: 367ms\n",
            "575:\tlearn: 0.3035132\ttotal: 497ms\tremaining: 366ms\n",
            "576:\tlearn: 0.3031011\ttotal: 498ms\tremaining: 365ms\n",
            "577:\tlearn: 0.3027216\ttotal: 499ms\tremaining: 364ms\n",
            "578:\tlearn: 0.3024150\ttotal: 499ms\tremaining: 363ms\n",
            "579:\tlearn: 0.3021763\ttotal: 500ms\tremaining: 362ms\n",
            "580:\tlearn: 0.3018775\ttotal: 501ms\tremaining: 361ms\n",
            "581:\tlearn: 0.3015533\ttotal: 502ms\tremaining: 360ms\n",
            "582:\tlearn: 0.3012674\ttotal: 502ms\tremaining: 359ms\n",
            "583:\tlearn: 0.3008381\ttotal: 503ms\tremaining: 359ms\n",
            "584:\tlearn: 0.3004697\ttotal: 504ms\tremaining: 358ms\n",
            "585:\tlearn: 0.3001811\ttotal: 505ms\tremaining: 357ms\n",
            "586:\tlearn: 0.2999646\ttotal: 506ms\tremaining: 356ms\n",
            "587:\tlearn: 0.2995205\ttotal: 506ms\tremaining: 355ms\n",
            "588:\tlearn: 0.2993800\ttotal: 507ms\tremaining: 354ms\n",
            "589:\tlearn: 0.2990136\ttotal: 508ms\tremaining: 353ms\n",
            "590:\tlearn: 0.2986465\ttotal: 508ms\tremaining: 352ms\n",
            "591:\tlearn: 0.2981761\ttotal: 509ms\tremaining: 351ms\n",
            "592:\tlearn: 0.2980146\ttotal: 510ms\tremaining: 350ms\n",
            "593:\tlearn: 0.2978035\ttotal: 511ms\tremaining: 349ms\n",
            "594:\tlearn: 0.2974719\ttotal: 511ms\tremaining: 348ms\n",
            "595:\tlearn: 0.2972173\ttotal: 512ms\tremaining: 347ms\n",
            "596:\tlearn: 0.2971259\ttotal: 513ms\tremaining: 346ms\n",
            "597:\tlearn: 0.2967672\ttotal: 514ms\tremaining: 345ms\n",
            "598:\tlearn: 0.2965825\ttotal: 514ms\tremaining: 344ms\n",
            "599:\tlearn: 0.2963296\ttotal: 515ms\tremaining: 343ms\n",
            "600:\tlearn: 0.2962183\ttotal: 516ms\tremaining: 343ms\n",
            "601:\tlearn: 0.2959594\ttotal: 517ms\tremaining: 342ms\n",
            "602:\tlearn: 0.2953653\ttotal: 517ms\tremaining: 341ms\n",
            "603:\tlearn: 0.2950640\ttotal: 518ms\tremaining: 340ms\n",
            "604:\tlearn: 0.2946225\ttotal: 519ms\tremaining: 339ms\n",
            "605:\tlearn: 0.2942627\ttotal: 520ms\tremaining: 338ms\n",
            "606:\tlearn: 0.2941821\ttotal: 520ms\tremaining: 337ms\n",
            "607:\tlearn: 0.2938669\ttotal: 521ms\tremaining: 336ms\n",
            "608:\tlearn: 0.2936154\ttotal: 522ms\tremaining: 335ms\n",
            "609:\tlearn: 0.2932192\ttotal: 522ms\tremaining: 334ms\n",
            "610:\tlearn: 0.2929740\ttotal: 523ms\tremaining: 333ms\n",
            "611:\tlearn: 0.2926862\ttotal: 524ms\tremaining: 332ms\n",
            "612:\tlearn: 0.2923512\ttotal: 525ms\tremaining: 331ms\n",
            "613:\tlearn: 0.2920493\ttotal: 526ms\tremaining: 330ms\n",
            "614:\tlearn: 0.2917613\ttotal: 526ms\tremaining: 329ms\n",
            "615:\tlearn: 0.2913225\ttotal: 527ms\tremaining: 329ms\n",
            "616:\tlearn: 0.2909999\ttotal: 528ms\tremaining: 328ms\n",
            "617:\tlearn: 0.2907864\ttotal: 529ms\tremaining: 327ms\n",
            "618:\tlearn: 0.2905428\ttotal: 529ms\tremaining: 326ms\n",
            "619:\tlearn: 0.2902709\ttotal: 530ms\tremaining: 325ms\n",
            "620:\tlearn: 0.2899215\ttotal: 531ms\tremaining: 324ms\n",
            "621:\tlearn: 0.2896346\ttotal: 532ms\tremaining: 323ms\n",
            "622:\tlearn: 0.2894090\ttotal: 533ms\tremaining: 322ms\n",
            "623:\tlearn: 0.2892040\ttotal: 533ms\tremaining: 321ms\n",
            "624:\tlearn: 0.2891515\ttotal: 534ms\tremaining: 320ms\n",
            "625:\tlearn: 0.2890077\ttotal: 535ms\tremaining: 319ms\n",
            "626:\tlearn: 0.2886352\ttotal: 536ms\tremaining: 319ms\n",
            "627:\tlearn: 0.2884547\ttotal: 536ms\tremaining: 318ms\n",
            "628:\tlearn: 0.2881787\ttotal: 537ms\tremaining: 317ms\n",
            "629:\tlearn: 0.2879817\ttotal: 538ms\tremaining: 316ms\n",
            "630:\tlearn: 0.2875900\ttotal: 539ms\tremaining: 315ms\n",
            "631:\tlearn: 0.2873643\ttotal: 542ms\tremaining: 316ms\n",
            "632:\tlearn: 0.2869676\ttotal: 544ms\tremaining: 316ms\n",
            "633:\tlearn: 0.2865863\ttotal: 545ms\tremaining: 315ms\n",
            "634:\tlearn: 0.2864064\ttotal: 546ms\tremaining: 314ms\n",
            "635:\tlearn: 0.2859426\ttotal: 547ms\tremaining: 313ms\n",
            "636:\tlearn: 0.2856533\ttotal: 548ms\tremaining: 312ms\n",
            "637:\tlearn: 0.2855087\ttotal: 548ms\tremaining: 311ms\n",
            "638:\tlearn: 0.2855043\ttotal: 549ms\tremaining: 310ms\n",
            "639:\tlearn: 0.2852071\ttotal: 550ms\tremaining: 309ms\n",
            "640:\tlearn: 0.2850475\ttotal: 550ms\tremaining: 308ms\n",
            "641:\tlearn: 0.2848015\ttotal: 551ms\tremaining: 307ms\n",
            "642:\tlearn: 0.2845559\ttotal: 552ms\tremaining: 307ms\n",
            "643:\tlearn: 0.2843706\ttotal: 553ms\tremaining: 306ms\n",
            "644:\tlearn: 0.2841387\ttotal: 554ms\tremaining: 305ms\n",
            "645:\tlearn: 0.2839673\ttotal: 554ms\tremaining: 304ms\n",
            "646:\tlearn: 0.2838259\ttotal: 555ms\tremaining: 303ms\n",
            "647:\tlearn: 0.2834970\ttotal: 556ms\tremaining: 302ms\n",
            "648:\tlearn: 0.2831108\ttotal: 557ms\tremaining: 301ms\n",
            "649:\tlearn: 0.2828729\ttotal: 557ms\tremaining: 300ms\n",
            "650:\tlearn: 0.2827338\ttotal: 558ms\tremaining: 299ms\n",
            "651:\tlearn: 0.2825179\ttotal: 559ms\tremaining: 298ms\n",
            "652:\tlearn: 0.2821141\ttotal: 560ms\tremaining: 297ms\n",
            "653:\tlearn: 0.2818851\ttotal: 561ms\tremaining: 297ms\n",
            "654:\tlearn: 0.2816371\ttotal: 562ms\tremaining: 296ms\n",
            "655:\tlearn: 0.2813826\ttotal: 563ms\tremaining: 295ms\n",
            "656:\tlearn: 0.2811802\ttotal: 564ms\tremaining: 294ms\n",
            "657:\tlearn: 0.2807886\ttotal: 565ms\tremaining: 293ms\n",
            "658:\tlearn: 0.2805715\ttotal: 565ms\tremaining: 293ms\n",
            "659:\tlearn: 0.2802656\ttotal: 566ms\tremaining: 292ms\n",
            "660:\tlearn: 0.2800192\ttotal: 567ms\tremaining: 291ms\n",
            "661:\tlearn: 0.2799049\ttotal: 568ms\tremaining: 290ms\n",
            "662:\tlearn: 0.2795271\ttotal: 568ms\tremaining: 289ms\n",
            "663:\tlearn: 0.2793869\ttotal: 569ms\tremaining: 288ms\n",
            "664:\tlearn: 0.2790330\ttotal: 570ms\tremaining: 287ms\n",
            "665:\tlearn: 0.2787988\ttotal: 570ms\tremaining: 286ms\n",
            "666:\tlearn: 0.2785669\ttotal: 571ms\tremaining: 285ms\n",
            "667:\tlearn: 0.2782403\ttotal: 572ms\tremaining: 284ms\n",
            "668:\tlearn: 0.2779326\ttotal: 573ms\tremaining: 283ms\n",
            "669:\tlearn: 0.2776710\ttotal: 574ms\tremaining: 283ms\n",
            "670:\tlearn: 0.2775417\ttotal: 574ms\tremaining: 282ms\n",
            "671:\tlearn: 0.2773454\ttotal: 575ms\tremaining: 281ms\n",
            "672:\tlearn: 0.2769854\ttotal: 576ms\tremaining: 280ms\n",
            "673:\tlearn: 0.2765096\ttotal: 576ms\tremaining: 279ms\n",
            "674:\tlearn: 0.2763784\ttotal: 577ms\tremaining: 278ms\n",
            "675:\tlearn: 0.2762519\ttotal: 578ms\tremaining: 277ms\n",
            "676:\tlearn: 0.2759881\ttotal: 578ms\tremaining: 276ms\n",
            "677:\tlearn: 0.2757415\ttotal: 579ms\tremaining: 275ms\n",
            "678:\tlearn: 0.2756090\ttotal: 580ms\tremaining: 274ms\n",
            "679:\tlearn: 0.2754448\ttotal: 581ms\tremaining: 273ms\n",
            "680:\tlearn: 0.2750690\ttotal: 581ms\tremaining: 272ms\n",
            "681:\tlearn: 0.2747872\ttotal: 582ms\tremaining: 271ms\n",
            "682:\tlearn: 0.2745089\ttotal: 583ms\tremaining: 271ms\n",
            "683:\tlearn: 0.2741332\ttotal: 584ms\tremaining: 270ms\n",
            "684:\tlearn: 0.2737883\ttotal: 584ms\tremaining: 269ms\n",
            "685:\tlearn: 0.2735552\ttotal: 585ms\tremaining: 268ms\n",
            "686:\tlearn: 0.2732052\ttotal: 586ms\tremaining: 267ms\n",
            "687:\tlearn: 0.2728676\ttotal: 587ms\tremaining: 266ms\n",
            "688:\tlearn: 0.2727335\ttotal: 587ms\tremaining: 265ms\n",
            "689:\tlearn: 0.2725529\ttotal: 588ms\tremaining: 264ms\n",
            "690:\tlearn: 0.2723035\ttotal: 589ms\tremaining: 263ms\n",
            "691:\tlearn: 0.2721790\ttotal: 589ms\tremaining: 262ms\n",
            "692:\tlearn: 0.2719243\ttotal: 590ms\tremaining: 261ms\n",
            "693:\tlearn: 0.2715740\ttotal: 591ms\tremaining: 261ms\n",
            "694:\tlearn: 0.2715545\ttotal: 592ms\tremaining: 260ms\n",
            "695:\tlearn: 0.2713761\ttotal: 592ms\tremaining: 259ms\n",
            "696:\tlearn: 0.2711191\ttotal: 593ms\tremaining: 258ms\n",
            "697:\tlearn: 0.2709201\ttotal: 594ms\tremaining: 257ms\n",
            "698:\tlearn: 0.2708104\ttotal: 594ms\tremaining: 256ms\n",
            "699:\tlearn: 0.2706082\ttotal: 595ms\tremaining: 255ms\n",
            "700:\tlearn: 0.2703963\ttotal: 596ms\tremaining: 254ms\n",
            "701:\tlearn: 0.2702074\ttotal: 597ms\tremaining: 253ms\n",
            "702:\tlearn: 0.2701672\ttotal: 597ms\tremaining: 252ms\n",
            "703:\tlearn: 0.2700130\ttotal: 598ms\tremaining: 251ms\n",
            "704:\tlearn: 0.2697115\ttotal: 599ms\tremaining: 251ms\n",
            "705:\tlearn: 0.2694983\ttotal: 599ms\tremaining: 250ms\n",
            "706:\tlearn: 0.2693134\ttotal: 600ms\tremaining: 249ms\n",
            "707:\tlearn: 0.2691230\ttotal: 601ms\tremaining: 248ms\n",
            "708:\tlearn: 0.2688936\ttotal: 602ms\tremaining: 247ms\n",
            "709:\tlearn: 0.2685679\ttotal: 603ms\tremaining: 246ms\n",
            "710:\tlearn: 0.2682431\ttotal: 603ms\tremaining: 245ms\n",
            "711:\tlearn: 0.2680093\ttotal: 604ms\tremaining: 244ms\n",
            "712:\tlearn: 0.2678049\ttotal: 605ms\tremaining: 244ms\n",
            "713:\tlearn: 0.2674330\ttotal: 606ms\tremaining: 243ms\n",
            "714:\tlearn: 0.2671317\ttotal: 607ms\tremaining: 242ms\n",
            "715:\tlearn: 0.2668690\ttotal: 607ms\tremaining: 241ms\n",
            "716:\tlearn: 0.2666598\ttotal: 608ms\tremaining: 240ms\n",
            "717:\tlearn: 0.2664217\ttotal: 609ms\tremaining: 239ms\n",
            "718:\tlearn: 0.2661780\ttotal: 610ms\tremaining: 238ms\n",
            "719:\tlearn: 0.2660715\ttotal: 611ms\tremaining: 237ms\n",
            "720:\tlearn: 0.2658108\ttotal: 611ms\tremaining: 237ms\n",
            "721:\tlearn: 0.2656382\ttotal: 612ms\tremaining: 236ms\n",
            "722:\tlearn: 0.2653683\ttotal: 613ms\tremaining: 235ms\n",
            "723:\tlearn: 0.2650938\ttotal: 614ms\tremaining: 234ms\n",
            "724:\tlearn: 0.2649834\ttotal: 614ms\tremaining: 233ms\n",
            "725:\tlearn: 0.2648414\ttotal: 615ms\tremaining: 232ms\n",
            "726:\tlearn: 0.2646300\ttotal: 616ms\tremaining: 231ms\n",
            "727:\tlearn: 0.2643834\ttotal: 617ms\tremaining: 230ms\n",
            "728:\tlearn: 0.2641628\ttotal: 617ms\tremaining: 229ms\n",
            "729:\tlearn: 0.2638650\ttotal: 618ms\tremaining: 229ms\n",
            "730:\tlearn: 0.2637218\ttotal: 619ms\tremaining: 228ms\n",
            "731:\tlearn: 0.2633532\ttotal: 619ms\tremaining: 227ms\n",
            "732:\tlearn: 0.2631610\ttotal: 620ms\tremaining: 226ms\n",
            "733:\tlearn: 0.2630032\ttotal: 621ms\tremaining: 225ms\n",
            "734:\tlearn: 0.2628521\ttotal: 622ms\tremaining: 224ms\n",
            "735:\tlearn: 0.2626264\ttotal: 623ms\tremaining: 223ms\n",
            "736:\tlearn: 0.2622368\ttotal: 623ms\tremaining: 222ms\n",
            "737:\tlearn: 0.2619502\ttotal: 624ms\tremaining: 222ms\n",
            "738:\tlearn: 0.2616583\ttotal: 625ms\tremaining: 221ms\n",
            "739:\tlearn: 0.2613608\ttotal: 626ms\tremaining: 220ms\n",
            "740:\tlearn: 0.2612170\ttotal: 627ms\tremaining: 219ms\n",
            "741:\tlearn: 0.2610794\ttotal: 627ms\tremaining: 218ms\n",
            "742:\tlearn: 0.2609563\ttotal: 628ms\tremaining: 217ms\n",
            "743:\tlearn: 0.2607537\ttotal: 629ms\tremaining: 216ms\n",
            "744:\tlearn: 0.2604280\ttotal: 629ms\tremaining: 215ms\n",
            "745:\tlearn: 0.2601359\ttotal: 630ms\tremaining: 215ms\n",
            "746:\tlearn: 0.2598886\ttotal: 631ms\tremaining: 214ms\n",
            "747:\tlearn: 0.2596833\ttotal: 632ms\tremaining: 213ms\n",
            "748:\tlearn: 0.2595272\ttotal: 632ms\tremaining: 212ms\n",
            "749:\tlearn: 0.2593711\ttotal: 633ms\tremaining: 211ms\n",
            "750:\tlearn: 0.2591306\ttotal: 634ms\tremaining: 210ms\n",
            "751:\tlearn: 0.2586852\ttotal: 635ms\tremaining: 209ms\n",
            "752:\tlearn: 0.2585144\ttotal: 636ms\tremaining: 209ms\n",
            "753:\tlearn: 0.2582008\ttotal: 636ms\tremaining: 208ms\n",
            "754:\tlearn: 0.2580732\ttotal: 637ms\tremaining: 207ms\n",
            "755:\tlearn: 0.2579404\ttotal: 638ms\tremaining: 206ms\n",
            "756:\tlearn: 0.2577193\ttotal: 639ms\tremaining: 205ms\n",
            "757:\tlearn: 0.2574661\ttotal: 639ms\tremaining: 204ms\n",
            "758:\tlearn: 0.2572317\ttotal: 640ms\tremaining: 203ms\n",
            "759:\tlearn: 0.2571216\ttotal: 641ms\tremaining: 202ms\n",
            "760:\tlearn: 0.2567632\ttotal: 642ms\tremaining: 202ms\n",
            "761:\tlearn: 0.2566514\ttotal: 642ms\tremaining: 201ms\n",
            "762:\tlearn: 0.2563829\ttotal: 643ms\tremaining: 200ms\n",
            "763:\tlearn: 0.2562544\ttotal: 644ms\tremaining: 199ms\n",
            "764:\tlearn: 0.2561443\ttotal: 645ms\tremaining: 198ms\n",
            "765:\tlearn: 0.2560355\ttotal: 646ms\tremaining: 197ms\n",
            "766:\tlearn: 0.2558546\ttotal: 646ms\tremaining: 196ms\n",
            "767:\tlearn: 0.2557165\ttotal: 647ms\tremaining: 195ms\n",
            "768:\tlearn: 0.2554586\ttotal: 648ms\tremaining: 195ms\n",
            "769:\tlearn: 0.2554193\ttotal: 648ms\tremaining: 194ms\n",
            "770:\tlearn: 0.2552896\ttotal: 649ms\tremaining: 193ms\n",
            "771:\tlearn: 0.2551006\ttotal: 650ms\tremaining: 192ms\n",
            "772:\tlearn: 0.2550462\ttotal: 651ms\tremaining: 191ms\n",
            "773:\tlearn: 0.2548079\ttotal: 651ms\tremaining: 190ms\n",
            "774:\tlearn: 0.2544993\ttotal: 652ms\tremaining: 189ms\n",
            "775:\tlearn: 0.2542978\ttotal: 653ms\tremaining: 188ms\n",
            "776:\tlearn: 0.2540253\ttotal: 654ms\tremaining: 188ms\n",
            "777:\tlearn: 0.2538313\ttotal: 654ms\tremaining: 187ms\n",
            "778:\tlearn: 0.2535755\ttotal: 655ms\tremaining: 186ms\n",
            "779:\tlearn: 0.2534291\ttotal: 656ms\tremaining: 185ms\n",
            "780:\tlearn: 0.2530827\ttotal: 657ms\tremaining: 184ms\n",
            "781:\tlearn: 0.2527426\ttotal: 657ms\tremaining: 183ms\n",
            "782:\tlearn: 0.2524267\ttotal: 658ms\tremaining: 182ms\n",
            "783:\tlearn: 0.2522092\ttotal: 659ms\tremaining: 182ms\n",
            "784:\tlearn: 0.2520373\ttotal: 660ms\tremaining: 181ms\n",
            "785:\tlearn: 0.2517136\ttotal: 660ms\tremaining: 180ms\n",
            "786:\tlearn: 0.2516256\ttotal: 661ms\tremaining: 179ms\n",
            "787:\tlearn: 0.2514503\ttotal: 662ms\tremaining: 178ms\n",
            "788:\tlearn: 0.2512174\ttotal: 663ms\tremaining: 177ms\n",
            "789:\tlearn: 0.2511444\ttotal: 664ms\tremaining: 176ms\n",
            "790:\tlearn: 0.2510244\ttotal: 664ms\tremaining: 176ms\n",
            "791:\tlearn: 0.2509448\ttotal: 665ms\tremaining: 175ms\n",
            "792:\tlearn: 0.2506890\ttotal: 666ms\tremaining: 174ms\n",
            "793:\tlearn: 0.2504241\ttotal: 667ms\tremaining: 173ms\n",
            "794:\tlearn: 0.2500619\ttotal: 667ms\tremaining: 172ms\n",
            "795:\tlearn: 0.2497989\ttotal: 668ms\tremaining: 171ms\n",
            "796:\tlearn: 0.2496026\ttotal: 669ms\tremaining: 170ms\n",
            "797:\tlearn: 0.2495089\ttotal: 670ms\tremaining: 169ms\n",
            "798:\tlearn: 0.2492318\ttotal: 670ms\tremaining: 169ms\n",
            "799:\tlearn: 0.2490652\ttotal: 671ms\tremaining: 168ms\n",
            "800:\tlearn: 0.2489474\ttotal: 672ms\tremaining: 167ms\n",
            "801:\tlearn: 0.2486072\ttotal: 673ms\tremaining: 166ms\n",
            "802:\tlearn: 0.2484908\ttotal: 673ms\tremaining: 165ms\n",
            "803:\tlearn: 0.2482410\ttotal: 674ms\tremaining: 164ms\n",
            "804:\tlearn: 0.2480040\ttotal: 675ms\tremaining: 163ms\n",
            "805:\tlearn: 0.2477369\ttotal: 676ms\tremaining: 163ms\n",
            "806:\tlearn: 0.2474428\ttotal: 676ms\tremaining: 162ms\n",
            "807:\tlearn: 0.2471954\ttotal: 677ms\tremaining: 161ms\n",
            "808:\tlearn: 0.2468915\ttotal: 678ms\tremaining: 160ms\n",
            "809:\tlearn: 0.2467611\ttotal: 679ms\tremaining: 159ms\n",
            "810:\tlearn: 0.2464154\ttotal: 680ms\tremaining: 158ms\n",
            "811:\tlearn: 0.2461697\ttotal: 680ms\tremaining: 158ms\n",
            "812:\tlearn: 0.2460615\ttotal: 681ms\tremaining: 157ms\n",
            "813:\tlearn: 0.2459026\ttotal: 682ms\tremaining: 156ms\n",
            "814:\tlearn: 0.2457751\ttotal: 683ms\tremaining: 155ms\n",
            "815:\tlearn: 0.2455104\ttotal: 683ms\tremaining: 154ms\n",
            "816:\tlearn: 0.2451024\ttotal: 684ms\tremaining: 153ms\n",
            "817:\tlearn: 0.2447016\ttotal: 685ms\tremaining: 152ms\n",
            "818:\tlearn: 0.2444708\ttotal: 686ms\tremaining: 152ms\n",
            "819:\tlearn: 0.2441931\ttotal: 687ms\tremaining: 151ms\n",
            "820:\tlearn: 0.2440319\ttotal: 687ms\tremaining: 150ms\n",
            "821:\tlearn: 0.2438819\ttotal: 688ms\tremaining: 149ms\n",
            "822:\tlearn: 0.2437189\ttotal: 689ms\tremaining: 148ms\n",
            "823:\tlearn: 0.2435081\ttotal: 690ms\tremaining: 147ms\n",
            "824:\tlearn: 0.2432224\ttotal: 690ms\tremaining: 146ms\n",
            "825:\tlearn: 0.2431993\ttotal: 691ms\tremaining: 146ms\n",
            "826:\tlearn: 0.2430397\ttotal: 691ms\tremaining: 145ms\n",
            "827:\tlearn: 0.2428705\ttotal: 692ms\tremaining: 144ms\n",
            "828:\tlearn: 0.2426330\ttotal: 693ms\tremaining: 143ms\n",
            "829:\tlearn: 0.2424091\ttotal: 694ms\tremaining: 142ms\n",
            "830:\tlearn: 0.2422806\ttotal: 695ms\tremaining: 141ms\n",
            "831:\tlearn: 0.2420971\ttotal: 695ms\tremaining: 140ms\n",
            "832:\tlearn: 0.2417940\ttotal: 696ms\tremaining: 140ms\n",
            "833:\tlearn: 0.2417242\ttotal: 697ms\tremaining: 139ms\n",
            "834:\tlearn: 0.2415454\ttotal: 697ms\tremaining: 138ms\n",
            "835:\tlearn: 0.2413032\ttotal: 698ms\tremaining: 137ms\n",
            "836:\tlearn: 0.2411796\ttotal: 699ms\tremaining: 136ms\n",
            "837:\tlearn: 0.2409180\ttotal: 700ms\tremaining: 135ms\n",
            "838:\tlearn: 0.2407799\ttotal: 701ms\tremaining: 134ms\n",
            "839:\tlearn: 0.2406155\ttotal: 701ms\tremaining: 134ms\n",
            "840:\tlearn: 0.2403601\ttotal: 702ms\tremaining: 133ms\n",
            "841:\tlearn: 0.2401351\ttotal: 703ms\tremaining: 132ms\n",
            "842:\tlearn: 0.2400223\ttotal: 704ms\tremaining: 131ms\n",
            "843:\tlearn: 0.2398432\ttotal: 705ms\tremaining: 130ms\n",
            "844:\tlearn: 0.2397400\ttotal: 705ms\tremaining: 129ms\n",
            "845:\tlearn: 0.2395154\ttotal: 706ms\tremaining: 129ms\n",
            "846:\tlearn: 0.2394372\ttotal: 707ms\tremaining: 128ms\n",
            "847:\tlearn: 0.2393145\ttotal: 708ms\tremaining: 127ms\n",
            "848:\tlearn: 0.2391131\ttotal: 708ms\tremaining: 126ms\n",
            "849:\tlearn: 0.2389082\ttotal: 709ms\tremaining: 125ms\n",
            "850:\tlearn: 0.2387120\ttotal: 710ms\tremaining: 124ms\n",
            "851:\tlearn: 0.2384056\ttotal: 711ms\tremaining: 123ms\n",
            "852:\tlearn: 0.2382296\ttotal: 712ms\tremaining: 123ms\n",
            "853:\tlearn: 0.2380702\ttotal: 712ms\tremaining: 122ms\n",
            "854:\tlearn: 0.2378833\ttotal: 713ms\tremaining: 121ms\n",
            "855:\tlearn: 0.2375972\ttotal: 714ms\tremaining: 120ms\n",
            "856:\tlearn: 0.2374969\ttotal: 715ms\tremaining: 119ms\n",
            "857:\tlearn: 0.2373088\ttotal: 715ms\tremaining: 118ms\n",
            "858:\tlearn: 0.2371799\ttotal: 716ms\tremaining: 118ms\n",
            "859:\tlearn: 0.2370001\ttotal: 718ms\tremaining: 117ms\n",
            "860:\tlearn: 0.2367743\ttotal: 719ms\tremaining: 116ms\n",
            "861:\tlearn: 0.2364274\ttotal: 720ms\tremaining: 115ms\n",
            "862:\tlearn: 0.2360213\ttotal: 721ms\tremaining: 115ms\n",
            "863:\tlearn: 0.2358280\ttotal: 723ms\tremaining: 114ms\n",
            "864:\tlearn: 0.2356935\ttotal: 724ms\tremaining: 113ms\n",
            "865:\tlearn: 0.2354526\ttotal: 725ms\tremaining: 112ms\n",
            "866:\tlearn: 0.2351820\ttotal: 726ms\tremaining: 111ms\n",
            "867:\tlearn: 0.2350424\ttotal: 727ms\tremaining: 110ms\n",
            "868:\tlearn: 0.2348556\ttotal: 727ms\tremaining: 110ms\n",
            "869:\tlearn: 0.2347659\ttotal: 728ms\tremaining: 109ms\n",
            "870:\tlearn: 0.2345933\ttotal: 729ms\tremaining: 108ms\n",
            "871:\tlearn: 0.2343245\ttotal: 730ms\tremaining: 107ms\n",
            "872:\tlearn: 0.2341773\ttotal: 730ms\tremaining: 106ms\n",
            "873:\tlearn: 0.2340504\ttotal: 731ms\tremaining: 105ms\n",
            "874:\tlearn: 0.2338979\ttotal: 732ms\tremaining: 105ms\n",
            "875:\tlearn: 0.2337058\ttotal: 733ms\tremaining: 104ms\n",
            "876:\tlearn: 0.2335936\ttotal: 733ms\tremaining: 103ms\n",
            "877:\tlearn: 0.2332704\ttotal: 734ms\tremaining: 102ms\n",
            "878:\tlearn: 0.2332197\ttotal: 736ms\tremaining: 101ms\n",
            "879:\tlearn: 0.2330752\ttotal: 737ms\tremaining: 101ms\n",
            "880:\tlearn: 0.2328834\ttotal: 739ms\tremaining: 99.8ms\n",
            "881:\tlearn: 0.2327897\ttotal: 739ms\tremaining: 98.9ms\n",
            "882:\tlearn: 0.2325696\ttotal: 740ms\tremaining: 98.1ms\n",
            "883:\tlearn: 0.2323852\ttotal: 741ms\tremaining: 97.2ms\n",
            "884:\tlearn: 0.2321073\ttotal: 742ms\tremaining: 96.4ms\n",
            "885:\tlearn: 0.2318674\ttotal: 742ms\tremaining: 95.5ms\n",
            "886:\tlearn: 0.2317045\ttotal: 743ms\tremaining: 94.6ms\n",
            "887:\tlearn: 0.2314473\ttotal: 744ms\tremaining: 93.8ms\n",
            "888:\tlearn: 0.2311758\ttotal: 745ms\tremaining: 93ms\n",
            "889:\tlearn: 0.2310259\ttotal: 745ms\tremaining: 92.1ms\n",
            "890:\tlearn: 0.2307373\ttotal: 746ms\tremaining: 91.3ms\n",
            "891:\tlearn: 0.2305730\ttotal: 747ms\tremaining: 90.4ms\n",
            "892:\tlearn: 0.2304640\ttotal: 748ms\tremaining: 89.6ms\n",
            "893:\tlearn: 0.2303867\ttotal: 748ms\tremaining: 88.7ms\n",
            "894:\tlearn: 0.2302326\ttotal: 749ms\tremaining: 87.9ms\n",
            "895:\tlearn: 0.2300740\ttotal: 750ms\tremaining: 87ms\n",
            "896:\tlearn: 0.2300428\ttotal: 750ms\tremaining: 86.2ms\n",
            "897:\tlearn: 0.2298101\ttotal: 751ms\tremaining: 85.3ms\n",
            "898:\tlearn: 0.2295707\ttotal: 752ms\tremaining: 84.5ms\n",
            "899:\tlearn: 0.2293263\ttotal: 753ms\tremaining: 83.6ms\n",
            "900:\tlearn: 0.2291717\ttotal: 753ms\tremaining: 82.8ms\n",
            "901:\tlearn: 0.2290479\ttotal: 754ms\tremaining: 82ms\n",
            "902:\tlearn: 0.2289252\ttotal: 755ms\tremaining: 81.1ms\n",
            "903:\tlearn: 0.2288225\ttotal: 756ms\tremaining: 80.3ms\n",
            "904:\tlearn: 0.2287304\ttotal: 756ms\tremaining: 79.4ms\n",
            "905:\tlearn: 0.2285204\ttotal: 757ms\tremaining: 78.6ms\n",
            "906:\tlearn: 0.2283339\ttotal: 758ms\tremaining: 77.7ms\n",
            "907:\tlearn: 0.2282252\ttotal: 759ms\tremaining: 76.9ms\n",
            "908:\tlearn: 0.2280258\ttotal: 760ms\tremaining: 76ms\n",
            "909:\tlearn: 0.2277006\ttotal: 760ms\tremaining: 75.2ms\n",
            "910:\tlearn: 0.2275228\ttotal: 761ms\tremaining: 74.4ms\n",
            "911:\tlearn: 0.2272245\ttotal: 762ms\tremaining: 73.5ms\n",
            "912:\tlearn: 0.2270828\ttotal: 763ms\tremaining: 72.7ms\n",
            "913:\tlearn: 0.2270344\ttotal: 763ms\tremaining: 71.8ms\n",
            "914:\tlearn: 0.2269123\ttotal: 764ms\tremaining: 71ms\n",
            "915:\tlearn: 0.2266250\ttotal: 765ms\tremaining: 70.1ms\n",
            "916:\tlearn: 0.2264611\ttotal: 766ms\tremaining: 69.3ms\n",
            "917:\tlearn: 0.2262844\ttotal: 766ms\tremaining: 68.4ms\n",
            "918:\tlearn: 0.2260135\ttotal: 767ms\tremaining: 67.6ms\n",
            "919:\tlearn: 0.2257104\ttotal: 768ms\tremaining: 66.8ms\n",
            "920:\tlearn: 0.2255571\ttotal: 768ms\tremaining: 65.9ms\n",
            "921:\tlearn: 0.2253909\ttotal: 769ms\tremaining: 65.1ms\n",
            "922:\tlearn: 0.2252446\ttotal: 770ms\tremaining: 64.2ms\n",
            "923:\tlearn: 0.2249500\ttotal: 771ms\tremaining: 63.4ms\n",
            "924:\tlearn: 0.2247926\ttotal: 772ms\tremaining: 62.6ms\n",
            "925:\tlearn: 0.2246572\ttotal: 772ms\tremaining: 61.7ms\n",
            "926:\tlearn: 0.2243843\ttotal: 773ms\tremaining: 60.9ms\n",
            "927:\tlearn: 0.2243124\ttotal: 773ms\tremaining: 60ms\n",
            "928:\tlearn: 0.2242170\ttotal: 774ms\tremaining: 59.2ms\n",
            "929:\tlearn: 0.2239973\ttotal: 775ms\tremaining: 58.3ms\n",
            "930:\tlearn: 0.2237549\ttotal: 776ms\tremaining: 57.5ms\n",
            "931:\tlearn: 0.2234978\ttotal: 776ms\tremaining: 56.7ms\n",
            "932:\tlearn: 0.2231312\ttotal: 777ms\tremaining: 55.8ms\n",
            "933:\tlearn: 0.2229450\ttotal: 778ms\tremaining: 55ms\n",
            "934:\tlearn: 0.2228193\ttotal: 779ms\tremaining: 54.1ms\n",
            "935:\tlearn: 0.2226466\ttotal: 780ms\tremaining: 53.3ms\n",
            "936:\tlearn: 0.2224666\ttotal: 780ms\tremaining: 52.5ms\n",
            "937:\tlearn: 0.2223680\ttotal: 781ms\tremaining: 51.6ms\n",
            "938:\tlearn: 0.2221025\ttotal: 782ms\tremaining: 50.8ms\n",
            "939:\tlearn: 0.2219397\ttotal: 783ms\tremaining: 50ms\n",
            "940:\tlearn: 0.2217241\ttotal: 783ms\tremaining: 49.1ms\n",
            "941:\tlearn: 0.2216028\ttotal: 784ms\tremaining: 48.3ms\n",
            "942:\tlearn: 0.2214311\ttotal: 785ms\tremaining: 47.4ms\n",
            "943:\tlearn: 0.2211856\ttotal: 786ms\tremaining: 46.6ms\n",
            "944:\tlearn: 0.2210109\ttotal: 786ms\tremaining: 45.8ms\n",
            "945:\tlearn: 0.2208370\ttotal: 787ms\tremaining: 44.9ms\n",
            "946:\tlearn: 0.2206053\ttotal: 788ms\tremaining: 44.1ms\n",
            "947:\tlearn: 0.2204599\ttotal: 789ms\tremaining: 43.3ms\n",
            "948:\tlearn: 0.2201944\ttotal: 789ms\tremaining: 42.4ms\n",
            "949:\tlearn: 0.2199360\ttotal: 790ms\tremaining: 41.6ms\n",
            "950:\tlearn: 0.2197596\ttotal: 791ms\tremaining: 40.8ms\n",
            "951:\tlearn: 0.2195350\ttotal: 792ms\tremaining: 39.9ms\n",
            "952:\tlearn: 0.2193698\ttotal: 793ms\tremaining: 39.1ms\n",
            "953:\tlearn: 0.2191451\ttotal: 793ms\tremaining: 38.3ms\n",
            "954:\tlearn: 0.2190691\ttotal: 794ms\tremaining: 37.4ms\n",
            "955:\tlearn: 0.2188370\ttotal: 795ms\tremaining: 36.6ms\n",
            "956:\tlearn: 0.2185376\ttotal: 796ms\tremaining: 35.7ms\n",
            "957:\tlearn: 0.2184215\ttotal: 796ms\tremaining: 34.9ms\n",
            "958:\tlearn: 0.2181978\ttotal: 797ms\tremaining: 34.1ms\n",
            "959:\tlearn: 0.2179167\ttotal: 798ms\tremaining: 33.2ms\n",
            "960:\tlearn: 0.2177427\ttotal: 799ms\tremaining: 32.4ms\n",
            "961:\tlearn: 0.2175570\ttotal: 800ms\tremaining: 31.6ms\n",
            "962:\tlearn: 0.2174791\ttotal: 800ms\tremaining: 30.8ms\n",
            "963:\tlearn: 0.2173600\ttotal: 801ms\tremaining: 29.9ms\n",
            "964:\tlearn: 0.2172669\ttotal: 802ms\tremaining: 29.1ms\n",
            "965:\tlearn: 0.2170095\ttotal: 803ms\tremaining: 28.3ms\n",
            "966:\tlearn: 0.2167482\ttotal: 803ms\tremaining: 27.4ms\n",
            "967:\tlearn: 0.2166216\ttotal: 804ms\tremaining: 26.6ms\n",
            "968:\tlearn: 0.2164035\ttotal: 805ms\tremaining: 25.8ms\n",
            "969:\tlearn: 0.2160990\ttotal: 806ms\tremaining: 24.9ms\n",
            "970:\tlearn: 0.2159528\ttotal: 807ms\tremaining: 24.1ms\n",
            "971:\tlearn: 0.2157007\ttotal: 807ms\tremaining: 23.3ms\n",
            "972:\tlearn: 0.2155381\ttotal: 808ms\tremaining: 22.4ms\n",
            "973:\tlearn: 0.2154284\ttotal: 809ms\tremaining: 21.6ms\n",
            "974:\tlearn: 0.2152821\ttotal: 810ms\tremaining: 20.8ms\n",
            "975:\tlearn: 0.2151723\ttotal: 810ms\tremaining: 19.9ms\n",
            "976:\tlearn: 0.2149489\ttotal: 811ms\tremaining: 19.1ms\n",
            "977:\tlearn: 0.2147780\ttotal: 813ms\tremaining: 18.3ms\n",
            "978:\tlearn: 0.2144114\ttotal: 814ms\tremaining: 17.4ms\n",
            "979:\tlearn: 0.2142144\ttotal: 814ms\tremaining: 16.6ms\n",
            "980:\tlearn: 0.2141195\ttotal: 815ms\tremaining: 15.8ms\n",
            "981:\tlearn: 0.2139267\ttotal: 816ms\tremaining: 15ms\n",
            "982:\tlearn: 0.2137774\ttotal: 817ms\tremaining: 14.1ms\n",
            "983:\tlearn: 0.2136799\ttotal: 818ms\tremaining: 13.3ms\n",
            "984:\tlearn: 0.2135723\ttotal: 820ms\tremaining: 12.5ms\n",
            "985:\tlearn: 0.2133874\ttotal: 820ms\tremaining: 11.6ms\n",
            "986:\tlearn: 0.2131427\ttotal: 821ms\tremaining: 10.8ms\n",
            "987:\tlearn: 0.2128695\ttotal: 822ms\tremaining: 9.98ms\n",
            "988:\tlearn: 0.2125916\ttotal: 823ms\tremaining: 9.15ms\n",
            "989:\tlearn: 0.2123437\ttotal: 823ms\tremaining: 8.32ms\n",
            "990:\tlearn: 0.2121940\ttotal: 824ms\tremaining: 7.49ms\n",
            "991:\tlearn: 0.2119527\ttotal: 825ms\tremaining: 6.65ms\n",
            "992:\tlearn: 0.2116976\ttotal: 826ms\tremaining: 5.82ms\n",
            "993:\tlearn: 0.2114618\ttotal: 827ms\tremaining: 4.99ms\n",
            "994:\tlearn: 0.2112886\ttotal: 827ms\tremaining: 4.16ms\n",
            "995:\tlearn: 0.2110944\ttotal: 829ms\tremaining: 3.33ms\n",
            "996:\tlearn: 0.2109560\ttotal: 830ms\tremaining: 2.5ms\n",
            "997:\tlearn: 0.2109556\ttotal: 830ms\tremaining: 1.66ms\n",
            "998:\tlearn: 0.2107181\ttotal: 831ms\tremaining: 831us\n",
            "999:\tlearn: 0.2104362\ttotal: 832ms\tremaining: 0us\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7f1db2b40550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fs1bjm-CnI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "517ee403-b624-4ee3-c967-41ad3d91d7d6"
      },
      "source": [
        "cm = confusion_matrix(Y_test, CDABPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[28 10]\n",
            " [ 3 82]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3cHPNflCnJD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "4db92df9-779b-446a-f546-f27ddc512eed"
      },
      "source": [
        "accuracy = accuracy_score(Y_test, CDABPred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(Y_test, CDABPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 89.43%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.74      0.81        38\n",
            "           1       0.89      0.96      0.93        85\n",
            "\n",
            "    accuracy                           0.89       123\n",
            "   macro avg       0.90      0.85      0.87       123\n",
            "weighted avg       0.89      0.89      0.89       123\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r5lF8XDCnJH"
      },
      "source": [
        "CDABPred = CDAB.predict(XTest)\n",
        "\n",
        "EVALUATION['CATB'] = list(CDABPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7dBXr4wRALl"
      },
      "source": [
        "EVALUATION = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWm6WYhm4Fgj"
      },
      "source": [
        "###ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoYzMitY7kdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ea2675-0176-4fa4-aa01-6fdcdd90502c"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units = 32, activation = 'relu', input_shape=(15,)))\n",
        "model.add(Dense(units = 256, activation = 'relu'))\n",
        "#model.add(Dense(units = 1024, activation = 'relu'))\n",
        "\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               8448      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 9,217\n",
            "Trainable params: 9,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsKy91V7wKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd1890e-c7dc-4d88-b02b-148d526b9658"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_train, Y_train, batch_size = 10, nb_epoch = 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1086/1086 [==============================] - 0s 409us/step - loss: -1642363.4150 - accuracy: 0.6575\n",
            "Epoch 2/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -21418889.3587 - accuracy: 0.6575\n",
            "Epoch 3/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -106023508.8338 - accuracy: 0.6575\n",
            "Epoch 4/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -311055278.4576 - accuracy: 0.6575\n",
            "Epoch 5/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -657311095.5322 - accuracy: 0.6575\n",
            "Epoch 6/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -1204206235.4696 - accuracy: 0.6575\n",
            "Epoch 7/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1985752759.4770 - accuracy: 0.6575\n",
            "Epoch 8/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -3029431948.6998 - accuracy: 0.6575\n",
            "Epoch 9/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -4357422829.7311 - accuracy: 0.6575\n",
            "Epoch 10/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -6011603120.8250 - accuracy: 0.6575\n",
            "Epoch 11/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -8024117945.5912 - accuracy: 0.6575\n",
            "Epoch 12/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -10391061925.5101 - accuracy: 0.6575\n",
            "Epoch 13/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -13074811270.8361 - accuracy: 0.6575\n",
            "Epoch 14/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -16122015687.7201 - accuracy: 0.6575\n",
            "Epoch 15/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -19632434671.5580 - accuracy: 0.6575\n",
            "Epoch 16/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -23510282583.9263 - accuracy: 0.6575\n",
            "Epoch 17/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -27825420694.2173 - accuracy: 0.6575\n",
            "Epoch 18/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -32570275272.1915 - accuracy: 0.6575\n",
            "Epoch 19/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -37736964579.0055 - accuracy: 0.6575\n",
            "Epoch 20/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -43399719128.7514 - accuracy: 0.6575\n",
            "Epoch 21/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -49601618024.6630 - accuracy: 0.6575\n",
            "Epoch 22/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -56247314863.1455 - accuracy: 0.6575\n",
            "Epoch 23/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -63346910611.5654 - accuracy: 0.6575\n",
            "Epoch 24/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -70953509035.1381 - accuracy: 0.6575\n",
            "Epoch 25/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -79063188314.7551 - accuracy: 0.6575\n",
            "Epoch 26/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -87567521331.8600 - accuracy: 0.6575\n",
            "Epoch 27/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -96618770062.8508 - accuracy: 0.6575\n",
            "Epoch 28/1000\n",
            "1086/1086 [==============================] - 0s 143us/step - loss: -106191823312.5009 - accuracy: 0.6575\n",
            "Epoch 29/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -116319908995.0645 - accuracy: 0.6575\n",
            "Epoch 30/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -126981542117.1271 - accuracy: 0.6575\n",
            "Epoch 31/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -138254177031.0718 - accuracy: 0.6575\n",
            "Epoch 32/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -150116432318.9392 - accuracy: 0.6575\n",
            "Epoch 33/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -162609108413.0534 - accuracy: 0.6575\n",
            "Epoch 34/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -175577664875.9632 - accuracy: 0.6575\n",
            "Epoch 35/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -189306206595.5359 - accuracy: 0.6575\n",
            "Epoch 36/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -203668813777.7974 - accuracy: 0.6575\n",
            "Epoch 37/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -218563592963.3002 - accuracy: 0.6575\n",
            "Epoch 38/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -233904437684.5672 - accuracy: 0.6575\n",
            "Epoch 39/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -249841671997.7606 - accuracy: 0.6575\n",
            "Epoch 40/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -266433399525.5985 - accuracy: 0.6575\n",
            "Epoch 41/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -283670750791.1897 - accuracy: 0.6575\n",
            "Epoch 42/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -301549020087.8674 - accuracy: 0.6575\n",
            "Epoch 43/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -320298711343.6169 - accuracy: 0.6575\n",
            "Epoch 44/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -339231208607.3517 - accuracy: 0.6575\n",
            "Epoch 45/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -358897560477.9374 - accuracy: 0.6575\n",
            "Epoch 46/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -378959212072.5451 - accuracy: 0.6575\n",
            "Epoch 47/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -399754542186.0773 - accuracy: 0.6575\n",
            "Epoch 48/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -421228601764.5378 - accuracy: 0.6575\n",
            "Epoch 49/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -443322428550.8361 - accuracy: 0.6575\n",
            "Epoch 50/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -466170216768.5894 - accuracy: 0.6575\n",
            "Epoch 51/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -489590472868.0663 - accuracy: 0.6575\n",
            "Epoch 52/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -513827355499.9631 - accuracy: 0.6575\n",
            "Epoch 53/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -538629240803.7127 - accuracy: 0.6575\n",
            "Epoch 54/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -564102381853.7017 - accuracy: 0.6575\n",
            "Epoch 55/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -590343756523.7274 - accuracy: 0.6575\n",
            "Epoch 56/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -617264819482.8729 - accuracy: 0.6575\n",
            "Epoch 57/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -645206866319.7937 - accuracy: 0.6575\n",
            "Epoch 58/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -673773472619.0203 - accuracy: 0.6575\n",
            "Epoch 59/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -703240734596.4789 - accuracy: 0.6575\n",
            "Epoch 60/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -733263251493.7163 - accuracy: 0.6575\n",
            "Epoch 61/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -764167577015.3960 - accuracy: 0.6575\n",
            "Epoch 62/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -795530687933.9963 - accuracy: 0.6575\n",
            "Epoch 63/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -827645730227.6243 - accuracy: 0.6575\n",
            "Epoch 64/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -860449657186.5341 - accuracy: 0.6575\n",
            "Epoch 65/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -894036771831.5138 - accuracy: 0.6575\n",
            "Epoch 66/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -927983932455.6022 - accuracy: 0.6575\n",
            "Epoch 67/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -963050995266.0037 - accuracy: 0.6575\n",
            "Epoch 68/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -998415458217.2523 - accuracy: 0.6575\n",
            "Epoch 69/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1034766544799.8232 - accuracy: 0.6575\n",
            "Epoch 70/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1072015817744.0295 - accuracy: 0.6575\n",
            "Epoch 71/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1110333645705.1934 - accuracy: 0.6575\n",
            "Epoch 72/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1148895584270.1436 - accuracy: 0.6575\n",
            "Epoch 73/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1188219727438.2615 - accuracy: 0.6575\n",
            "Epoch 74/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1228265748876.0222 - accuracy: 0.6575\n",
            "Epoch 75/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1268889728446.9395 - accuracy: 0.6575\n",
            "Epoch 76/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1310394833571.1233 - accuracy: 0.6575\n",
            "Epoch 77/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1352574204398.0847 - accuracy: 0.6575\n",
            "Epoch 78/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1395731394147.0056 - accuracy: 0.6575\n",
            "Epoch 79/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1439463233683.0938 - accuracy: 0.6575\n",
            "Epoch 80/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1484529581272.8694 - accuracy: 0.6575\n",
            "Epoch 81/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1530236712252.8179 - accuracy: 0.6575\n",
            "Epoch 82/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -1577480113669.6575 - accuracy: 0.6575\n",
            "Epoch 83/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -1625653681617.7974 - accuracy: 0.6575\n",
            "Epoch 84/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1674580281634.4163 - accuracy: 0.6575\n",
            "Epoch 85/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1724320253563.5212 - accuracy: 0.6575\n",
            "Epoch 86/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1774701313142.8066 - accuracy: 0.6575\n",
            "Epoch 87/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1826492404217.3997 - accuracy: 0.6575\n",
            "Epoch 88/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1878619339679.8232 - accuracy: 0.6575\n",
            "Epoch 89/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -1931840580161.0605 - accuracy: 0.6575\n",
            "Epoch 90/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -1985899755538.8584 - accuracy: 0.6575\n",
            "Epoch 91/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -2040823776951.8674 - accuracy: 0.6575\n",
            "Epoch 92/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -2096715609474.5930 - accuracy: 0.6575\n",
            "Epoch 93/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -2153528779636.4492 - accuracy: 0.6575\n",
            "Epoch 94/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -2211188512070.2466 - accuracy: 0.6575\n",
            "Epoch 95/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -2269235607495.4253 - accuracy: 0.6575\n",
            "Epoch 96/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -2328701467632.9136 - accuracy: 0.6575\n",
            "Epoch 97/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -2388759375713.5913 - accuracy: 0.6575\n",
            "Epoch 98/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -2450026645537.9448 - accuracy: 0.6575\n",
            "Epoch 99/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -2512160207051.6685 - accuracy: 0.6575\n",
            "Epoch 100/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -2574799937800.0146 - accuracy: 0.6575\n",
            "Epoch 101/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -2639126732881.0903 - accuracy: 0.6575\n",
            "Epoch 102/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -2704145034366.3496 - accuracy: 0.6575\n",
            "Epoch 103/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -2770442542658.9463 - accuracy: 0.6575\n",
            "Epoch 104/1000\n",
            "1086/1086 [==============================] - 0s 143us/step - loss: -2837799638926.8506 - accuracy: 0.6575\n",
            "Epoch 105/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -2906572795345.7979 - accuracy: 0.6575\n",
            "Epoch 106/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -2975716748044.7295 - accuracy: 0.6575\n",
            "Epoch 107/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -3046038798652.8179 - accuracy: 0.6575\n",
            "Epoch 108/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -3116537868970.6665 - accuracy: 0.6575\n",
            "Epoch 109/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -3189443756967.3662 - accuracy: 0.6575\n",
            "Epoch 110/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -3262812825166.2612 - accuracy: 0.6575\n",
            "Epoch 111/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -3337153699681.5913 - accuracy: 0.6575\n",
            "Epoch 112/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -3412737083275.0791 - accuracy: 0.6575\n",
            "Epoch 113/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -3489128848035.1230 - accuracy: 0.6575\n",
            "Epoch 114/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -3566067278736.7368 - accuracy: 0.6575\n",
            "Epoch 115/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -3644966426963.4473 - accuracy: 0.6575\n",
            "Epoch 116/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -3724637324953.6943 - accuracy: 0.6575\n",
            "Epoch 117/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -3804905065273.9888 - accuracy: 0.6575\n",
            "Epoch 118/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -3886246621148.1694 - accuracy: 0.6575\n",
            "Epoch 119/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -3969575543824.9727 - accuracy: 0.6575\n",
            "Epoch 120/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -4053589018341.1274 - accuracy: 0.6575\n",
            "Epoch 121/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -4139148103234.9468 - accuracy: 0.6575\n",
            "Epoch 122/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -4225310192788.9800 - accuracy: 0.6575\n",
            "Epoch 123/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -4313284294953.9595 - accuracy: 0.6575\n",
            "Epoch 124/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -4402295233669.8926 - accuracy: 0.6575\n",
            "Epoch 125/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -4491434486457.7539 - accuracy: 0.6575\n",
            "Epoch 126/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -4582095834902.1582 - accuracy: 0.6575\n",
            "Epoch 127/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -4673834868532.3311 - accuracy: 0.6575\n",
            "Epoch 128/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -4766615320858.8730 - accuracy: 0.6575\n",
            "Epoch 129/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -4860467819680.2949 - accuracy: 0.6575\n",
            "Epoch 130/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -4955175771420.7588 - accuracy: 0.6575\n",
            "Epoch 131/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -5050247567103.5283 - accuracy: 0.6575\n",
            "Epoch 132/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -5146983750608.8545 - accuracy: 0.6575\n",
            "Epoch 133/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -5246075297414.8359 - accuracy: 0.6575\n",
            "Epoch 134/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -5344759609928.6045 - accuracy: 0.6575\n",
            "Epoch 135/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -5445448965133.2002 - accuracy: 0.6575\n",
            "Epoch 136/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -5547358635764.2129 - accuracy: 0.6575\n",
            "Epoch 137/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -5651106749415.4844 - accuracy: 0.6575\n",
            "Epoch 138/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -5755377937323.1387 - accuracy: 0.6575\n",
            "Epoch 139/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -5861762354573.9072 - accuracy: 0.6575\n",
            "Epoch 140/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -5969280823318.6299 - accuracy: 0.6575\n",
            "Epoch 141/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -6077839101733.2441 - accuracy: 0.6575\n",
            "Epoch 142/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -6188359180303.0869 - accuracy: 0.6575\n",
            "Epoch 143/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -6300269913135.1455 - accuracy: 0.6575\n",
            "Epoch 144/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -6412524981132.9648 - accuracy: 0.6575\n",
            "Epoch 145/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -6524620317215.1162 - accuracy: 0.6575\n",
            "Epoch 146/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -6639332343649.5918 - accuracy: 0.6575\n",
            "Epoch 147/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -6754876226882.4756 - accuracy: 0.6575\n",
            "Epoch 148/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -6872115792268.0225 - accuracy: 0.6575\n",
            "Epoch 149/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -6989977925175.6318 - accuracy: 0.6575\n",
            "Epoch 150/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -7108884965877.6279 - accuracy: 0.6575\n",
            "Epoch 151/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -7229510005693.9961 - accuracy: 0.6575\n",
            "Epoch 152/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -7351369819243.4922 - accuracy: 0.6575\n",
            "Epoch 153/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -7474165403021.9072 - accuracy: 0.6575\n",
            "Epoch 154/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -7599484162568.4863 - accuracy: 0.6575\n",
            "Epoch 155/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -7724238791153.8564 - accuracy: 0.6575\n",
            "Epoch 156/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -7852263402260.2725 - accuracy: 0.6575\n",
            "Epoch 157/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -7981290449126.0693 - accuracy: 0.6575\n",
            "Epoch 158/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -8112360533555.8604 - accuracy: 0.6575\n",
            "Epoch 159/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -8245201429611.4922 - accuracy: 0.6575\n",
            "Epoch 160/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -8378559445636.9502 - accuracy: 0.6575\n",
            "Epoch 161/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -8513724914459.8154 - accuracy: 0.6575\n",
            "Epoch 162/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -8649267902942.9980 - accuracy: 0.6575\n",
            "Epoch 163/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -8786139278507.6104 - accuracy: 0.6575\n",
            "Epoch 164/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -8925808375804.2285 - accuracy: 0.6575\n",
            "Epoch 165/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -9067255728825.7520 - accuracy: 0.6575\n",
            "Epoch 166/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -9209724505304.8691 - accuracy: 0.6575\n",
            "Epoch 167/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -9352748277171.6250 - accuracy: 0.6575\n",
            "Epoch 168/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -9497523268025.2812 - accuracy: 0.6575\n",
            "Epoch 169/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -9643048073951.4688 - accuracy: 0.6575\n",
            "Epoch 170/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -9790750944487.9551 - accuracy: 0.6575\n",
            "Epoch 171/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -9939885857201.7383 - accuracy: 0.6575\n",
            "Epoch 172/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -10089824163972.0078 - accuracy: 0.6575\n",
            "Epoch 173/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -10242030243832.4551 - accuracy: 0.6575\n",
            "Epoch 174/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -10395688779082.0176 - accuracy: 0.6575\n",
            "Epoch 175/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -10551042924393.1328 - accuracy: 0.6575\n",
            "Epoch 176/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -10707065780039.1914 - accuracy: 0.6575\n",
            "Epoch 177/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -10863885112782.0254 - accuracy: 0.6575\n",
            "Epoch 178/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -11021139029435.1680 - accuracy: 0.6575\n",
            "Epoch 179/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -11180356477425.8574 - accuracy: 0.6575\n",
            "Epoch 180/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -11340052514306.8281 - accuracy: 0.6575\n",
            "Epoch 181/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -11501905358387.8613 - accuracy: 0.6575\n",
            "Epoch 182/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -11666641094840.8086 - accuracy: 0.6575\n",
            "Epoch 183/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -11831251696819.1523 - accuracy: 0.6575\n",
            "Epoch 184/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -11996934477714.6230 - accuracy: 0.6575\n",
            "Epoch 185/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -12165353201498.0469 - accuracy: 0.6575\n",
            "Epoch 186/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -12335138985965.1406 - accuracy: 0.6575\n",
            "Epoch 187/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -12507652908139.4902 - accuracy: 0.6575\n",
            "Epoch 188/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -12681239109224.6621 - accuracy: 0.6575\n",
            "Epoch 189/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -12856121418919.8379 - accuracy: 0.6575\n",
            "Epoch 190/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -13034977541691.4023 - accuracy: 0.6575\n",
            "Epoch 191/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -13213289379662.7324 - accuracy: 0.6575\n",
            "Epoch 192/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -13394185215924.5684 - accuracy: 0.6575\n",
            "Epoch 193/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -13575429925893.6582 - accuracy: 0.6575\n",
            "Epoch 194/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -13759197013733.1270 - accuracy: 0.6575\n",
            "Epoch 195/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -13944308957908.1543 - accuracy: 0.6575\n",
            "Epoch 196/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -14130593280143.3223 - accuracy: 0.6575\n",
            "Epoch 197/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -14318608813785.8105 - accuracy: 0.6575\n",
            "Epoch 198/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -14507884570552.3379 - accuracy: 0.6575\n",
            "Epoch 199/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -14699019231941.0684 - accuracy: 0.6575\n",
            "Epoch 200/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -14891753933045.1562 - accuracy: 0.6575\n",
            "Epoch 201/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -15086270451629.0234 - accuracy: 0.6575\n",
            "Epoch 202/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -15283879504041.7227 - accuracy: 0.6575\n",
            "Epoch 203/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -15482949958276.9512 - accuracy: 0.6575\n",
            "Epoch 204/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -15685865568976.3828 - accuracy: 0.6575\n",
            "Epoch 205/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -15888897484366.2617 - accuracy: 0.6575\n",
            "Epoch 206/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -16094273791264.5312 - accuracy: 0.6575\n",
            "Epoch 207/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -16300706851212.0215 - accuracy: 0.6575\n",
            "Epoch 208/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -16509216478428.6406 - accuracy: 0.6575\n",
            "Epoch 209/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -16720073088797.7031 - accuracy: 0.6575\n",
            "Epoch 210/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -16932662288987.4609 - accuracy: 0.6575\n",
            "Epoch 211/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -17145465998945.1211 - accuracy: 0.6575\n",
            "Epoch 212/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -17359854563984.2637 - accuracy: 0.6575\n",
            "Epoch 213/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -17574453049400.5742 - accuracy: 0.6575\n",
            "Epoch 214/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -17790271523634.4453 - accuracy: 0.6575\n",
            "Epoch 215/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -18009474614847.1758 - accuracy: 0.6575\n",
            "Epoch 216/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -18229723546354.3242 - accuracy: 0.6575\n",
            "Epoch 217/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -18452003998755.8320 - accuracy: 0.6575\n",
            "Epoch 218/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -18676110148217.6328 - accuracy: 0.6575\n",
            "Epoch 219/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -18901879405958.3672 - accuracy: 0.6575\n",
            "Epoch 220/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -19130016446067.9766 - accuracy: 0.6575\n",
            "Epoch 221/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -19359329595806.8789 - accuracy: 0.6575\n",
            "Epoch 222/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -19593855236695.6914 - accuracy: 0.6575\n",
            "Epoch 223/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -19827871826764.8477 - accuracy: 0.6575\n",
            "Epoch 224/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -20062987964091.6367 - accuracy: 0.6575\n",
            "Epoch 225/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -20301295880014.7344 - accuracy: 0.6575\n",
            "Epoch 226/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -20541242577386.3125 - accuracy: 0.6575\n",
            "Epoch 227/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -20783612888186.5781 - accuracy: 0.6575\n",
            "Epoch 228/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -21025286195746.8867 - accuracy: 0.6575\n",
            "Epoch 229/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -21268678492461.7344 - accuracy: 0.6575\n",
            "Epoch 230/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -21514719216611.7148 - accuracy: 0.6575\n",
            "Epoch 231/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -21763165730359.6289 - accuracy: 0.6575\n",
            "Epoch 232/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -22013107314884.1250 - accuracy: 0.6575\n",
            "Epoch 233/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -22265086350285.0859 - accuracy: 0.6575\n",
            "Epoch 234/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -22518580626433.8867 - accuracy: 0.6575\n",
            "Epoch 235/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -22773635182765.4961 - accuracy: 0.6575\n",
            "Epoch 236/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -23029369643807.5859 - accuracy: 0.6575\n",
            "Epoch 237/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -23286265585679.0898 - accuracy: 0.6575\n",
            "Epoch 238/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -23543880411969.5312 - accuracy: 0.6575\n",
            "Epoch 239/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -23806328439870.2305 - accuracy: 0.6575\n",
            "Epoch 240/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -24071540306636.6133 - accuracy: 0.6575\n",
            "Epoch 241/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -24336085661360.3242 - accuracy: 0.6575\n",
            "Epoch 242/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -24601332886126.3203 - accuracy: 0.6575\n",
            "Epoch 243/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -24869189552071.4258 - accuracy: 0.6575\n",
            "Epoch 244/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -25140229144817.3828 - accuracy: 0.6575\n",
            "Epoch 245/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -25413566987744.8828 - accuracy: 0.6575\n",
            "Epoch 246/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -25689597533348.0664 - accuracy: 0.6575\n",
            "Epoch 247/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -25967120120468.0352 - accuracy: 0.6575\n",
            "Epoch 248/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -26244954869194.2539 - accuracy: 0.6575\n",
            "Epoch 249/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -26522629524938.2539 - accuracy: 0.6575\n",
            "Epoch 250/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -26805889779564.9062 - accuracy: 0.6575\n",
            "Epoch 251/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -27090146897882.2812 - accuracy: 0.6575\n",
            "Epoch 252/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -27376427058087.3672 - accuracy: 0.6575\n",
            "Epoch 253/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -27665819456853.3320 - accuracy: 0.6575\n",
            "Epoch 254/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -27956134820349.1719 - accuracy: 0.6575\n",
            "Epoch 255/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -28249805411246.9102 - accuracy: 0.6575\n",
            "Epoch 256/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -28541278711860.8008 - accuracy: 0.6575\n",
            "Epoch 257/1000\n",
            "1086/1086 [==============================] - 0s 158us/step - loss: -28839256313531.6367 - accuracy: 0.6575\n",
            "Epoch 258/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -29136032064257.4141 - accuracy: 0.6575\n",
            "Epoch 259/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -29436913107215.5586 - accuracy: 0.6575\n",
            "Epoch 260/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -29739875893368.6914 - accuracy: 0.6575\n",
            "Epoch 261/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -30043363142827.6094 - accuracy: 0.6575\n",
            "Epoch 262/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -30352305578753.4141 - accuracy: 0.6575\n",
            "Epoch 263/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -30660937300296.1328 - accuracy: 0.6575\n",
            "Epoch 264/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -30974298961644.6719 - accuracy: 0.6575\n",
            "Epoch 265/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -31287833985386.0781 - accuracy: 0.6575\n",
            "Epoch 266/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -31600843018579.4453 - accuracy: 0.6575\n",
            "Epoch 267/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -31915021268173.5547 - accuracy: 0.6575\n",
            "Epoch 268/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -32231952645057.7695 - accuracy: 0.6575\n",
            "Epoch 269/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -32551189966780.1094 - accuracy: 0.6575\n",
            "Epoch 270/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -32875101966856.4883 - accuracy: 0.6575\n",
            "Epoch 271/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -33199932009945.3398 - accuracy: 0.6575\n",
            "Epoch 272/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -33523794747105.3555 - accuracy: 0.6575\n",
            "Epoch 273/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -33850154173653.0977 - accuracy: 0.6575\n",
            "Epoch 274/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -34179848040138.7266 - accuracy: 0.6575\n",
            "Epoch 275/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -34512793453901.7891 - accuracy: 0.6575\n",
            "Epoch 276/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -34846162987690.6680 - accuracy: 0.6575\n",
            "Epoch 277/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -35180818263030.5703 - accuracy: 0.6575\n",
            "Epoch 278/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -35518910565562.6953 - accuracy: 0.6575\n",
            "Epoch 279/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -35859774339249.2656 - accuracy: 0.6575\n",
            "Epoch 280/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -36202983940865.4141 - accuracy: 0.6575\n",
            "Epoch 281/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -36551064798119.3672 - accuracy: 0.6575\n",
            "Epoch 282/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -36897646533433.9844 - accuracy: 0.6575\n",
            "Epoch 283/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -37247841121304.5156 - accuracy: 0.6575\n",
            "Epoch 284/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -37597376403889.7344 - accuracy: 0.6575\n",
            "Epoch 285/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -37949940015963.9375 - accuracy: 0.6575\n",
            "Epoch 286/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -38306789659597.0859 - accuracy: 0.6575\n",
            "Epoch 287/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -38664087894766.5547 - accuracy: 0.6575\n",
            "Epoch 288/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -39025187249255.7188 - accuracy: 0.6575\n",
            "Epoch 289/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -39390538754881.5312 - accuracy: 0.6575\n",
            "Epoch 290/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -39754517122234.6953 - accuracy: 0.6575\n",
            "Epoch 291/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -40122453382787.0625 - accuracy: 0.6575\n",
            "Epoch 292/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -40490063003104.8828 - accuracy: 0.6575\n",
            "Epoch 293/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -40859293968976.1484 - accuracy: 0.6575\n",
            "Epoch 294/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -41230139420966.1875 - accuracy: 0.6575\n",
            "Epoch 295/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -41606041765167.6172 - accuracy: 0.6575\n",
            "Epoch 296/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -41977855198119.3672 - accuracy: 0.6575\n",
            "Epoch 297/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -42358056539296.2969 - accuracy: 0.6575\n",
            "Epoch 298/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -42737688142382.2031 - accuracy: 0.6575\n",
            "Epoch 299/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -43120041145308.1719 - accuracy: 0.6575\n",
            "Epoch 300/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -43500194730655.3516 - accuracy: 0.6575\n",
            "Epoch 301/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -43882051316636.0547 - accuracy: 0.6575\n",
            "Epoch 302/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -44271249144799.9375 - accuracy: 0.6575\n",
            "Epoch 303/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -44657355250489.9844 - accuracy: 0.6575\n",
            "Epoch 304/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -45047386751971.7109 - accuracy: 0.6575\n",
            "Epoch 305/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -45438919121993.5469 - accuracy: 0.6575\n",
            "Epoch 306/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -45833056879919.6172 - accuracy: 0.6575\n",
            "Epoch 307/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -46230421005298.7969 - accuracy: 0.6575\n",
            "Epoch 308/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -46628140534699.1328 - accuracy: 0.6575\n",
            "Epoch 309/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -47028697776339.2109 - accuracy: 0.6575\n",
            "Epoch 310/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -47434381434231.2734 - accuracy: 0.6575\n",
            "Epoch 311/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -47836885746629.5391 - accuracy: 0.6575\n",
            "Epoch 312/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -48246150202792.3047 - accuracy: 0.6575\n",
            "Epoch 313/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -48656660456676.1797 - accuracy: 0.6575\n",
            "Epoch 314/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -49072808555272.9609 - accuracy: 0.6575\n",
            "Epoch 315/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -49488648393405.5234 - accuracy: 0.6575\n",
            "Epoch 316/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -49906462572031.0547 - accuracy: 0.6575\n",
            "Epoch 317/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -50328682862301.5859 - accuracy: 0.6575\n",
            "Epoch 318/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -50753214013304.2188 - accuracy: 0.6575\n",
            "Epoch 319/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -51181827254864.1484 - accuracy: 0.6575\n",
            "Epoch 320/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -51614546310802.1562 - accuracy: 0.6575\n",
            "Epoch 321/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -52052415232629.8594 - accuracy: 0.6575\n",
            "Epoch 322/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -52488384843119.7344 - accuracy: 0.6575\n",
            "Epoch 323/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -52924608301452.0234 - accuracy: 0.6575\n",
            "Epoch 324/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -53363126490392.9922 - accuracy: 0.6575\n",
            "Epoch 325/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -53805684250062.0312 - accuracy: 0.6575\n",
            "Epoch 326/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -54249095119240.2500 - accuracy: 0.6575\n",
            "Epoch 327/1000\n",
            "1086/1086 [==============================] - 0s 141us/step - loss: -54696496328213.6875 - accuracy: 0.6575\n",
            "Epoch 328/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -55150254043641.3984 - accuracy: 0.6575\n",
            "Epoch 329/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -55601112477809.1484 - accuracy: 0.6575\n",
            "Epoch 330/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -56058193268532.3281 - accuracy: 0.6575\n",
            "Epoch 331/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -56513150677889.6484 - accuracy: 0.6575\n",
            "Epoch 332/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -56970275736430.7891 - accuracy: 0.6575\n",
            "Epoch 333/1000\n",
            "1086/1086 [==============================] - 0s 144us/step - loss: -57431624531032.6328 - accuracy: 0.6575\n",
            "Epoch 334/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -57893601519212.4297 - accuracy: 0.6575\n",
            "Epoch 335/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -58360145795716.9531 - accuracy: 0.6575\n",
            "Epoch 336/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -58827650082231.3984 - accuracy: 0.6575\n",
            "Epoch 337/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -59295897052267.4922 - accuracy: 0.6575\n",
            "Epoch 338/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -59766085936356.1797 - accuracy: 0.6575\n",
            "Epoch 339/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -60239907709999.1406 - accuracy: 0.6575\n",
            "Epoch 340/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -60717076525199.3203 - accuracy: 0.6575\n",
            "Epoch 341/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -61197114846832.2031 - accuracy: 0.6575\n",
            "Epoch 342/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -61678505825711.8516 - accuracy: 0.6575\n",
            "Epoch 343/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -62161661947111.9531 - accuracy: 0.6575\n",
            "Epoch 344/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -62643130027828.3281 - accuracy: 0.6575\n",
            "Epoch 345/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -63129242209889.1172 - accuracy: 0.6575\n",
            "Epoch 346/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -63613529185987.1797 - accuracy: 0.6575\n",
            "Epoch 347/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -64104960623074.7656 - accuracy: 0.6575\n",
            "Epoch 348/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -64598767548985.5156 - accuracy: 0.6575\n",
            "Epoch 349/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -65094472181537.4688 - accuracy: 0.6575\n",
            "Epoch 350/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -65599227401165.0859 - accuracy: 0.6575\n",
            "Epoch 351/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -66104573135877.6562 - accuracy: 0.6575\n",
            "Epoch 352/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -66613104691233.9453 - accuracy: 0.6575\n",
            "Epoch 353/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -67121713937768.1875 - accuracy: 0.6575\n",
            "Epoch 354/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -67637366923098.0547 - accuracy: 0.6575\n",
            "Epoch 355/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -68150387594038.2109 - accuracy: 0.6575\n",
            "Epoch 356/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -68669078281711.9688 - accuracy: 0.6575\n",
            "Epoch 357/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -69191463267982.3750 - accuracy: 0.6575\n",
            "Epoch 358/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -69716403908057.3516 - accuracy: 0.6575\n",
            "Epoch 359/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -70246423313436.2969 - accuracy: 0.6575\n",
            "Epoch 360/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -70782701448092.0625 - accuracy: 0.6575\n",
            "Epoch 361/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -71318155564101.7656 - accuracy: 0.6575\n",
            "Epoch 362/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -71855648271169.5312 - accuracy: 0.6575\n",
            "Epoch 363/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -72390227503183.2031 - accuracy: 0.6575\n",
            "Epoch 364/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -72930498942564.8906 - accuracy: 0.6575\n",
            "Epoch 365/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -73475020052521.5000 - accuracy: 0.6575\n",
            "Epoch 366/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -74025737874422.5625 - accuracy: 0.6575\n",
            "Epoch 367/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -74575175121797.4219 - accuracy: 0.6575\n",
            "Epoch 368/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -75125971085179.9844 - accuracy: 0.6575\n",
            "Epoch 369/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -75674009506584.0469 - accuracy: 0.6575\n",
            "Epoch 370/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -76230782478686.7656 - accuracy: 0.6575\n",
            "Epoch 371/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -76787851724562.3906 - accuracy: 0.6575\n",
            "Epoch 372/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -77345560463243.0781 - accuracy: 0.6575\n",
            "Epoch 373/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -77907520115268.8281 - accuracy: 0.6575\n",
            "Epoch 374/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -78475921683329.6562 - accuracy: 0.6575\n",
            "Epoch 375/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -79045954251374.3125 - accuracy: 0.6575\n",
            "Epoch 376/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -79616319084887.2188 - accuracy: 0.6575\n",
            "Epoch 377/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -80190713419757.1406 - accuracy: 0.6575\n",
            "Epoch 378/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -80771390563133.7656 - accuracy: 0.6575\n",
            "Epoch 379/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -81353905864341.9219 - accuracy: 0.6575\n",
            "Epoch 380/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -81939430321836.5625 - accuracy: 0.6575\n",
            "Epoch 381/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -82528714591237.6562 - accuracy: 0.6575\n",
            "Epoch 382/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -83119024537418.9531 - accuracy: 0.6575\n",
            "Epoch 383/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -83714992892375.4531 - accuracy: 0.6575\n",
            "Epoch 384/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -84310709473951.3594 - accuracy: 0.6575\n",
            "Epoch 385/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -84911173339381.1562 - accuracy: 0.6575\n",
            "Epoch 386/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -85514499209966.5469 - accuracy: 0.6575\n",
            "Epoch 387/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -86121632064378.1094 - accuracy: 0.6575\n",
            "Epoch 388/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -86728774417406.1094 - accuracy: 0.6575\n",
            "Epoch 389/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -87340494694660.2500 - accuracy: 0.6575\n",
            "Epoch 390/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -87949815415447.8125 - accuracy: 0.6575\n",
            "Epoch 391/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -88567957342422.9844 - accuracy: 0.6575\n",
            "Epoch 392/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -89193490531480.7500 - accuracy: 0.6575\n",
            "Epoch 393/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -89817923983492.0156 - accuracy: 0.6575\n",
            "Epoch 394/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -90449234207881.6719 - accuracy: 0.6575\n",
            "Epoch 395/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -91080580475381.6250 - accuracy: 0.6575\n",
            "Epoch 396/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -91715111381161.7344 - accuracy: 0.6575\n",
            "Epoch 397/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -92351892159787.8438 - accuracy: 0.6575\n",
            "Epoch 398/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -92987474701483.6094 - accuracy: 0.6575\n",
            "Epoch 399/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -93625783114010.8750 - accuracy: 0.6575\n",
            "Epoch 400/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -94262540731031.8125 - accuracy: 0.6575\n",
            "Epoch 401/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -94904176175394.4219 - accuracy: 0.6575\n",
            "Epoch 402/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -95543247614879.8281 - accuracy: 0.6575\n",
            "Epoch 403/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -96187852521230.6094 - accuracy: 0.6575\n",
            "Epoch 404/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -96835099993346.3594 - accuracy: 0.6575\n",
            "Epoch 405/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -97489164796085.0469 - accuracy: 0.6575\n",
            "Epoch 406/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -98149635789411.0000 - accuracy: 0.6575\n",
            "Epoch 407/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -98810989023782.6562 - accuracy: 0.6575\n",
            "Epoch 408/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -99479661865612.5000 - accuracy: 0.6575\n",
            "Epoch 409/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -100150877249339.8750 - accuracy: 0.6575\n",
            "Epoch 410/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -100825692989730.4219 - accuracy: 0.6575\n",
            "Epoch 411/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -101505399688559.7344 - accuracy: 0.6575\n",
            "Epoch 412/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -102187314702686.7656 - accuracy: 0.6575\n",
            "Epoch 413/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -102863954149677.7344 - accuracy: 0.6575\n",
            "Epoch 414/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -103549177600433.7500 - accuracy: 0.6575\n",
            "Epoch 415/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -104237213112361.5000 - accuracy: 0.6575\n",
            "Epoch 416/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -104928049527685.4219 - accuracy: 0.6575\n",
            "Epoch 417/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -105623358577315.1250 - accuracy: 0.6575\n",
            "Epoch 418/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -106327456115581.8750 - accuracy: 0.6575\n",
            "Epoch 419/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -107024836990142.4688 - accuracy: 0.6575\n",
            "Epoch 420/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -107734188259625.9688 - accuracy: 0.6575\n",
            "Epoch 421/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -108440185690636.2656 - accuracy: 0.6575\n",
            "Epoch 422/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -109152926721493.5625 - accuracy: 0.6575\n",
            "Epoch 423/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -109866690622307.4688 - accuracy: 0.6575\n",
            "Epoch 424/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -110579453102626.8906 - accuracy: 0.6575\n",
            "Epoch 425/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -111299733780536.5781 - accuracy: 0.6575\n",
            "Epoch 426/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -112024041233843.6250 - accuracy: 0.6575\n",
            "Epoch 427/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -112748437262011.6406 - accuracy: 0.6575\n",
            "Epoch 428/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -113478606455628.8438 - accuracy: 0.6575\n",
            "Epoch 429/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -114205638005467.6875 - accuracy: 0.6575\n",
            "Epoch 430/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -114937753008788.0469 - accuracy: 0.6575\n",
            "Epoch 431/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -115671270854723.8750 - accuracy: 0.6575\n",
            "Epoch 432/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -116406561854744.9844 - accuracy: 0.6575\n",
            "Epoch 433/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -117144802477812.2188 - accuracy: 0.6575\n",
            "Epoch 434/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -117888041684878.8594 - accuracy: 0.6575\n",
            "Epoch 435/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -118638275216597.0938 - accuracy: 0.6575\n",
            "Epoch 436/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -119386717554329.7031 - accuracy: 0.6575\n",
            "Epoch 437/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -120134420388424.5938 - accuracy: 0.6575\n",
            "Epoch 438/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -120881025101062.1250 - accuracy: 0.6575\n",
            "Epoch 439/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -121631449704134.9531 - accuracy: 0.6575\n",
            "Epoch 440/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -122384480646651.2812 - accuracy: 0.6575\n",
            "Epoch 441/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -123139656710381.6094 - accuracy: 0.6575\n",
            "Epoch 442/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -123895431009892.8906 - accuracy: 0.6575\n",
            "Epoch 443/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -124662314896812.0938 - accuracy: 0.6575\n",
            "Epoch 444/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -125433795276490.7188 - accuracy: 0.6575\n",
            "Epoch 445/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -126204656904603.1094 - accuracy: 0.6575\n",
            "Epoch 446/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -126989084149673.2500 - accuracy: 0.6575\n",
            "Epoch 447/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -127774251963854.0156 - accuracy: 0.6575\n",
            "Epoch 448/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -128554556494616.0469 - accuracy: 0.6575\n",
            "Epoch 449/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -129338227742701.1406 - accuracy: 0.6575\n",
            "Epoch 450/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -130125080916382.8906 - accuracy: 0.6575\n",
            "Epoch 451/1000\n",
            "1086/1086 [==============================] - 0s 118us/step - loss: -130919592208704.5938 - accuracy: 0.6575\n",
            "Epoch 452/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -131715812240585.7812 - accuracy: 0.6575\n",
            "Epoch 453/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -132515574118637.6250 - accuracy: 0.6575\n",
            "Epoch 454/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -133319855347662.9688 - accuracy: 0.6575\n",
            "Epoch 455/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -134131748809265.9688 - accuracy: 0.6575\n",
            "Epoch 456/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -134939912495209.6094 - accuracy: 0.6575\n",
            "Epoch 457/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -135753505924565.5625 - accuracy: 0.6575\n",
            "Epoch 458/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -136571276018720.0625 - accuracy: 0.6575\n",
            "Epoch 459/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -137390454064039.3594 - accuracy: 0.6575\n",
            "Epoch 460/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -138211984517199.2031 - accuracy: 0.6575\n",
            "Epoch 461/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -139034806788955.9219 - accuracy: 0.6575\n",
            "Epoch 462/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -139867190267790.8594 - accuracy: 0.6575\n",
            "Epoch 463/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -140694077723627.2500 - accuracy: 0.6575\n",
            "Epoch 464/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -141531052027745.5938 - accuracy: 0.6575\n",
            "Epoch 465/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -142360466534845.0312 - accuracy: 0.6575\n",
            "Epoch 466/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -143204898059469.5625 - accuracy: 0.6575\n",
            "Epoch 467/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -144044303096483.1250 - accuracy: 0.6575\n",
            "Epoch 468/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -144888918332395.2500 - accuracy: 0.6575\n",
            "Epoch 469/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -145738339593297.0938 - accuracy: 0.6575\n",
            "Epoch 470/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -146586808530306.5938 - accuracy: 0.6575\n",
            "Epoch 471/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -147443704550379.2500 - accuracy: 0.6575\n",
            "Epoch 472/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -148302592459687.3438 - accuracy: 0.6575\n",
            "Epoch 473/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -149161349466447.6875 - accuracy: 0.6575\n",
            "Epoch 474/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -150024474547782.7188 - accuracy: 0.6575\n",
            "Epoch 475/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -150894953426332.9688 - accuracy: 0.6575\n",
            "Epoch 476/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -151767127215984.6875 - accuracy: 0.6575\n",
            "Epoch 477/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -152642483602765.8125 - accuracy: 0.6575\n",
            "Epoch 478/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -153526715708893.0938 - accuracy: 0.6575\n",
            "Epoch 479/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -154398607433049.1250 - accuracy: 0.6575\n",
            "Epoch 480/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -155285309312941.0000 - accuracy: 0.6575\n",
            "Epoch 481/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -156168835366791.2812 - accuracy: 0.6575\n",
            "Epoch 482/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -157059182411625.1562 - accuracy: 0.6575\n",
            "Epoch 483/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -157958696300234.7188 - accuracy: 0.6575\n",
            "Epoch 484/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -158858656076477.5312 - accuracy: 0.6575\n",
            "Epoch 485/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -159767654057414.4688 - accuracy: 0.6575\n",
            "Epoch 486/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -160671213780207.5000 - accuracy: 0.6575\n",
            "Epoch 487/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -161578815436645.3750 - accuracy: 0.6575\n",
            "Epoch 488/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -162486720081580.5312 - accuracy: 0.6575\n",
            "Epoch 489/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -163402714545391.5000 - accuracy: 0.6575\n",
            "Epoch 490/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -164326168450932.4688 - accuracy: 0.6575\n",
            "Epoch 491/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -165248354805507.3125 - accuracy: 0.6575\n",
            "Epoch 492/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -166165057323883.0312 - accuracy: 0.6575\n",
            "Epoch 493/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -167087855306997.1562 - accuracy: 0.6575\n",
            "Epoch 494/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -168014378231796.6875 - accuracy: 0.6575\n",
            "Epoch 495/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -168946659142193.9688 - accuracy: 0.6575\n",
            "Epoch 496/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -169889909772174.8438 - accuracy: 0.6575\n",
            "Epoch 497/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -170830827807549.7812 - accuracy: 0.6575\n",
            "Epoch 498/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -171778019885900.8438 - accuracy: 0.6575\n",
            "Epoch 499/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -172726421140625.1875 - accuracy: 0.6575\n",
            "Epoch 500/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -173666277125170.9375 - accuracy: 0.6575\n",
            "Epoch 501/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -174625187724406.8125 - accuracy: 0.6575\n",
            "Epoch 502/1000\n",
            "1086/1086 [==============================] - 0s 117us/step - loss: -175578783368043.0312 - accuracy: 0.6575\n",
            "Epoch 503/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -176542454809843.2812 - accuracy: 0.6575\n",
            "Epoch 504/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -177505719303739.4062 - accuracy: 0.6575\n",
            "Epoch 505/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -178471927432887.8438 - accuracy: 0.6575\n",
            "Epoch 506/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -179439616010862.3438 - accuracy: 0.6575\n",
            "Epoch 507/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -180415880600347.8125 - accuracy: 0.6575\n",
            "Epoch 508/1000\n",
            "1086/1086 [==============================] - 0s 119us/step - loss: -181394041884226.9688 - accuracy: 0.6575\n",
            "Epoch 509/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -182373580328333.9062 - accuracy: 0.6575\n",
            "Epoch 510/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -183357926684577.6875 - accuracy: 0.6575\n",
            "Epoch 511/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -184339468478030.2812 - accuracy: 0.6575\n",
            "Epoch 512/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -185326284690833.6562 - accuracy: 0.6575\n",
            "Epoch 513/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -186312225746272.6562 - accuracy: 0.6575\n",
            "Epoch 514/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -187313201914896.9688 - accuracy: 0.6575\n",
            "Epoch 515/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -188322940766905.7500 - accuracy: 0.6575\n",
            "Epoch 516/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -189335766100922.2188 - accuracy: 0.6575\n",
            "Epoch 517/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -190342236047665.5000 - accuracy: 0.6575\n",
            "Epoch 518/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -191354510996595.0625 - accuracy: 0.6575\n",
            "Epoch 519/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -192371064848896.9375 - accuracy: 0.6575\n",
            "Epoch 520/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -193392078979977.1875 - accuracy: 0.6575\n",
            "Epoch 521/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -194410344331350.7500 - accuracy: 0.6575\n",
            "Epoch 522/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -195442984706213.9375 - accuracy: 0.6575\n",
            "Epoch 523/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -196474861650161.3750 - accuracy: 0.6575\n",
            "Epoch 524/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -197510999707655.5312 - accuracy: 0.6575\n",
            "Epoch 525/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -198547789530928.5625 - accuracy: 0.6575\n",
            "Epoch 526/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -199589568041178.7500 - accuracy: 0.6575\n",
            "Epoch 527/1000\n",
            "1086/1086 [==============================] - 0s 142us/step - loss: -200630122944825.0625 - accuracy: 0.6575\n",
            "Epoch 528/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -201674842865724.3438 - accuracy: 0.6575\n",
            "Epoch 529/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -202716680909214.8750 - accuracy: 0.6575\n",
            "Epoch 530/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -203773062099570.0938 - accuracy: 0.6575\n",
            "Epoch 531/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -204828332093854.8750 - accuracy: 0.6575\n",
            "Epoch 532/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -205900518297185.1250 - accuracy: 0.6575\n",
            "Epoch 533/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -206965316135051.5312 - accuracy: 0.6575\n",
            "Epoch 534/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -208030951325303.7500 - accuracy: 0.6575\n",
            "Epoch 535/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -209101750321725.2812 - accuracy: 0.6575\n",
            "Epoch 536/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -210185458719153.7188 - accuracy: 0.6575\n",
            "Epoch 537/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -211267255542740.6250 - accuracy: 0.6575\n",
            "Epoch 538/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -212349258634551.1562 - accuracy: 0.6575\n",
            "Epoch 539/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -213439890722823.5312 - accuracy: 0.6575\n",
            "Epoch 540/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -214534673139112.3125 - accuracy: 0.6575\n",
            "Epoch 541/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -215633043659366.7812 - accuracy: 0.6575\n",
            "Epoch 542/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -216736431832062.1250 - accuracy: 0.6575\n",
            "Epoch 543/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -217839419091475.8125 - accuracy: 0.6575\n",
            "Epoch 544/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -218941392534797.6875 - accuracy: 0.6575\n",
            "Epoch 545/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -220045463308744.3750 - accuracy: 0.6575\n",
            "Epoch 546/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -221154068059417.0000 - accuracy: 0.6575\n",
            "Epoch 547/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -222258283707028.0312 - accuracy: 0.6575\n",
            "Epoch 548/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -223370707546979.5000 - accuracy: 0.6575\n",
            "Epoch 549/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -224483652738725.0000 - accuracy: 0.6575\n",
            "Epoch 550/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -225605431663810.2188 - accuracy: 0.6575\n",
            "Epoch 551/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -226720978557995.3750 - accuracy: 0.6575\n",
            "Epoch 552/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -227840474643816.2188 - accuracy: 0.6575\n",
            "Epoch 553/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -228956919929627.8125 - accuracy: 0.6575\n",
            "Epoch 554/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -230087943254249.8438 - accuracy: 0.6575\n",
            "Epoch 555/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -231215919058800.6875 - accuracy: 0.6575\n",
            "Epoch 556/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -232351557236883.0938 - accuracy: 0.6575\n",
            "Epoch 557/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -233483409064479.1250 - accuracy: 0.6575\n",
            "Epoch 558/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -234636249911514.7500 - accuracy: 0.6575\n",
            "Epoch 559/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -235790096255553.0625 - accuracy: 0.6575\n",
            "Epoch 560/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -236954871875376.5625 - accuracy: 0.6575\n",
            "Epoch 561/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -238104014515069.8750 - accuracy: 0.6575\n",
            "Epoch 562/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -239274354493232.5625 - accuracy: 0.6575\n",
            "Epoch 563/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -240440986527068.8750 - accuracy: 0.6575\n",
            "Epoch 564/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -241612130527431.8750 - accuracy: 0.6575\n",
            "Epoch 565/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -242797185396607.7500 - accuracy: 0.6575\n",
            "Epoch 566/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -243979598970427.4062 - accuracy: 0.6575\n",
            "Epoch 567/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -245165540100673.0625 - accuracy: 0.6575\n",
            "Epoch 568/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -246356403842400.6562 - accuracy: 0.6575\n",
            "Epoch 569/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -247553464319728.4375 - accuracy: 0.6575\n",
            "Epoch 570/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -248744554175380.5000 - accuracy: 0.6575\n",
            "Epoch 571/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -249954553107284.4062 - accuracy: 0.6575\n",
            "Epoch 572/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -251166020427941.9375 - accuracy: 0.6575\n",
            "Epoch 573/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -252383434929870.5000 - accuracy: 0.6575\n",
            "Epoch 574/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -253602780018563.5312 - accuracy: 0.6575\n",
            "Epoch 575/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -254831341141171.1562 - accuracy: 0.6575\n",
            "Epoch 576/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -256055454187007.0625 - accuracy: 0.6575\n",
            "Epoch 577/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -257280957895659.2500 - accuracy: 0.6575\n",
            "Epoch 578/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -258514413013086.3125 - accuracy: 0.6575\n",
            "Epoch 579/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -259750049657156.3750 - accuracy: 0.6575\n",
            "Epoch 580/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -260988455122155.7188 - accuracy: 0.6575\n",
            "Epoch 581/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -262227667005824.7188 - accuracy: 0.6575\n",
            "Epoch 582/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -263460739011623.5938 - accuracy: 0.6575\n",
            "Epoch 583/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -264690611488590.7188 - accuracy: 0.6575\n",
            "Epoch 584/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -265939401660216.0938 - accuracy: 0.6575\n",
            "Epoch 585/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -267181300700440.9688 - accuracy: 0.6575\n",
            "Epoch 586/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -268430589655337.9688 - accuracy: 0.6575\n",
            "Epoch 587/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -269692731456208.4062 - accuracy: 0.6575\n",
            "Epoch 588/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -270952131743057.5938 - accuracy: 0.6575\n",
            "Epoch 589/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -272217013699917.7812 - accuracy: 0.6575\n",
            "Epoch 590/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -273485855958877.8438 - accuracy: 0.6575\n",
            "Epoch 591/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -274755603117684.0000 - accuracy: 0.6575\n",
            "Epoch 592/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -276036362653371.6562 - accuracy: 0.6575\n",
            "Epoch 593/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -277318627212316.3125 - accuracy: 0.6575\n",
            "Epoch 594/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -278609639710131.6250 - accuracy: 0.6575\n",
            "Epoch 595/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -279900027675212.3750 - accuracy: 0.6575\n",
            "Epoch 596/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -281190455718393.3750 - accuracy: 0.6575\n",
            "Epoch 597/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -282473524794377.4375 - accuracy: 0.6575\n",
            "Epoch 598/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -283785409258301.7500 - accuracy: 0.6575\n",
            "Epoch 599/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -285097252864452.5625 - accuracy: 0.6575\n",
            "Epoch 600/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -286402122750808.1250 - accuracy: 0.6575\n",
            "Epoch 601/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -287721290509694.8125 - accuracy: 0.6575\n",
            "Epoch 602/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -289043853316284.6250 - accuracy: 0.6575\n",
            "Epoch 603/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -290374163391474.8125 - accuracy: 0.6575\n",
            "Epoch 604/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -291703816637113.7500 - accuracy: 0.6575\n",
            "Epoch 605/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -293037959610488.6875 - accuracy: 0.6575\n",
            "Epoch 606/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -294378266275681.5625 - accuracy: 0.6575\n",
            "Epoch 607/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -295728857023550.2500 - accuracy: 0.6575\n",
            "Epoch 608/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -297076889746973.2500 - accuracy: 0.6575\n",
            "Epoch 609/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -298426764391659.7500 - accuracy: 0.6575\n",
            "Epoch 610/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -299780245014863.6250 - accuracy: 0.6575\n",
            "Epoch 611/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -301137847846191.5625 - accuracy: 0.6575\n",
            "Epoch 612/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -302499834658393.5625 - accuracy: 0.6575\n",
            "Epoch 613/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -303870161322708.1875 - accuracy: 0.6575\n",
            "Epoch 614/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -305241910865550.3750 - accuracy: 0.6575\n",
            "Epoch 615/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -306617261264722.5000 - accuracy: 0.6575\n",
            "Epoch 616/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -307997137933924.8750 - accuracy: 0.6575\n",
            "Epoch 617/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -309382452218946.0000 - accuracy: 0.6575\n",
            "Epoch 618/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -310764194071191.8125 - accuracy: 0.6575\n",
            "Epoch 619/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -312147846651949.2500 - accuracy: 0.6575\n",
            "Epoch 620/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -313539758116600.0000 - accuracy: 0.6575\n",
            "Epoch 621/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -314930484097529.3750 - accuracy: 0.6575\n",
            "Epoch 622/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -316337586550146.5625 - accuracy: 0.6575\n",
            "Epoch 623/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -317745825264242.1250 - accuracy: 0.6575\n",
            "Epoch 624/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -319158757496722.6250 - accuracy: 0.6575\n",
            "Epoch 625/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -320580538363715.3750 - accuracy: 0.6575\n",
            "Epoch 626/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -322001092746796.3125 - accuracy: 0.6575\n",
            "Epoch 627/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -323430913779308.4375 - accuracy: 0.6575\n",
            "Epoch 628/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -324861085208553.3750 - accuracy: 0.6575\n",
            "Epoch 629/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -326292670575912.0625 - accuracy: 0.6575\n",
            "Epoch 630/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -327742856103336.3125 - accuracy: 0.6575\n",
            "Epoch 631/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -329188959142038.8750 - accuracy: 0.6575\n",
            "Epoch 632/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -330637820478254.6250 - accuracy: 0.6575\n",
            "Epoch 633/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -332085964460069.7500 - accuracy: 0.6575\n",
            "Epoch 634/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -333538568643902.6875 - accuracy: 0.6575\n",
            "Epoch 635/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -334996679282182.6250 - accuracy: 0.6575\n",
            "Epoch 636/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -336462634311725.2500 - accuracy: 0.6575\n",
            "Epoch 637/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -337924638917603.6875 - accuracy: 0.6575\n",
            "Epoch 638/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -339391648875914.1250 - accuracy: 0.6575\n",
            "Epoch 639/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -340866017322513.9375 - accuracy: 0.6575\n",
            "Epoch 640/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -342343213681066.1875 - accuracy: 0.6575\n",
            "Epoch 641/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -343814058545400.9375 - accuracy: 0.6575\n",
            "Epoch 642/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -345298776106906.1250 - accuracy: 0.6575\n",
            "Epoch 643/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -346781533959041.6250 - accuracy: 0.6575\n",
            "Epoch 644/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -348266312436439.9375 - accuracy: 0.6575\n",
            "Epoch 645/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -349760796816197.2500 - accuracy: 0.6575\n",
            "Epoch 646/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -351257456396382.3125 - accuracy: 0.6575\n",
            "Epoch 647/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -352768983139933.3750 - accuracy: 0.6575\n",
            "Epoch 648/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -354288093297119.0000 - accuracy: 0.6575\n",
            "Epoch 649/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -355806486851093.6875 - accuracy: 0.6575\n",
            "Epoch 650/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -357341585001869.8750 - accuracy: 0.6575\n",
            "Epoch 651/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -358875297415047.3125 - accuracy: 0.6575\n",
            "Epoch 652/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -360423077675430.4375 - accuracy: 0.6575\n",
            "Epoch 653/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -361970910739242.8750 - accuracy: 0.6575\n",
            "Epoch 654/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -363510109042322.1875 - accuracy: 0.6575\n",
            "Epoch 655/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -365057043933685.6250 - accuracy: 0.6575\n",
            "Epoch 656/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -366609432046884.2500 - accuracy: 0.6575\n",
            "Epoch 657/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -368166903959227.6875 - accuracy: 0.6575\n",
            "Epoch 658/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -369741088869532.5625 - accuracy: 0.6575\n",
            "Epoch 659/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -371319658322172.6875 - accuracy: 0.6575\n",
            "Epoch 660/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -372912170226425.8750 - accuracy: 0.6575\n",
            "Epoch 661/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -374510191026492.8125 - accuracy: 0.6575\n",
            "Epoch 662/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -376104542351261.9375 - accuracy: 0.6575\n",
            "Epoch 663/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -377701953197759.4375 - accuracy: 0.6575\n",
            "Epoch 664/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -379296607443679.5000 - accuracy: 0.6575\n",
            "Epoch 665/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -380894763531775.0625 - accuracy: 0.6575\n",
            "Epoch 666/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -382491834578819.5000 - accuracy: 0.6575\n",
            "Epoch 667/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -384101824335155.3750 - accuracy: 0.6575\n",
            "Epoch 668/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -385724928197995.9375 - accuracy: 0.6575\n",
            "Epoch 669/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -387325323133440.9375 - accuracy: 0.6575\n",
            "Epoch 670/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -388953136938977.8125 - accuracy: 0.6575\n",
            "Epoch 671/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -390571759302893.6250 - accuracy: 0.6575\n",
            "Epoch 672/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -392200428510854.8750 - accuracy: 0.6575\n",
            "Epoch 673/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -393836219586179.0625 - accuracy: 0.6575\n",
            "Epoch 674/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -395463516757065.5625 - accuracy: 0.6575\n",
            "Epoch 675/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -397107810428615.0000 - accuracy: 0.6575\n",
            "Epoch 676/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -398757340134005.8750 - accuracy: 0.6575\n",
            "Epoch 677/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -400418294842520.7500 - accuracy: 0.6575\n",
            "Epoch 678/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -402068039446128.2500 - accuracy: 0.6575\n",
            "Epoch 679/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -403733791320324.2500 - accuracy: 0.6575\n",
            "Epoch 680/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -405400936843750.5625 - accuracy: 0.6575\n",
            "Epoch 681/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -407076255763159.9375 - accuracy: 0.6575\n",
            "Epoch 682/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -408757782919194.3750 - accuracy: 0.6575\n",
            "Epoch 683/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -410451245455823.8750 - accuracy: 0.6575\n",
            "Epoch 684/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -412143320193910.3125 - accuracy: 0.6575\n",
            "Epoch 685/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -413819860763257.6250 - accuracy: 0.6575\n",
            "Epoch 686/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -415517543928841.4375 - accuracy: 0.6575\n",
            "Epoch 687/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -417203059567214.3125 - accuracy: 0.6575\n",
            "Epoch 688/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -418891573283272.3750 - accuracy: 0.6575\n",
            "Epoch 689/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -420585378965432.3125 - accuracy: 0.6575\n",
            "Epoch 690/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -422292306060335.1250 - accuracy: 0.6575\n",
            "Epoch 691/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -424005193436950.1250 - accuracy: 0.6575\n",
            "Epoch 692/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -425703814757494.8125 - accuracy: 0.6575\n",
            "Epoch 693/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -427423608007368.8750 - accuracy: 0.6575\n",
            "Epoch 694/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -429145259449779.6250 - accuracy: 0.6575\n",
            "Epoch 695/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -430869592297992.5000 - accuracy: 0.6575\n",
            "Epoch 696/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -432623950602436.1250 - accuracy: 0.6575\n",
            "Epoch 697/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -434360248305305.6875 - accuracy: 0.6575\n",
            "Epoch 698/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -436106166542351.0625 - accuracy: 0.6575\n",
            "Epoch 699/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -437860572801284.2500 - accuracy: 0.6575\n",
            "Epoch 700/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -439624461224844.9375 - accuracy: 0.6575\n",
            "Epoch 701/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -441398824675467.5625 - accuracy: 0.6575\n",
            "Epoch 702/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -443173025245353.7500 - accuracy: 0.6575\n",
            "Epoch 703/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -444938873748085.8750 - accuracy: 0.6575\n",
            "Epoch 704/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -446726987626703.4375 - accuracy: 0.6575\n",
            "Epoch 705/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -448500632372135.3750 - accuracy: 0.6575\n",
            "Epoch 706/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -450301308425859.0625 - accuracy: 0.6575\n",
            "Epoch 707/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -452079930492177.4375 - accuracy: 0.6575\n",
            "Epoch 708/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -453864371272792.6250 - accuracy: 0.6575\n",
            "Epoch 709/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -455651893340329.7500 - accuracy: 0.6575\n",
            "Epoch 710/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -457440552883028.3750 - accuracy: 0.6575\n",
            "Epoch 711/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -459240661350094.5000 - accuracy: 0.6575\n",
            "Epoch 712/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -461030741973968.8750 - accuracy: 0.6575\n",
            "Epoch 713/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -462836479919226.5625 - accuracy: 0.6575\n",
            "Epoch 714/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -464643268819573.8750 - accuracy: 0.6575\n",
            "Epoch 715/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -466452542756211.5000 - accuracy: 0.6575\n",
            "Epoch 716/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -468256501687041.4375 - accuracy: 0.6575\n",
            "Epoch 717/1000\n",
            "1086/1086 [==============================] - 0s 140us/step - loss: -470080210830699.9375 - accuracy: 0.6575\n",
            "Epoch 718/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -471911486227457.8750 - accuracy: 0.6575\n",
            "Epoch 719/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -473749826734264.8125 - accuracy: 0.6575\n",
            "Epoch 720/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -475589882836445.1250 - accuracy: 0.6575\n",
            "Epoch 721/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -477424857994098.5625 - accuracy: 0.6575\n",
            "Epoch 722/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -479266898308629.6875 - accuracy: 0.6575\n",
            "Epoch 723/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -481117548253795.0000 - accuracy: 0.6575\n",
            "Epoch 724/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -482983450689867.8750 - accuracy: 0.6575\n",
            "Epoch 725/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -484841687603784.6250 - accuracy: 0.6575\n",
            "Epoch 726/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -486707896702957.1250 - accuracy: 0.6575\n",
            "Epoch 727/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -488580359296974.9375 - accuracy: 0.6575\n",
            "Epoch 728/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -490458414903650.5000 - accuracy: 0.6575\n",
            "Epoch 729/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -492341928178471.1250 - accuracy: 0.6575\n",
            "Epoch 730/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -494239903362310.1250 - accuracy: 0.6575\n",
            "Epoch 731/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -496130955258418.0000 - accuracy: 0.6575\n",
            "Epoch 732/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -498013846095242.1250 - accuracy: 0.6575\n",
            "Epoch 733/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -499921856629343.2500 - accuracy: 0.6575\n",
            "Epoch 734/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -501833254502279.3125 - accuracy: 0.6575\n",
            "Epoch 735/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -503764029506441.1875 - accuracy: 0.6575\n",
            "Epoch 736/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -505701287946771.8125 - accuracy: 0.6575\n",
            "Epoch 737/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -507626859608814.5625 - accuracy: 0.6575\n",
            "Epoch 738/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -509557172455518.3125 - accuracy: 0.6575\n",
            "Epoch 739/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -511510605533070.8125 - accuracy: 0.6575\n",
            "Epoch 740/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -513446514687818.9375 - accuracy: 0.6575\n",
            "Epoch 741/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -515379627359893.9375 - accuracy: 0.6575\n",
            "Epoch 742/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -517327595107744.7500 - accuracy: 0.6575\n",
            "Epoch 743/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -519266430955904.6875 - accuracy: 0.6575\n",
            "Epoch 744/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -521227318828527.9375 - accuracy: 0.6575\n",
            "Epoch 745/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -523183096440911.1875 - accuracy: 0.6575\n",
            "Epoch 746/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -525143424658301.8750 - accuracy: 0.6575\n",
            "Epoch 747/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -527113241029562.1875 - accuracy: 0.6575\n",
            "Epoch 748/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -529092878583298.8125 - accuracy: 0.6575\n",
            "Epoch 749/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -531073480647365.0625 - accuracy: 0.6575\n",
            "Epoch 750/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -533076828371815.2500 - accuracy: 0.6575\n",
            "Epoch 751/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -535084022866840.2500 - accuracy: 0.6575\n",
            "Epoch 752/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -537102384879717.8750 - accuracy: 0.6575\n",
            "Epoch 753/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -539109980994799.5000 - accuracy: 0.6575\n",
            "Epoch 754/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -541127968990466.3750 - accuracy: 0.6575\n",
            "Epoch 755/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -543161382104899.3750 - accuracy: 0.6575\n",
            "Epoch 756/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -545180270441134.4375 - accuracy: 0.6575\n",
            "Epoch 757/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -547199571473613.6250 - accuracy: 0.6575\n",
            "Epoch 758/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -549227957879327.0625 - accuracy: 0.6575\n",
            "Epoch 759/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -551236802520141.3750 - accuracy: 0.6575\n",
            "Epoch 760/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -553261754442135.3125 - accuracy: 0.6575\n",
            "Epoch 761/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -555296909439240.0000 - accuracy: 0.6575\n",
            "Epoch 762/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -557334654618931.3125 - accuracy: 0.6575\n",
            "Epoch 763/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -559376620444474.0000 - accuracy: 0.6575\n",
            "Epoch 764/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -561418499966225.4375 - accuracy: 0.6575\n",
            "Epoch 765/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -563488063143785.1250 - accuracy: 0.6575\n",
            "Epoch 766/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -565560695794889.7500 - accuracy: 0.6575\n",
            "Epoch 767/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -567645807608833.8750 - accuracy: 0.6575\n",
            "Epoch 768/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -569716520550943.0000 - accuracy: 0.6575\n",
            "Epoch 769/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -571804295160879.1250 - accuracy: 0.6575\n",
            "Epoch 770/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -573900563458489.3750 - accuracy: 0.6575\n",
            "Epoch 771/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -575991962211639.2500 - accuracy: 0.6575\n",
            "Epoch 772/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -578095162904915.5000 - accuracy: 0.6575\n",
            "Epoch 773/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -580209359933598.3750 - accuracy: 0.6575\n",
            "Epoch 774/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -582325629289292.8750 - accuracy: 0.6575\n",
            "Epoch 775/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -584456567428588.2500 - accuracy: 0.6575\n",
            "Epoch 776/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -586571783827629.5000 - accuracy: 0.6575\n",
            "Epoch 777/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -588684835821941.5000 - accuracy: 0.6575\n",
            "Epoch 778/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -590820513124057.7500 - accuracy: 0.6575\n",
            "Epoch 779/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -592959968875167.2500 - accuracy: 0.6575\n",
            "Epoch 780/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -595096661102297.7500 - accuracy: 0.6575\n",
            "Epoch 781/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -597232740172681.1250 - accuracy: 0.6575\n",
            "Epoch 782/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -599374618232353.0000 - accuracy: 0.6575\n",
            "Epoch 783/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -601516223622194.8750 - accuracy: 0.6575\n",
            "Epoch 784/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -603678889156019.6250 - accuracy: 0.6575\n",
            "Epoch 785/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -605835834586106.3750 - accuracy: 0.6575\n",
            "Epoch 786/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -608018623058078.3750 - accuracy: 0.6575\n",
            "Epoch 787/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -610191709569024.0000 - accuracy: 0.6575\n",
            "Epoch 788/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -612378237717781.2500 - accuracy: 0.6575\n",
            "Epoch 789/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -614553473656006.0000 - accuracy: 0.6575\n",
            "Epoch 790/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -616754970173093.0000 - accuracy: 0.6575\n",
            "Epoch 791/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -618946900765958.1250 - accuracy: 0.6575\n",
            "Epoch 792/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -621159983641352.8750 - accuracy: 0.6575\n",
            "Epoch 793/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -623375409409195.6250 - accuracy: 0.6575\n",
            "Epoch 794/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -625600007953813.5000 - accuracy: 0.6575\n",
            "Epoch 795/1000\n",
            "1086/1086 [==============================] - 0s 139us/step - loss: -627807478245062.8750 - accuracy: 0.6575\n",
            "Epoch 796/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -630042267892583.2500 - accuracy: 0.6575\n",
            "Epoch 797/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -632252089811034.5000 - accuracy: 0.6575\n",
            "Epoch 798/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -634487598448097.0000 - accuracy: 0.6575\n",
            "Epoch 799/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -636727199797014.1250 - accuracy: 0.6575\n",
            "Epoch 800/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -638965694824870.5000 - accuracy: 0.6575\n",
            "Epoch 801/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -641215532075313.5000 - accuracy: 0.6575\n",
            "Epoch 802/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -643463330552211.6250 - accuracy: 0.6575\n",
            "Epoch 803/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -645726290252387.0000 - accuracy: 0.6575\n",
            "Epoch 804/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -647988532466848.3750 - accuracy: 0.6575\n",
            "Epoch 805/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -650271082723052.6250 - accuracy: 0.6575\n",
            "Epoch 806/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -652561309512426.7500 - accuracy: 0.6575\n",
            "Epoch 807/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -654852633041329.7500 - accuracy: 0.6575\n",
            "Epoch 808/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -657155141113444.8750 - accuracy: 0.6575\n",
            "Epoch 809/1000\n",
            "1086/1086 [==============================] - 0s 136us/step - loss: -659460813612682.5000 - accuracy: 0.6575\n",
            "Epoch 810/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -661785072497839.3750 - accuracy: 0.6575\n",
            "Epoch 811/1000\n",
            "1086/1086 [==============================] - 0s 135us/step - loss: -664108218091222.0000 - accuracy: 0.6575\n",
            "Epoch 812/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -666429076178163.2500 - accuracy: 0.6575\n",
            "Epoch 813/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -668773675591849.7500 - accuracy: 0.6575\n",
            "Epoch 814/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -671099567982927.7500 - accuracy: 0.6575\n",
            "Epoch 815/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -673454570925469.0000 - accuracy: 0.6575\n",
            "Epoch 816/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -675791875034811.6250 - accuracy: 0.6575\n",
            "Epoch 817/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -678148550078547.0000 - accuracy: 0.6575\n",
            "Epoch 818/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -680506023161999.3750 - accuracy: 0.6575\n",
            "Epoch 819/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -682872291589902.6250 - accuracy: 0.6575\n",
            "Epoch 820/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -685227420279010.3750 - accuracy: 0.6575\n",
            "Epoch 821/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -687568143919726.2500 - accuracy: 0.6575\n",
            "Epoch 822/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -689944902327037.6250 - accuracy: 0.6575\n",
            "Epoch 823/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -692297136125193.8750 - accuracy: 0.6575\n",
            "Epoch 824/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -694656680743915.2500 - accuracy: 0.6575\n",
            "Epoch 825/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -697031835901695.5000 - accuracy: 0.6575\n",
            "Epoch 826/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -699428008008113.7500 - accuracy: 0.6575\n",
            "Epoch 827/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -701829777268389.0000 - accuracy: 0.6575\n",
            "Epoch 828/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -704223862326213.5000 - accuracy: 0.6575\n",
            "Epoch 829/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -706620791647403.6250 - accuracy: 0.6575\n",
            "Epoch 830/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -709025068904500.7500 - accuracy: 0.6575\n",
            "Epoch 831/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -711412467071515.2500 - accuracy: 0.6575\n",
            "Epoch 832/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -713826626570364.5000 - accuracy: 0.6575\n",
            "Epoch 833/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -716238441569142.3750 - accuracy: 0.6575\n",
            "Epoch 834/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -718649329497353.8750 - accuracy: 0.6575\n",
            "Epoch 835/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -721093393404665.8750 - accuracy: 0.6575\n",
            "Epoch 836/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -723528202391921.6250 - accuracy: 0.6575\n",
            "Epoch 837/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -725980486316296.0000 - accuracy: 0.6575\n",
            "Epoch 838/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -728440773381125.6250 - accuracy: 0.6575\n",
            "Epoch 839/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -730911892619354.5000 - accuracy: 0.6575\n",
            "Epoch 840/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -733371721273985.1250 - accuracy: 0.6575\n",
            "Epoch 841/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -735847956307892.5000 - accuracy: 0.6575\n",
            "Epoch 842/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -738314110648350.1250 - accuracy: 0.6575\n",
            "Epoch 843/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -740800517866377.1250 - accuracy: 0.6575\n",
            "Epoch 844/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -743268430339055.0000 - accuracy: 0.6575\n",
            "Epoch 845/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -745751738976863.1250 - accuracy: 0.6575\n",
            "Epoch 846/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -748216067182146.8750 - accuracy: 0.6575\n",
            "Epoch 847/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -750674814362912.6250 - accuracy: 0.6575\n",
            "Epoch 848/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -753155510546997.7500 - accuracy: 0.6575\n",
            "Epoch 849/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -755617841648528.7500 - accuracy: 0.6575\n",
            "Epoch 850/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -758099295676917.6250 - accuracy: 0.6575\n",
            "Epoch 851/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -760592196148237.2500 - accuracy: 0.6575\n",
            "Epoch 852/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -763103651560023.6250 - accuracy: 0.6575\n",
            "Epoch 853/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -765612434284830.6250 - accuracy: 0.6575\n",
            "Epoch 854/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -768148882743099.8750 - accuracy: 0.6575\n",
            "Epoch 855/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -770691258992674.0000 - accuracy: 0.6575\n",
            "Epoch 856/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -773256509037460.5000 - accuracy: 0.6575\n",
            "Epoch 857/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -775815339063201.7500 - accuracy: 0.6575\n",
            "Epoch 858/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -778396086381045.6250 - accuracy: 0.6575\n",
            "Epoch 859/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -780980004434847.7500 - accuracy: 0.6575\n",
            "Epoch 860/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -783566740309748.1250 - accuracy: 0.6575\n",
            "Epoch 861/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -786165564446852.0000 - accuracy: 0.6575\n",
            "Epoch 862/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -788742414698680.8750 - accuracy: 0.6575\n",
            "Epoch 863/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -791325745430196.0000 - accuracy: 0.6575\n",
            "Epoch 864/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -793907897363386.2500 - accuracy: 0.6575\n",
            "Epoch 865/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -796502148961802.3750 - accuracy: 0.6575\n",
            "Epoch 866/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -799120298194306.6250 - accuracy: 0.6575\n",
            "Epoch 867/1000\n",
            "1086/1086 [==============================] - 0s 137us/step - loss: -801745788806704.0000 - accuracy: 0.6575\n",
            "Epoch 868/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -804376643833501.3750 - accuracy: 0.6575\n",
            "Epoch 869/1000\n",
            "1086/1086 [==============================] - 0s 148us/step - loss: -807004997865570.1250 - accuracy: 0.6575\n",
            "Epoch 870/1000\n",
            "1086/1086 [==============================] - 0s 138us/step - loss: -809638785827027.2500 - accuracy: 0.6575\n",
            "Epoch 871/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -812261946349066.3750 - accuracy: 0.6575\n",
            "Epoch 872/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -814929862920919.8750 - accuracy: 0.6575\n",
            "Epoch 873/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -817586111994380.2500 - accuracy: 0.6575\n",
            "Epoch 874/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -820249591064051.7500 - accuracy: 0.6575\n",
            "Epoch 875/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -822930985596351.0000 - accuracy: 0.6575\n",
            "Epoch 876/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -825595983006967.1250 - accuracy: 0.6575\n",
            "Epoch 877/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -828266359676188.7500 - accuracy: 0.6575\n",
            "Epoch 878/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -830950250931669.6250 - accuracy: 0.6575\n",
            "Epoch 879/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -833632605893928.1250 - accuracy: 0.6575\n",
            "Epoch 880/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -836345438582750.0000 - accuracy: 0.6575\n",
            "Epoch 881/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -839071425796529.7500 - accuracy: 0.6575\n",
            "Epoch 882/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -841784045713253.3750 - accuracy: 0.6575\n",
            "Epoch 883/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -844494466814382.0000 - accuracy: 0.6575\n",
            "Epoch 884/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -847190243796618.5000 - accuracy: 0.6575\n",
            "Epoch 885/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -849881095934080.2500 - accuracy: 0.6575\n",
            "Epoch 886/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -852599326336831.6250 - accuracy: 0.6575\n",
            "Epoch 887/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -855315632918354.5000 - accuracy: 0.6575\n",
            "Epoch 888/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -858040971753951.0000 - accuracy: 0.6575\n",
            "Epoch 889/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -860768817612140.0000 - accuracy: 0.6575\n",
            "Epoch 890/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -863485313832398.1250 - accuracy: 0.6575\n",
            "Epoch 891/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -866208462209144.7500 - accuracy: 0.6575\n",
            "Epoch 892/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -868955196117995.2500 - accuracy: 0.6575\n",
            "Epoch 893/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -871687926520302.1250 - accuracy: 0.6575\n",
            "Epoch 894/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -874470365960746.3750 - accuracy: 0.6575\n",
            "Epoch 895/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -877228780346773.5000 - accuracy: 0.6575\n",
            "Epoch 896/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -880010816794163.7500 - accuracy: 0.6575\n",
            "Epoch 897/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -882795781287687.0000 - accuracy: 0.6575\n",
            "Epoch 898/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -885571907048306.5000 - accuracy: 0.6575\n",
            "Epoch 899/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -888351159686696.5000 - accuracy: 0.6575\n",
            "Epoch 900/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -891146084983100.8750 - accuracy: 0.6575\n",
            "Epoch 901/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -893927227267445.5000 - accuracy: 0.6575\n",
            "Epoch 902/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -896731634069979.2500 - accuracy: 0.6575\n",
            "Epoch 903/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -899564505068598.7500 - accuracy: 0.6575\n",
            "Epoch 904/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -902379195586179.0000 - accuracy: 0.6575\n",
            "Epoch 905/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -905203453257113.2500 - accuracy: 0.6575\n",
            "Epoch 906/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -908017762046683.6250 - accuracy: 0.6575\n",
            "Epoch 907/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -910874761987918.7500 - accuracy: 0.6575\n",
            "Epoch 908/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -913726190763760.3750 - accuracy: 0.6575\n",
            "Epoch 909/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -916582968095979.7500 - accuracy: 0.6575\n",
            "Epoch 910/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -919460263648601.1250 - accuracy: 0.6575\n",
            "Epoch 911/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -922305519467186.1250 - accuracy: 0.6575\n",
            "Epoch 912/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -925193322039058.3750 - accuracy: 0.6575\n",
            "Epoch 913/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -928063341354811.8750 - accuracy: 0.6575\n",
            "Epoch 914/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -930938787729174.1250 - accuracy: 0.6575\n",
            "Epoch 915/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -933806254590800.6250 - accuracy: 0.6575\n",
            "Epoch 916/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -936696486198632.2500 - accuracy: 0.6575\n",
            "Epoch 917/1000\n",
            "1086/1086 [==============================] - 0s 120us/step - loss: -939585741006746.1250 - accuracy: 0.6575\n",
            "Epoch 918/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -942487722871300.7500 - accuracy: 0.6575\n",
            "Epoch 919/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -945386035202625.0000 - accuracy: 0.6575\n",
            "Epoch 920/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -948276884457445.6250 - accuracy: 0.6575\n",
            "Epoch 921/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -951195148057839.5000 - accuracy: 0.6575\n",
            "Epoch 922/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -954092261206366.7500 - accuracy: 0.6575\n",
            "Epoch 923/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -957009994690763.7500 - accuracy: 0.6575\n",
            "Epoch 924/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -959946115548161.8750 - accuracy: 0.6575\n",
            "Epoch 925/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -962872409605091.7500 - accuracy: 0.6575\n",
            "Epoch 926/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -965818206414776.3750 - accuracy: 0.6575\n",
            "Epoch 927/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -968770387183016.3750 - accuracy: 0.6575\n",
            "Epoch 928/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -971744561595333.5000 - accuracy: 0.6575\n",
            "Epoch 929/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -974687681612564.2500 - accuracy: 0.6575\n",
            "Epoch 930/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -977619176915426.8750 - accuracy: 0.6575\n",
            "Epoch 931/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -980555796232640.8750 - accuracy: 0.6575\n",
            "Epoch 932/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -983515472430685.2500 - accuracy: 0.6575\n",
            "Epoch 933/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -986487552942939.8750 - accuracy: 0.6575\n",
            "Epoch 934/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -989467994244067.7500 - accuracy: 0.6575\n",
            "Epoch 935/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -992481526108801.1250 - accuracy: 0.6575\n",
            "Epoch 936/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -995506369775831.0000 - accuracy: 0.6575\n",
            "Epoch 937/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -998546254174521.1250 - accuracy: 0.6575\n",
            "Epoch 938/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1001557844282162.3750 - accuracy: 0.6575\n",
            "Epoch 939/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1004571444065847.6250 - accuracy: 0.6575\n",
            "Epoch 940/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1007602598565326.1250 - accuracy: 0.6575\n",
            "Epoch 941/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1010625862959104.0000 - accuracy: 0.6575\n",
            "Epoch 942/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1013666913602173.3750 - accuracy: 0.6575\n",
            "Epoch 943/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1016708493747156.6250 - accuracy: 0.6575\n",
            "Epoch 944/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1019740601328838.0000 - accuracy: 0.6575\n",
            "Epoch 945/1000\n",
            "1086/1086 [==============================] - 0s 134us/step - loss: -1022788976188368.8750 - accuracy: 0.6575\n",
            "Epoch 946/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1025841968058858.3750 - accuracy: 0.6575\n",
            "Epoch 947/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1028900939157843.5000 - accuracy: 0.6575\n",
            "Epoch 948/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1031977883677418.7500 - accuracy: 0.6575\n",
            "Epoch 949/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1035069009196750.5000 - accuracy: 0.6575\n",
            "Epoch 950/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1038142214897905.3750 - accuracy: 0.6575\n",
            "Epoch 951/1000\n",
            "1086/1086 [==============================] - 0s 133us/step - loss: -1041227469366832.0000 - accuracy: 0.6575\n",
            "Epoch 952/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1044337366412610.5000 - accuracy: 0.6575\n",
            "Epoch 953/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1047451932089508.1250 - accuracy: 0.6575\n",
            "Epoch 954/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1050591134188908.0000 - accuracy: 0.6575\n",
            "Epoch 955/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1053767339065796.6250 - accuracy: 0.6575\n",
            "Epoch 956/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1056912008909712.6250 - accuracy: 0.6575\n",
            "Epoch 957/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1060035006265400.5000 - accuracy: 0.6575\n",
            "Epoch 958/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1063189899819939.6250 - accuracy: 0.6575\n",
            "Epoch 959/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1066342444297749.7500 - accuracy: 0.6575\n",
            "Epoch 960/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1069493985464056.0000 - accuracy: 0.6575\n",
            "Epoch 961/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -1072656134619950.6250 - accuracy: 0.6575\n",
            "Epoch 962/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1075817028319469.7500 - accuracy: 0.6575\n",
            "Epoch 963/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1078995190180975.3750 - accuracy: 0.6575\n",
            "Epoch 964/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1082177021224254.6250 - accuracy: 0.6575\n",
            "Epoch 965/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1085362191478082.6250 - accuracy: 0.6575\n",
            "Epoch 966/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1088550980914181.6250 - accuracy: 0.6575\n",
            "Epoch 967/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1091750058529982.3750 - accuracy: 0.6575\n",
            "Epoch 968/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1094956331252221.1250 - accuracy: 0.6575\n",
            "Epoch 969/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1098196227728340.6250 - accuracy: 0.6575\n",
            "Epoch 970/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1101444036266530.8750 - accuracy: 0.6575\n",
            "Epoch 971/1000\n",
            "1086/1086 [==============================] - 0s 131us/step - loss: -1104684957931350.2500 - accuracy: 0.6575\n",
            "Epoch 972/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1107935668561003.6250 - accuracy: 0.6575\n",
            "Epoch 973/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1111185946725472.1250 - accuracy: 0.6575\n",
            "Epoch 974/1000\n",
            "1086/1086 [==============================] - 0s 129us/step - loss: -1114446394240684.5000 - accuracy: 0.6575\n",
            "Epoch 975/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1117714471696953.3750 - accuracy: 0.6575\n",
            "Epoch 976/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1120964138358161.6250 - accuracy: 0.6575\n",
            "Epoch 977/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1124233853780903.3750 - accuracy: 0.6575\n",
            "Epoch 978/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1127495383494135.5000 - accuracy: 0.6575\n",
            "Epoch 979/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1130763960178789.7500 - accuracy: 0.6575\n",
            "Epoch 980/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1134035283300269.0000 - accuracy: 0.6575\n",
            "Epoch 981/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1137326784048058.2500 - accuracy: 0.6575\n",
            "Epoch 982/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1140650138914552.0000 - accuracy: 0.6575\n",
            "Epoch 983/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1143929636670801.5000 - accuracy: 0.6575\n",
            "Epoch 984/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1147225635599143.2500 - accuracy: 0.6575\n",
            "Epoch 985/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1150528816504515.2500 - accuracy: 0.6575\n",
            "Epoch 986/1000\n",
            "1086/1086 [==============================] - 0s 132us/step - loss: -1153835757537766.5000 - accuracy: 0.6575\n",
            "Epoch 987/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1157143507832236.0000 - accuracy: 0.6575\n",
            "Epoch 988/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1160474190986871.7500 - accuracy: 0.6575\n",
            "Epoch 989/1000\n",
            "1086/1086 [==============================] - 0s 125us/step - loss: -1163783234025890.7500 - accuracy: 0.6575\n",
            "Epoch 990/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1167113975915265.5000 - accuracy: 0.6575\n",
            "Epoch 991/1000\n",
            "1086/1086 [==============================] - 0s 127us/step - loss: -1170434121625532.0000 - accuracy: 0.6575\n",
            "Epoch 992/1000\n",
            "1086/1086 [==============================] - 0s 126us/step - loss: -1173802747963290.2500 - accuracy: 0.6575\n",
            "Epoch 993/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1177133087921704.5000 - accuracy: 0.6575\n",
            "Epoch 994/1000\n",
            "1086/1086 [==============================] - 0s 122us/step - loss: -1180485406427019.0000 - accuracy: 0.6575\n",
            "Epoch 995/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1183845786353724.2500 - accuracy: 0.6575\n",
            "Epoch 996/1000\n",
            "1086/1086 [==============================] - 0s 123us/step - loss: -1187234315776264.0000 - accuracy: 0.6575\n",
            "Epoch 997/1000\n",
            "1086/1086 [==============================] - 0s 121us/step - loss: -1190627476981069.7500 - accuracy: 0.6575\n",
            "Epoch 998/1000\n",
            "1086/1086 [==============================] - 0s 124us/step - loss: -1194035297625949.7500 - accuracy: 0.6575\n",
            "Epoch 999/1000\n",
            "1086/1086 [==============================] - 0s 128us/step - loss: -1197433710288609.2500 - accuracy: 0.6575\n",
            "Epoch 1000/1000\n",
            "1086/1086 [==============================] - 0s 130us/step - loss: -1200843402388678.0000 - accuracy: 0.6575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f162ef9e160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHUL8pr4vAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de74ffe-b8e3-412a-f01a-49b2439cd015"
      },
      "source": [
        "ANNPred = model.predict(X_test)\n",
        "cm = confusion_matrix(Y_test, ANNPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[173   0]\n",
            " [ 99   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni4o6y-a462j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c345aa-4b88-44e3-8e11-e6b24a280b9f"
      },
      "source": [
        "print(\"Accuracy score %f\" % accuracy_score(Y_test, ANNPred))\n",
        "print(classification_report(Y_test, ANNPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score 0.636029\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      1.00      0.78       173\n",
            "           2       0.00      0.00      0.00        99\n",
            "\n",
            "    accuracy                           0.64       272\n",
            "   macro avg       0.32      0.50      0.39       272\n",
            "weighted avg       0.40      0.64      0.49       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZqDxRweOg5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acdf234-b742-4c20-8c62-c135c7f10ba0"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_test, Y_test, batch_size = 10, nb_epoch = 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "272/272 [==============================] - 0s 445us/step - loss: -1476907715658691.7500 - accuracy: 0.6360\n",
            "Epoch 2/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1477966037053681.0000 - accuracy: 0.6360\n",
            "Epoch 3/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1478899448506729.2500 - accuracy: 0.6360\n",
            "Epoch 4/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1479889646579953.0000 - accuracy: 0.6360\n",
            "Epoch 5/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1480863534500683.2500 - accuracy: 0.6360\n",
            "Epoch 6/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1481882356216049.0000 - accuracy: 0.6360\n",
            "Epoch 7/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1482833773847009.7500 - accuracy: 0.6360\n",
            "Epoch 8/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1483855381490025.2500 - accuracy: 0.6360\n",
            "Epoch 9/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1484933764450183.5000 - accuracy: 0.6360\n",
            "Epoch 10/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1485986769496425.2500 - accuracy: 0.6360\n",
            "Epoch 11/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1486993416057073.0000 - accuracy: 0.6360\n",
            "Epoch 12/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1488013217435407.0000 - accuracy: 0.6360\n",
            "Epoch 13/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1489008115164581.5000 - accuracy: 0.6360\n",
            "Epoch 14/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1490049698519642.5000 - accuracy: 0.6360\n",
            "Epoch 15/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1491098769694238.2500 - accuracy: 0.6360\n",
            "Epoch 16/1000\n",
            "272/272 [==============================] - 0s 161us/step - loss: -1492075824761675.2500 - accuracy: 0.6360\n",
            "Epoch 17/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1493168881617739.2500 - accuracy: 0.6360\n",
            "Epoch 18/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1494202641362221.0000 - accuracy: 0.6360\n",
            "Epoch 19/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1495270134798938.5000 - accuracy: 0.6360\n",
            "Epoch 20/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1496344656915757.0000 - accuracy: 0.6360\n",
            "Epoch 21/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1497345841445948.2500 - accuracy: 0.6360\n",
            "Epoch 22/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1498399150656331.2500 - accuracy: 0.6360\n",
            "Epoch 23/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1499501511758547.0000 - accuracy: 0.6360\n",
            "Epoch 24/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1500479780059256.5000 - accuracy: 0.6360\n",
            "Epoch 25/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1501625844575533.0000 - accuracy: 0.6360\n",
            "Epoch 26/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1502648066529882.5000 - accuracy: 0.6360\n",
            "Epoch 27/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1503713866500939.2500 - accuracy: 0.6360\n",
            "Epoch 28/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1504728055653797.5000 - accuracy: 0.6360\n",
            "Epoch 29/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1505806705646170.5000 - accuracy: 0.6360\n",
            "Epoch 30/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1506896751531670.7500 - accuracy: 0.6360\n",
            "Epoch 31/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1507931725033713.0000 - accuracy: 0.6360\n",
            "Epoch 32/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1509007135327171.7500 - accuracy: 0.6360\n",
            "Epoch 33/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -1510039457597319.5000 - accuracy: 0.6360\n",
            "Epoch 34/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1510998133440512.0000 - accuracy: 0.6360\n",
            "Epoch 35/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1512094167681867.2500 - accuracy: 0.6360\n",
            "Epoch 36/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1513105950815412.7500 - accuracy: 0.6360\n",
            "Epoch 37/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1514128230057863.5000 - accuracy: 0.6360\n",
            "Epoch 38/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1515166402840455.5000 - accuracy: 0.6360\n",
            "Epoch 39/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1516250511287717.5000 - accuracy: 0.6360\n",
            "Epoch 40/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1517317775040030.2500 - accuracy: 0.6360\n",
            "Epoch 41/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1518437986236536.5000 - accuracy: 0.6360\n",
            "Epoch 42/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1519547780447292.2500 - accuracy: 0.6360\n",
            "Epoch 43/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1520554785266266.5000 - accuracy: 0.6360\n",
            "Epoch 44/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1521698673139230.2500 - accuracy: 0.6360\n",
            "Epoch 45/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1522779529736433.0000 - accuracy: 0.6360\n",
            "Epoch 46/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1523803485112199.5000 - accuracy: 0.6360\n",
            "Epoch 47/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1524863435754315.2500 - accuracy: 0.6360\n",
            "Epoch 48/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1525902235448500.7500 - accuracy: 0.6360\n",
            "Epoch 49/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1526912478779030.7500 - accuracy: 0.6360\n",
            "Epoch 50/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1527990122230964.7500 - accuracy: 0.6360\n",
            "Epoch 51/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1528999202197263.0000 - accuracy: 0.6360\n",
            "Epoch 52/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1530053020337091.7500 - accuracy: 0.6360\n",
            "Epoch 53/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1531111088170405.5000 - accuracy: 0.6360\n",
            "Epoch 54/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1532197760416828.2500 - accuracy: 0.6360\n",
            "Epoch 55/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1533228758751834.5000 - accuracy: 0.6360\n",
            "Epoch 56/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1534278745379539.0000 - accuracy: 0.6360\n",
            "Epoch 57/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1535349725778643.0000 - accuracy: 0.6360\n",
            "Epoch 58/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1536402087723971.7500 - accuracy: 0.6360\n",
            "Epoch 59/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1537446446505502.2500 - accuracy: 0.6360\n",
            "Epoch 60/1000\n",
            "272/272 [==============================] - 0s 208us/step - loss: -1538537398484028.2500 - accuracy: 0.6360\n",
            "Epoch 61/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1539607367654942.2500 - accuracy: 0.6360\n",
            "Epoch 62/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1540642541866767.0000 - accuracy: 0.6360\n",
            "Epoch 63/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1541709923152113.0000 - accuracy: 0.6360\n",
            "Epoch 64/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1542814118090390.7500 - accuracy: 0.6360\n",
            "Epoch 65/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1543874002735827.0000 - accuracy: 0.6360\n",
            "Epoch 66/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1544964702285101.0000 - accuracy: 0.6360\n",
            "Epoch 67/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1546005130386974.2500 - accuracy: 0.6360\n",
            "Epoch 68/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -1547042520795015.5000 - accuracy: 0.6360\n",
            "Epoch 69/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1548174569674390.7500 - accuracy: 0.6360\n",
            "Epoch 70/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1549342491510663.5000 - accuracy: 0.6360\n",
            "Epoch 71/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1550429971654053.5000 - accuracy: 0.6360\n",
            "Epoch 72/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1551544117146804.7500 - accuracy: 0.6360\n",
            "Epoch 73/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1552614512162936.5000 - accuracy: 0.6360\n",
            "Epoch 74/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1553697251385825.7500 - accuracy: 0.6360\n",
            "Epoch 75/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1554703573782287.0000 - accuracy: 0.6360\n",
            "Epoch 76/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1555784993588163.7500 - accuracy: 0.6360\n",
            "Epoch 77/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1556864092155181.0000 - accuracy: 0.6360\n",
            "Epoch 78/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1557997965125029.5000 - accuracy: 0.6360\n",
            "Epoch 79/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1559072778100254.2500 - accuracy: 0.6360\n",
            "Epoch 80/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1560161757337238.7500 - accuracy: 0.6360\n",
            "Epoch 81/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1561286772553848.5000 - accuracy: 0.6360\n",
            "Epoch 82/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1562263312523745.7500 - accuracy: 0.6360\n",
            "Epoch 83/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1563437635569543.5000 - accuracy: 0.6360\n",
            "Epoch 84/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1564455806450748.2500 - accuracy: 0.6360\n",
            "Epoch 85/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1565523849572593.0000 - accuracy: 0.6360\n",
            "Epoch 86/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1566590735498300.2500 - accuracy: 0.6360\n",
            "Epoch 87/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1567645984821007.0000 - accuracy: 0.6360\n",
            "Epoch 88/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1568694707128199.5000 - accuracy: 0.6360\n",
            "Epoch 89/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1569827017164679.5000 - accuracy: 0.6360\n",
            "Epoch 90/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1571046672163297.7500 - accuracy: 0.6360\n",
            "Epoch 91/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1572251851816960.0000 - accuracy: 0.6360\n",
            "Epoch 92/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1573341017708182.7500 - accuracy: 0.6360\n",
            "Epoch 93/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1574408542825773.0000 - accuracy: 0.6360\n",
            "Epoch 94/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1575517845344979.0000 - accuracy: 0.6360\n",
            "Epoch 95/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1576548794136696.5000 - accuracy: 0.6360\n",
            "Epoch 96/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1577636612168282.5000 - accuracy: 0.6360\n",
            "Epoch 97/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1578680907649747.0000 - accuracy: 0.6360\n",
            "Epoch 98/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1579777689583616.0000 - accuracy: 0.6360\n",
            "Epoch 99/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1580848332792289.7500 - accuracy: 0.6360\n",
            "Epoch 100/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1581905248579825.0000 - accuracy: 0.6360\n",
            "Epoch 101/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1583184074907166.2500 - accuracy: 0.6360\n",
            "Epoch 102/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1584348254036449.7500 - accuracy: 0.6360\n",
            "Epoch 103/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1585468733560470.7500 - accuracy: 0.6360\n",
            "Epoch 104/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1586523863946902.7500 - accuracy: 0.6360\n",
            "Epoch 105/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1587654368875219.0000 - accuracy: 0.6360\n",
            "Epoch 106/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1588672829417833.2500 - accuracy: 0.6360\n",
            "Epoch 107/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1589718145718874.5000 - accuracy: 0.6360\n",
            "Epoch 108/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1590729345582019.7500 - accuracy: 0.6360\n",
            "Epoch 109/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1591787665461007.0000 - accuracy: 0.6360\n",
            "Epoch 110/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1592853996117534.2500 - accuracy: 0.6360\n",
            "Epoch 111/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1593896716159819.2500 - accuracy: 0.6360\n",
            "Epoch 112/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1594910423646208.0000 - accuracy: 0.6360\n",
            "Epoch 113/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1596012023821492.7500 - accuracy: 0.6360\n",
            "Epoch 114/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1597191209615360.0000 - accuracy: 0.6360\n",
            "Epoch 115/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1598296733469515.2500 - accuracy: 0.6360\n",
            "Epoch 116/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1599354430291968.0000 - accuracy: 0.6360\n",
            "Epoch 117/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1600390818569758.2500 - accuracy: 0.6360\n",
            "Epoch 118/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1601460709405876.7500 - accuracy: 0.6360\n",
            "Epoch 119/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1602599684746541.0000 - accuracy: 0.6360\n",
            "Epoch 120/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1603593135409995.2500 - accuracy: 0.6360\n",
            "Epoch 121/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1604631033668065.7500 - accuracy: 0.6360\n",
            "Epoch 122/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1605667581699493.5000 - accuracy: 0.6360\n",
            "Epoch 123/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1606763086286607.0000 - accuracy: 0.6360\n",
            "Epoch 124/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1607830214556732.2500 - accuracy: 0.6360\n",
            "Epoch 125/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1608930359154326.7500 - accuracy: 0.6360\n",
            "Epoch 126/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1610021793107727.0000 - accuracy: 0.6360\n",
            "Epoch 127/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1611090734149873.0000 - accuracy: 0.6360\n",
            "Epoch 128/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1612171323791962.5000 - accuracy: 0.6360\n",
            "Epoch 129/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1613143229919473.0000 - accuracy: 0.6360\n",
            "Epoch 130/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1614249868841683.0000 - accuracy: 0.6360\n",
            "Epoch 131/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1615293789318445.0000 - accuracy: 0.6360\n",
            "Epoch 132/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1616443767151194.5000 - accuracy: 0.6360\n",
            "Epoch 133/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1617489477824753.0000 - accuracy: 0.6360\n",
            "Epoch 134/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1618563984739147.2500 - accuracy: 0.6360\n",
            "Epoch 135/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1619682941684555.2500 - accuracy: 0.6360\n",
            "Epoch 136/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1620762316539783.5000 - accuracy: 0.6360\n",
            "Epoch 137/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1621828377987915.2500 - accuracy: 0.6360\n",
            "Epoch 138/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -1622914759161976.5000 - accuracy: 0.6360\n",
            "Epoch 139/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1623948276376997.5000 - accuracy: 0.6360\n",
            "Epoch 140/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1625017069693289.2500 - accuracy: 0.6360\n",
            "Epoch 141/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1626036163221142.7500 - accuracy: 0.6360\n",
            "Epoch 142/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1627211063700781.0000 - accuracy: 0.6360\n",
            "Epoch 143/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1628379907990949.5000 - accuracy: 0.6360\n",
            "Epoch 144/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1629567192985841.0000 - accuracy: 0.6360\n",
            "Epoch 145/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1630761588682993.0000 - accuracy: 0.6360\n",
            "Epoch 146/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1631855649797300.7500 - accuracy: 0.6360\n",
            "Epoch 147/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1632895447859200.0000 - accuracy: 0.6360\n",
            "Epoch 148/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1633953465607589.5000 - accuracy: 0.6360\n",
            "Epoch 149/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1634976260947968.0000 - accuracy: 0.6360\n",
            "Epoch 150/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1636075031664278.7500 - accuracy: 0.6360\n",
            "Epoch 151/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1637222013067745.7500 - accuracy: 0.6360\n",
            "Epoch 152/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1638293086574230.7500 - accuracy: 0.6360\n",
            "Epoch 153/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1639362389220894.2500 - accuracy: 0.6360\n",
            "Epoch 154/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1640460731994835.0000 - accuracy: 0.6360\n",
            "Epoch 155/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1641679841487209.2500 - accuracy: 0.6360\n",
            "Epoch 156/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1642791507876442.5000 - accuracy: 0.6360\n",
            "Epoch 157/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1643844818466936.5000 - accuracy: 0.6360\n",
            "Epoch 158/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1644915615673645.0000 - accuracy: 0.6360\n",
            "Epoch 159/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1645981537557082.5000 - accuracy: 0.6360\n",
            "Epoch 160/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1647066553731915.2500 - accuracy: 0.6360\n",
            "Epoch 161/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1648130918819237.5000 - accuracy: 0.6360\n",
            "Epoch 162/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1649114171618484.7500 - accuracy: 0.6360\n",
            "Epoch 163/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1650194674598851.7500 - accuracy: 0.6360\n",
            "Epoch 164/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1651231921351981.0000 - accuracy: 0.6360\n",
            "Epoch 165/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1652267557615736.5000 - accuracy: 0.6360\n",
            "Epoch 166/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1653343613392534.7500 - accuracy: 0.6360\n",
            "Epoch 167/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1654394846306785.7500 - accuracy: 0.6360\n",
            "Epoch 168/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1655501821575168.0000 - accuracy: 0.6360\n",
            "Epoch 169/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1656586680648643.7500 - accuracy: 0.6360\n",
            "Epoch 170/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1657797920146371.7500 - accuracy: 0.6360\n",
            "Epoch 171/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1658918983742644.7500 - accuracy: 0.6360\n",
            "Epoch 172/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1659990088629308.2500 - accuracy: 0.6360\n",
            "Epoch 173/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1661014169200037.5000 - accuracy: 0.6360\n",
            "Epoch 174/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1662054047447160.5000 - accuracy: 0.6360\n",
            "Epoch 175/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1663144823635004.2500 - accuracy: 0.6360\n",
            "Epoch 176/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1664197831387497.2500 - accuracy: 0.6360\n",
            "Epoch 177/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1665203867669203.0000 - accuracy: 0.6360\n",
            "Epoch 178/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1666351811171990.7500 - accuracy: 0.6360\n",
            "Epoch 179/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1667454483454554.5000 - accuracy: 0.6360\n",
            "Epoch 180/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1668498174848301.0000 - accuracy: 0.6360\n",
            "Epoch 181/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1669572925155689.2500 - accuracy: 0.6360\n",
            "Epoch 182/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1670662568172122.5000 - accuracy: 0.6360\n",
            "Epoch 183/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1671711216955633.0000 - accuracy: 0.6360\n",
            "Epoch 184/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1672752293231676.2500 - accuracy: 0.6360\n",
            "Epoch 185/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1673909316446569.2500 - accuracy: 0.6360\n",
            "Epoch 186/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1675023699688508.2500 - accuracy: 0.6360\n",
            "Epoch 187/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1676088487907087.0000 - accuracy: 0.6360\n",
            "Epoch 188/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1677144686314435.7500 - accuracy: 0.6360\n",
            "Epoch 189/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1678229528363971.7500 - accuracy: 0.6360\n",
            "Epoch 190/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -1679290026056282.5000 - accuracy: 0.6360\n",
            "Epoch 191/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1680376716806987.2500 - accuracy: 0.6360\n",
            "Epoch 192/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1681472861765632.0000 - accuracy: 0.6360\n",
            "Epoch 193/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1682584132186353.0000 - accuracy: 0.6360\n",
            "Epoch 194/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1683772403305291.2500 - accuracy: 0.6360\n",
            "Epoch 195/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1684783278233238.7500 - accuracy: 0.6360\n",
            "Epoch 196/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1685949448963011.7500 - accuracy: 0.6360\n",
            "Epoch 197/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1687020081296564.7500 - accuracy: 0.6360\n",
            "Epoch 198/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1688113209791187.0000 - accuracy: 0.6360\n",
            "Epoch 199/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1689227526992113.0000 - accuracy: 0.6360\n",
            "Epoch 200/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1690311461992568.5000 - accuracy: 0.6360\n",
            "Epoch 201/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1691405468688865.7500 - accuracy: 0.6360\n",
            "Epoch 202/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1692501261572698.5000 - accuracy: 0.6360\n",
            "Epoch 203/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1693674034491633.0000 - accuracy: 0.6360\n",
            "Epoch 204/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1694704234197233.0000 - accuracy: 0.6360\n",
            "Epoch 205/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1695856480624158.2500 - accuracy: 0.6360\n",
            "Epoch 206/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1696948247592960.0000 - accuracy: 0.6360\n",
            "Epoch 207/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1698083765651335.5000 - accuracy: 0.6360\n",
            "Epoch 208/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1699198967557421.0000 - accuracy: 0.6360\n",
            "Epoch 209/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1700360370643425.7500 - accuracy: 0.6360\n",
            "Epoch 210/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -1701498436683655.5000 - accuracy: 0.6360\n",
            "Epoch 211/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1702636686240105.2500 - accuracy: 0.6360\n",
            "Epoch 212/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1703808184630332.2500 - accuracy: 0.6360\n",
            "Epoch 213/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1705017687100235.2500 - accuracy: 0.6360\n",
            "Epoch 214/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1706275379561411.7500 - accuracy: 0.6360\n",
            "Epoch 215/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1707357140033054.2500 - accuracy: 0.6360\n",
            "Epoch 216/1000\n",
            "272/272 [==============================] - 0s 173us/step - loss: -1708478158879804.2500 - accuracy: 0.6360\n",
            "Epoch 217/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1709527705290631.5000 - accuracy: 0.6360\n",
            "Epoch 218/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1710665689680715.2500 - accuracy: 0.6360\n",
            "Epoch 219/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1711785467106003.0000 - accuracy: 0.6360\n",
            "Epoch 220/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1712848991112011.2500 - accuracy: 0.6360\n",
            "Epoch 221/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1713990291793317.5000 - accuracy: 0.6360\n",
            "Epoch 222/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1715140935471827.0000 - accuracy: 0.6360\n",
            "Epoch 223/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1716251834758324.7500 - accuracy: 0.6360\n",
            "Epoch 224/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1717371387017336.5000 - accuracy: 0.6360\n",
            "Epoch 225/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1718492262394217.2500 - accuracy: 0.6360\n",
            "Epoch 226/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1719589869006125.0000 - accuracy: 0.6360\n",
            "Epoch 227/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1720746321394748.2500 - accuracy: 0.6360\n",
            "Epoch 228/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1721844551570131.0000 - accuracy: 0.6360\n",
            "Epoch 229/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1723053228317274.5000 - accuracy: 0.6360\n",
            "Epoch 230/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1724196609126641.0000 - accuracy: 0.6360\n",
            "Epoch 231/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1725360754331407.0000 - accuracy: 0.6360\n",
            "Epoch 232/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1726476915680677.5000 - accuracy: 0.6360\n",
            "Epoch 233/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1727588931638934.7500 - accuracy: 0.6360\n",
            "Epoch 234/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1728795486839265.7500 - accuracy: 0.6360\n",
            "Epoch 235/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1729947370273852.2500 - accuracy: 0.6360\n",
            "Epoch 236/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1731112720446644.7500 - accuracy: 0.6360\n",
            "Epoch 237/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1732215688859407.0000 - accuracy: 0.6360\n",
            "Epoch 238/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1733385598795776.0000 - accuracy: 0.6360\n",
            "Epoch 239/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1734523242938368.0000 - accuracy: 0.6360\n",
            "Epoch 240/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1735636033015567.0000 - accuracy: 0.6360\n",
            "Epoch 241/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1736741027577856.0000 - accuracy: 0.6360\n",
            "Epoch 242/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1737815179688056.5000 - accuracy: 0.6360\n",
            "Epoch 243/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1739005098181571.7500 - accuracy: 0.6360\n",
            "Epoch 244/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1740068650745856.0000 - accuracy: 0.6360\n",
            "Epoch 245/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1741243369143356.2500 - accuracy: 0.6360\n",
            "Epoch 246/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1742407859915595.2500 - accuracy: 0.6360\n",
            "Epoch 247/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1743583559101741.0000 - accuracy: 0.6360\n",
            "Epoch 248/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1744709462493063.5000 - accuracy: 0.6360\n",
            "Epoch 249/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1745796698965895.5000 - accuracy: 0.6360\n",
            "Epoch 250/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1747047279647322.5000 - accuracy: 0.6360\n",
            "Epoch 251/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1748240008910486.7500 - accuracy: 0.6360\n",
            "Epoch 252/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1749414912751736.5000 - accuracy: 0.6360\n",
            "Epoch 253/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1750566776587203.7500 - accuracy: 0.6360\n",
            "Epoch 254/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1751703616672467.0000 - accuracy: 0.6360\n",
            "Epoch 255/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1752891054235166.2500 - accuracy: 0.6360\n",
            "Epoch 256/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1754000636756931.7500 - accuracy: 0.6360\n",
            "Epoch 257/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1755182072887416.5000 - accuracy: 0.6360\n",
            "Epoch 258/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1756298715405854.2500 - accuracy: 0.6360\n",
            "Epoch 259/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1757431415824865.7500 - accuracy: 0.6360\n",
            "Epoch 260/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1758593865698123.2500 - accuracy: 0.6360\n",
            "Epoch 261/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1759700430876913.0000 - accuracy: 0.6360\n",
            "Epoch 262/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1760917104407732.7500 - accuracy: 0.6360\n",
            "Epoch 263/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -1762082221889174.7500 - accuracy: 0.6360\n",
            "Epoch 264/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1763317133175145.2500 - accuracy: 0.6360\n",
            "Epoch 265/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1764486620986428.2500 - accuracy: 0.6360\n",
            "Epoch 266/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1765641724578635.2500 - accuracy: 0.6360\n",
            "Epoch 267/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1766789467926769.0000 - accuracy: 0.6360\n",
            "Epoch 268/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1767898174644705.7500 - accuracy: 0.6360\n",
            "Epoch 269/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -1769052386561807.0000 - accuracy: 0.6360\n",
            "Epoch 270/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1770234484682993.0000 - accuracy: 0.6360\n",
            "Epoch 271/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1771371236117323.2500 - accuracy: 0.6360\n",
            "Epoch 272/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1772559758030968.5000 - accuracy: 0.6360\n",
            "Epoch 273/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1773714887610127.0000 - accuracy: 0.6360\n",
            "Epoch 274/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1774874373714161.0000 - accuracy: 0.6360\n",
            "Epoch 275/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1776085556033656.5000 - accuracy: 0.6360\n",
            "Epoch 276/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1777252523974174.2500 - accuracy: 0.6360\n",
            "Epoch 277/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1778397241193652.7500 - accuracy: 0.6360\n",
            "Epoch 278/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1779543263828570.5000 - accuracy: 0.6360\n",
            "Epoch 279/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1780738065232113.0000 - accuracy: 0.6360\n",
            "Epoch 280/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1781914621906703.0000 - accuracy: 0.6360\n",
            "Epoch 281/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1783115925961788.2500 - accuracy: 0.6360\n",
            "Epoch 282/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1784472616767247.0000 - accuracy: 0.6360\n",
            "Epoch 283/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1785699671998464.0000 - accuracy: 0.6360\n",
            "Epoch 284/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1786938162999537.0000 - accuracy: 0.6360\n",
            "Epoch 285/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1788073921143265.7500 - accuracy: 0.6360\n",
            "Epoch 286/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1789225693224478.2500 - accuracy: 0.6360\n",
            "Epoch 287/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1790559765628807.5000 - accuracy: 0.6360\n",
            "Epoch 288/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1791791073884280.5000 - accuracy: 0.6360\n",
            "Epoch 289/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1792956061064734.2500 - accuracy: 0.6360\n",
            "Epoch 290/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1794197006843904.0000 - accuracy: 0.6360\n",
            "Epoch 291/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1795336648338733.0000 - accuracy: 0.6360\n",
            "Epoch 292/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1796540662345969.0000 - accuracy: 0.6360\n",
            "Epoch 293/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1797683572899840.0000 - accuracy: 0.6360\n",
            "Epoch 294/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1798815605958053.5000 - accuracy: 0.6360\n",
            "Epoch 295/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1800031658234579.0000 - accuracy: 0.6360\n",
            "Epoch 296/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1801144624896602.5000 - accuracy: 0.6360\n",
            "Epoch 297/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1802294875066127.0000 - accuracy: 0.6360\n",
            "Epoch 298/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1803642193452574.2500 - accuracy: 0.6360\n",
            "Epoch 299/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1804768451524728.5000 - accuracy: 0.6360\n",
            "Epoch 300/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1805947414881701.5000 - accuracy: 0.6360\n",
            "Epoch 301/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1807093679622023.5000 - accuracy: 0.6360\n",
            "Epoch 302/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1808175373777016.5000 - accuracy: 0.6360\n",
            "Epoch 303/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1809322733654739.0000 - accuracy: 0.6360\n",
            "Epoch 304/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1810424155849667.7500 - accuracy: 0.6360\n",
            "Epoch 305/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1811637722214641.0000 - accuracy: 0.6360\n",
            "Epoch 306/1000\n",
            "272/272 [==============================] - 0s 173us/step - loss: -1812767999046836.7500 - accuracy: 0.6360\n",
            "Epoch 307/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1813882392558652.2500 - accuracy: 0.6360\n",
            "Epoch 308/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1815096076256195.7500 - accuracy: 0.6360\n",
            "Epoch 309/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1816199221354014.2500 - accuracy: 0.6360\n",
            "Epoch 310/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1817386044235294.2500 - accuracy: 0.6360\n",
            "Epoch 311/1000\n",
            "272/272 [==============================] - 0s 152us/step - loss: -1818513272841637.5000 - accuracy: 0.6360\n",
            "Epoch 312/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1819607881944847.0000 - accuracy: 0.6360\n",
            "Epoch 313/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1820775049423209.2500 - accuracy: 0.6360\n",
            "Epoch 314/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1821909351503510.7500 - accuracy: 0.6360\n",
            "Epoch 315/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1823034999015303.5000 - accuracy: 0.6360\n",
            "Epoch 316/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1824140032382614.7500 - accuracy: 0.6360\n",
            "Epoch 317/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1825330017306503.5000 - accuracy: 0.6360\n",
            "Epoch 318/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1826466807784869.5000 - accuracy: 0.6360\n",
            "Epoch 319/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1827620826983725.0000 - accuracy: 0.6360\n",
            "Epoch 320/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1828768022913505.7500 - accuracy: 0.6360\n",
            "Epoch 321/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -1829953430679793.0000 - accuracy: 0.6360\n",
            "Epoch 322/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1831043947252555.2500 - accuracy: 0.6360\n",
            "Epoch 323/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1832244802856357.5000 - accuracy: 0.6360\n",
            "Epoch 324/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1833393741149364.7500 - accuracy: 0.6360\n",
            "Epoch 325/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1834588632637921.7500 - accuracy: 0.6360\n",
            "Epoch 326/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1835766682978183.5000 - accuracy: 0.6360\n",
            "Epoch 327/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1836944263864801.7500 - accuracy: 0.6360\n",
            "Epoch 328/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1838052118734245.5000 - accuracy: 0.6360\n",
            "Epoch 329/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1839307185038757.5000 - accuracy: 0.6360\n",
            "Epoch 330/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1840488038284348.2500 - accuracy: 0.6360\n",
            "Epoch 331/1000\n",
            "272/272 [==============================] - 0s 178us/step - loss: -1841634314659237.5000 - accuracy: 0.6360\n",
            "Epoch 332/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1842857633397820.2500 - accuracy: 0.6360\n",
            "Epoch 333/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1844064939070163.0000 - accuracy: 0.6360\n",
            "Epoch 334/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1845405906401400.5000 - accuracy: 0.6360\n",
            "Epoch 335/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1846771603934991.0000 - accuracy: 0.6360\n",
            "Epoch 336/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1847964254632056.5000 - accuracy: 0.6360\n",
            "Epoch 337/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1849216211893308.2500 - accuracy: 0.6360\n",
            "Epoch 338/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1850367550993588.7500 - accuracy: 0.6360\n",
            "Epoch 339/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1851550299077692.2500 - accuracy: 0.6360\n",
            "Epoch 340/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1852747491542919.5000 - accuracy: 0.6360\n",
            "Epoch 341/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1853881110325609.2500 - accuracy: 0.6360\n",
            "Epoch 342/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1855083297898496.0000 - accuracy: 0.6360\n",
            "Epoch 343/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1856214038262964.7500 - accuracy: 0.6360\n",
            "Epoch 344/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1857409687918230.7500 - accuracy: 0.6360\n",
            "Epoch 345/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1858646547150004.7500 - accuracy: 0.6360\n",
            "Epoch 346/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1859839712651625.2500 - accuracy: 0.6360\n",
            "Epoch 347/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -1860991334373978.5000 - accuracy: 0.6360\n",
            "Epoch 348/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1862188551048975.0000 - accuracy: 0.6360\n",
            "Epoch 349/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -1863408657397880.5000 - accuracy: 0.6360\n",
            "Epoch 350/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1864611580701033.2500 - accuracy: 0.6360\n",
            "Epoch 351/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1865891181555953.0000 - accuracy: 0.6360\n",
            "Epoch 352/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1867072940894569.2500 - accuracy: 0.6360\n",
            "Epoch 353/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1868230966825683.0000 - accuracy: 0.6360\n",
            "Epoch 354/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1869405360287985.0000 - accuracy: 0.6360\n",
            "Epoch 355/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1870664830334132.7500 - accuracy: 0.6360\n",
            "Epoch 356/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1871867301945825.7500 - accuracy: 0.6360\n",
            "Epoch 357/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1873025632829440.0000 - accuracy: 0.6360\n",
            "Epoch 358/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1874133826913219.7500 - accuracy: 0.6360\n",
            "Epoch 359/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1875392876295348.7500 - accuracy: 0.6360\n",
            "Epoch 360/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1876587136880399.0000 - accuracy: 0.6360\n",
            "Epoch 361/1000\n",
            "272/272 [==============================] - 0s 168us/step - loss: -1877736801404807.5000 - accuracy: 0.6360\n",
            "Epoch 362/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1879016045127077.5000 - accuracy: 0.6360\n",
            "Epoch 363/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1880253720089299.0000 - accuracy: 0.6360\n",
            "Epoch 364/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1881407469495717.5000 - accuracy: 0.6360\n",
            "Epoch 365/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1882570904043520.0000 - accuracy: 0.6360\n",
            "Epoch 366/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1883770982991751.5000 - accuracy: 0.6360\n",
            "Epoch 367/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1884966543321449.2500 - accuracy: 0.6360\n",
            "Epoch 368/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1886093069748103.5000 - accuracy: 0.6360\n",
            "Epoch 369/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1887365806003862.7500 - accuracy: 0.6360\n",
            "Epoch 370/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1888547729756883.0000 - accuracy: 0.6360\n",
            "Epoch 371/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1889833507156209.0000 - accuracy: 0.6360\n",
            "Epoch 372/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -1890964283064320.0000 - accuracy: 0.6360\n",
            "Epoch 373/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1892298086693948.2500 - accuracy: 0.6360\n",
            "Epoch 374/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -1893442253318505.2500 - accuracy: 0.6360\n",
            "Epoch 375/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1894701180819094.7500 - accuracy: 0.6360\n",
            "Epoch 376/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1895877231598170.5000 - accuracy: 0.6360\n",
            "Epoch 377/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1897099128957289.2500 - accuracy: 0.6360\n",
            "Epoch 378/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1898352115634899.0000 - accuracy: 0.6360\n",
            "Epoch 379/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1899569746079985.0000 - accuracy: 0.6360\n",
            "Epoch 380/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1900771589719943.5000 - accuracy: 0.6360\n",
            "Epoch 381/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -1901953273807811.7500 - accuracy: 0.6360\n",
            "Epoch 382/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -1903258663957684.7500 - accuracy: 0.6360\n",
            "Epoch 383/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1904448609128207.0000 - accuracy: 0.6360\n",
            "Epoch 384/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1905670879217543.5000 - accuracy: 0.6360\n",
            "Epoch 385/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1906923782614317.0000 - accuracy: 0.6360\n",
            "Epoch 386/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1908216929837537.7500 - accuracy: 0.6360\n",
            "Epoch 387/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1909422243215480.5000 - accuracy: 0.6360\n",
            "Epoch 388/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1910730095668525.0000 - accuracy: 0.6360\n",
            "Epoch 389/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1911921321242865.0000 - accuracy: 0.6360\n",
            "Epoch 390/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1913143097325688.5000 - accuracy: 0.6360\n",
            "Epoch 391/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1914375125767830.7500 - accuracy: 0.6360\n",
            "Epoch 392/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1915675865914910.2500 - accuracy: 0.6360\n",
            "Epoch 393/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1917101226605628.2500 - accuracy: 0.6360\n",
            "Epoch 394/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1918338887381233.0000 - accuracy: 0.6360\n",
            "Epoch 395/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1919550865878317.0000 - accuracy: 0.6360\n",
            "Epoch 396/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -1920780638956845.0000 - accuracy: 0.6360\n",
            "Epoch 397/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1922021390564412.2500 - accuracy: 0.6360\n",
            "Epoch 398/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1923212541011365.5000 - accuracy: 0.6360\n",
            "Epoch 399/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -1924433862833332.7500 - accuracy: 0.6360\n",
            "Epoch 400/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1925659100554661.5000 - accuracy: 0.6360\n",
            "Epoch 401/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1926940561460043.2500 - accuracy: 0.6360\n",
            "Epoch 402/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1928188690416579.7500 - accuracy: 0.6360\n",
            "Epoch 403/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1929394686417498.5000 - accuracy: 0.6360\n",
            "Epoch 404/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1930695616171791.0000 - accuracy: 0.6360\n",
            "Epoch 405/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1931951535997891.7500 - accuracy: 0.6360\n",
            "Epoch 406/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1933165291881411.7500 - accuracy: 0.6360\n",
            "Epoch 407/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -1934382584072071.5000 - accuracy: 0.6360\n",
            "Epoch 408/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1935687032843565.0000 - accuracy: 0.6360\n",
            "Epoch 409/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1936874077976696.5000 - accuracy: 0.6360\n",
            "Epoch 410/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1938099651215360.0000 - accuracy: 0.6360\n",
            "Epoch 411/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1939269085715034.5000 - accuracy: 0.6360\n",
            "Epoch 412/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -1940602922621530.5000 - accuracy: 0.6360\n",
            "Epoch 413/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1941786484277248.0000 - accuracy: 0.6360\n",
            "Epoch 414/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1943124454300250.5000 - accuracy: 0.6360\n",
            "Epoch 415/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1944318816782215.5000 - accuracy: 0.6360\n",
            "Epoch 416/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1945562729445496.5000 - accuracy: 0.6360\n",
            "Epoch 417/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1946736274085526.7500 - accuracy: 0.6360\n",
            "Epoch 418/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1947996881821214.2500 - accuracy: 0.6360\n",
            "Epoch 419/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1949209193904248.5000 - accuracy: 0.6360\n",
            "Epoch 420/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1950624741280587.2500 - accuracy: 0.6360\n",
            "Epoch 421/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1951919524035644.2500 - accuracy: 0.6360\n",
            "Epoch 422/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1953119369336470.7500 - accuracy: 0.6360\n",
            "Epoch 423/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1954412962389534.2500 - accuracy: 0.6360\n",
            "Epoch 424/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1955582838339704.5000 - accuracy: 0.6360\n",
            "Epoch 425/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1956809318114725.5000 - accuracy: 0.6360\n",
            "Epoch 426/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -1958076310309707.2500 - accuracy: 0.6360\n",
            "Epoch 427/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1959276311462851.7500 - accuracy: 0.6360\n",
            "Epoch 428/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1960523408574343.5000 - accuracy: 0.6360\n",
            "Epoch 429/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1961783115351943.5000 - accuracy: 0.6360\n",
            "Epoch 430/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1962955452988476.2500 - accuracy: 0.6360\n",
            "Epoch 431/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1964159058575360.0000 - accuracy: 0.6360\n",
            "Epoch 432/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1965426179803015.5000 - accuracy: 0.6360\n",
            "Epoch 433/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1966736495021839.0000 - accuracy: 0.6360\n",
            "Epoch 434/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1968053698344116.7500 - accuracy: 0.6360\n",
            "Epoch 435/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -1969314440305242.5000 - accuracy: 0.6360\n",
            "Epoch 436/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1970581197769547.2500 - accuracy: 0.6360\n",
            "Epoch 437/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1971811211958874.5000 - accuracy: 0.6360\n",
            "Epoch 438/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -1973072273485101.0000 - accuracy: 0.6360\n",
            "Epoch 439/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -1974300601818654.2500 - accuracy: 0.6360\n",
            "Epoch 440/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1975459904610785.7500 - accuracy: 0.6360\n",
            "Epoch 441/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -1976754673025024.0000 - accuracy: 0.6360\n",
            "Epoch 442/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -1977991023481555.0000 - accuracy: 0.6360\n",
            "Epoch 443/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1979260259286076.2500 - accuracy: 0.6360\n",
            "Epoch 444/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1980579310222396.2500 - accuracy: 0.6360\n",
            "Epoch 445/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -1981894195752237.0000 - accuracy: 0.6360\n",
            "Epoch 446/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -1983113662223059.0000 - accuracy: 0.6360\n",
            "Epoch 447/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -1984349615947776.0000 - accuracy: 0.6360\n",
            "Epoch 448/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -1985618151025965.0000 - accuracy: 0.6360\n",
            "Epoch 449/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1986996899306074.5000 - accuracy: 0.6360\n",
            "Epoch 450/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1988372340099915.2500 - accuracy: 0.6360\n",
            "Epoch 451/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -1989596024745261.0000 - accuracy: 0.6360\n",
            "Epoch 452/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -1990802450415616.0000 - accuracy: 0.6360\n",
            "Epoch 453/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -1992000322369656.5000 - accuracy: 0.6360\n",
            "Epoch 454/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1993355685728015.0000 - accuracy: 0.6360\n",
            "Epoch 455/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -1994645177183533.0000 - accuracy: 0.6360\n",
            "Epoch 456/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -1995893342531825.0000 - accuracy: 0.6360\n",
            "Epoch 457/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -1997221451684442.5000 - accuracy: 0.6360\n",
            "Epoch 458/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -1998476030123550.2500 - accuracy: 0.6360\n",
            "Epoch 459/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -1999751828348446.2500 - accuracy: 0.6360\n",
            "Epoch 460/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2001080677795719.5000 - accuracy: 0.6360\n",
            "Epoch 461/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2002347867273336.5000 - accuracy: 0.6360\n",
            "Epoch 462/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2003529005608237.0000 - accuracy: 0.6360\n",
            "Epoch 463/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2004768516842917.5000 - accuracy: 0.6360\n",
            "Epoch 464/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2006024323380404.7500 - accuracy: 0.6360\n",
            "Epoch 465/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2007229158422648.5000 - accuracy: 0.6360\n",
            "Epoch 466/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2008481133864357.5000 - accuracy: 0.6360\n",
            "Epoch 467/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2009721829280587.2500 - accuracy: 0.6360\n",
            "Epoch 468/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2010985469976094.2500 - accuracy: 0.6360\n",
            "Epoch 469/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2012231256136282.5000 - accuracy: 0.6360\n",
            "Epoch 470/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2013432675411365.5000 - accuracy: 0.6360\n",
            "Epoch 471/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -2014751022629827.7500 - accuracy: 0.6360\n",
            "Epoch 472/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2015936926804329.2500 - accuracy: 0.6360\n",
            "Epoch 473/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2017238159041957.5000 - accuracy: 0.6360\n",
            "Epoch 474/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2018465381551887.0000 - accuracy: 0.6360\n",
            "Epoch 475/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2019716576606328.5000 - accuracy: 0.6360\n",
            "Epoch 476/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2020954100511924.7500 - accuracy: 0.6360\n",
            "Epoch 477/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2022163687245703.5000 - accuracy: 0.6360\n",
            "Epoch 478/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2023451577949726.2500 - accuracy: 0.6360\n",
            "Epoch 479/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2024678577390531.7500 - accuracy: 0.6360\n",
            "Epoch 480/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2026012948022211.7500 - accuracy: 0.6360\n",
            "Epoch 481/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2027282062222757.5000 - accuracy: 0.6360\n",
            "Epoch 482/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2028544327680000.0000 - accuracy: 0.6360\n",
            "Epoch 483/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2029826278948864.0000 - accuracy: 0.6360\n",
            "Epoch 484/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2031017710907633.0000 - accuracy: 0.6360\n",
            "Epoch 485/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2032277830330729.2500 - accuracy: 0.6360\n",
            "Epoch 486/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2033610412523520.0000 - accuracy: 0.6360\n",
            "Epoch 487/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2034836225315539.0000 - accuracy: 0.6360\n",
            "Epoch 488/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2036092127427644.2500 - accuracy: 0.6360\n",
            "Epoch 489/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2037359399249317.5000 - accuracy: 0.6360\n",
            "Epoch 490/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2038680392826880.0000 - accuracy: 0.6360\n",
            "Epoch 491/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2039926431354639.0000 - accuracy: 0.6360\n",
            "Epoch 492/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2041261807447341.0000 - accuracy: 0.6360\n",
            "Epoch 493/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2042475748631973.5000 - accuracy: 0.6360\n",
            "Epoch 494/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2043793348325617.0000 - accuracy: 0.6360\n",
            "Epoch 495/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2045062166378616.5000 - accuracy: 0.6360\n",
            "Epoch 496/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2046438053279864.5000 - accuracy: 0.6360\n",
            "Epoch 497/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2047673449994842.5000 - accuracy: 0.6360\n",
            "Epoch 498/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2049045397349797.5000 - accuracy: 0.6360\n",
            "Epoch 499/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2050278404529694.2500 - accuracy: 0.6360\n",
            "Epoch 500/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2051618084965195.2500 - accuracy: 0.6360\n",
            "Epoch 501/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2053038486631604.7500 - accuracy: 0.6360\n",
            "Epoch 502/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2054407446123219.0000 - accuracy: 0.6360\n",
            "Epoch 503/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2055678324205929.2500 - accuracy: 0.6360\n",
            "Epoch 504/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2056942676175209.2500 - accuracy: 0.6360\n",
            "Epoch 505/1000\n",
            "272/272 [==============================] - 0s 158us/step - loss: -2058095746536147.0000 - accuracy: 0.6360\n",
            "Epoch 506/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2059485028255623.5000 - accuracy: 0.6360\n",
            "Epoch 507/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2060651673419776.0000 - accuracy: 0.6360\n",
            "Epoch 508/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2062011772590682.5000 - accuracy: 0.6360\n",
            "Epoch 509/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2063322675042906.5000 - accuracy: 0.6360\n",
            "Epoch 510/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2064634807181793.7500 - accuracy: 0.6360\n",
            "Epoch 511/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2065932509665882.5000 - accuracy: 0.6360\n",
            "Epoch 512/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2067192386790821.5000 - accuracy: 0.6360\n",
            "Epoch 513/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2068436479932536.5000 - accuracy: 0.6360\n",
            "Epoch 514/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2069696602162115.7500 - accuracy: 0.6360\n",
            "Epoch 515/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2070997498516179.0000 - accuracy: 0.6360\n",
            "Epoch 516/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2072273479347501.0000 - accuracy: 0.6360\n",
            "Epoch 517/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2073509392277985.7500 - accuracy: 0.6360\n",
            "Epoch 518/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2074753476329472.0000 - accuracy: 0.6360\n",
            "Epoch 519/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2076123758206494.2500 - accuracy: 0.6360\n",
            "Epoch 520/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2077319238593837.0000 - accuracy: 0.6360\n",
            "Epoch 521/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2078643356619475.0000 - accuracy: 0.6360\n",
            "Epoch 522/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2079985497507719.5000 - accuracy: 0.6360\n",
            "Epoch 523/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2081561054682654.2500 - accuracy: 0.6360\n",
            "Epoch 524/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2082808940307877.5000 - accuracy: 0.6360\n",
            "Epoch 525/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2084052153601807.0000 - accuracy: 0.6360\n",
            "Epoch 526/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2085361896822422.7500 - accuracy: 0.6360\n",
            "Epoch 527/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2086584962144858.5000 - accuracy: 0.6360\n",
            "Epoch 528/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2087921958919710.2500 - accuracy: 0.6360\n",
            "Epoch 529/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2089158215281965.0000 - accuracy: 0.6360\n",
            "Epoch 530/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2090433345748992.0000 - accuracy: 0.6360\n",
            "Epoch 531/1000\n",
            "272/272 [==============================] - 0s 167us/step - loss: -2091610434730104.5000 - accuracy: 0.6360\n",
            "Epoch 532/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2093015110963923.0000 - accuracy: 0.6360\n",
            "Epoch 533/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2094257535481976.5000 - accuracy: 0.6360\n",
            "Epoch 534/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2095625093112289.7500 - accuracy: 0.6360\n",
            "Epoch 535/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2096947258812777.2500 - accuracy: 0.6360\n",
            "Epoch 536/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2098245821437590.7500 - accuracy: 0.6360\n",
            "Epoch 537/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2099542225784591.0000 - accuracy: 0.6360\n",
            "Epoch 538/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2100861815874017.7500 - accuracy: 0.6360\n",
            "Epoch 539/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2102183614449543.5000 - accuracy: 0.6360\n",
            "Epoch 540/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2103452995959868.2500 - accuracy: 0.6360\n",
            "Epoch 541/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2104715135048282.2500 - accuracy: 0.6360\n",
            "Epoch 542/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2105940076597007.0000 - accuracy: 0.6360\n",
            "Epoch 543/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2107301532255171.7500 - accuracy: 0.6360\n",
            "Epoch 544/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2108574016116374.5000 - accuracy: 0.6360\n",
            "Epoch 545/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2109844458585148.2500 - accuracy: 0.6360\n",
            "Epoch 546/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2111181663147670.5000 - accuracy: 0.6360\n",
            "Epoch 547/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2112441277843335.5000 - accuracy: 0.6360\n",
            "Epoch 548/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2113827945146729.5000 - accuracy: 0.6360\n",
            "Epoch 549/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2115367478863269.7500 - accuracy: 0.6360\n",
            "Epoch 550/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2116764947309026.0000 - accuracy: 0.6360\n",
            "Epoch 551/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2118148827443441.0000 - accuracy: 0.6360\n",
            "Epoch 552/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2119452621691482.2500 - accuracy: 0.6360\n",
            "Epoch 553/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2120691332122503.5000 - accuracy: 0.6360\n",
            "Epoch 554/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2121924281091011.7500 - accuracy: 0.6360\n",
            "Epoch 555/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2123211556782080.0000 - accuracy: 0.6360\n",
            "Epoch 556/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2124450550359461.7500 - accuracy: 0.6360\n",
            "Epoch 557/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2125729546186029.2500 - accuracy: 0.6360\n",
            "Epoch 558/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2126978676470964.7500 - accuracy: 0.6360\n",
            "Epoch 559/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2128298100515298.0000 - accuracy: 0.6360\n",
            "Epoch 560/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2129531778614211.7500 - accuracy: 0.6360\n",
            "Epoch 561/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2130832371405522.7500 - accuracy: 0.6360\n",
            "Epoch 562/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2132139704381681.0000 - accuracy: 0.6360\n",
            "Epoch 563/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2133386033735077.7500 - accuracy: 0.6360\n",
            "Epoch 564/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2134700185662644.7500 - accuracy: 0.6360\n",
            "Epoch 565/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2135969040032226.0000 - accuracy: 0.6360\n",
            "Epoch 566/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2137304376124837.7500 - accuracy: 0.6360\n",
            "Epoch 567/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2138563931444886.5000 - accuracy: 0.6360\n",
            "Epoch 568/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2139861684599868.2500 - accuracy: 0.6360\n",
            "Epoch 569/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2141243906648425.5000 - accuracy: 0.6360\n",
            "Epoch 570/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2142662600564254.0000 - accuracy: 0.6360\n",
            "Epoch 571/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2144072833664903.5000 - accuracy: 0.6360\n",
            "Epoch 572/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2145355997858635.2500 - accuracy: 0.6360\n",
            "Epoch 573/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2146547670681479.5000 - accuracy: 0.6360\n",
            "Epoch 574/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2147837130371313.0000 - accuracy: 0.6360\n",
            "Epoch 575/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2149126046467011.7500 - accuracy: 0.6360\n",
            "Epoch 576/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2150338756253334.5000 - accuracy: 0.6360\n",
            "Epoch 577/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2151630339247887.0000 - accuracy: 0.6360\n",
            "Epoch 578/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2152948646620461.2500 - accuracy: 0.6360\n",
            "Epoch 579/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2154176930412664.5000 - accuracy: 0.6360\n",
            "Epoch 580/1000\n",
            "272/272 [==============================] - 0s 122us/step - loss: -2155454707608877.2500 - accuracy: 0.6360\n",
            "Epoch 581/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2156843698749440.0000 - accuracy: 0.6360\n",
            "Epoch 582/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2158097445482737.0000 - accuracy: 0.6360\n",
            "Epoch 583/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2159343125952632.5000 - accuracy: 0.6360\n",
            "Epoch 584/1000\n",
            "272/272 [==============================] - 0s 157us/step - loss: -2160631434059294.0000 - accuracy: 0.6360\n",
            "Epoch 585/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2161983325470720.0000 - accuracy: 0.6360\n",
            "Epoch 586/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2163245799748788.7500 - accuracy: 0.6360\n",
            "Epoch 587/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2164538571335077.7500 - accuracy: 0.6360\n",
            "Epoch 588/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2165812943743337.5000 - accuracy: 0.6360\n",
            "Epoch 589/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2167104983608621.2500 - accuracy: 0.6360\n",
            "Epoch 590/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2168432734641694.0000 - accuracy: 0.6360\n",
            "Epoch 591/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2169752595942219.2500 - accuracy: 0.6360\n",
            "Epoch 592/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2171093981886584.5000 - accuracy: 0.6360\n",
            "Epoch 593/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2172445934763068.2500 - accuracy: 0.6360\n",
            "Epoch 594/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2173805234672338.7500 - accuracy: 0.6360\n",
            "Epoch 595/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2175088294440237.2500 - accuracy: 0.6360\n",
            "Epoch 596/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2176428682757059.7500 - accuracy: 0.6360\n",
            "Epoch 597/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2177790318091806.0000 - accuracy: 0.6360\n",
            "Epoch 598/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2179111668863698.7500 - accuracy: 0.6360\n",
            "Epoch 599/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2180389984781251.7500 - accuracy: 0.6360\n",
            "Epoch 600/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2181751668658898.7500 - accuracy: 0.6360\n",
            "Epoch 601/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2183029481414174.0000 - accuracy: 0.6360\n",
            "Epoch 602/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2184422966658951.5000 - accuracy: 0.6360\n",
            "Epoch 603/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2185692065593464.5000 - accuracy: 0.6360\n",
            "Epoch 604/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2187034809659633.0000 - accuracy: 0.6360\n",
            "Epoch 605/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2188384294249893.7500 - accuracy: 0.6360\n",
            "Epoch 606/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2189771622626484.7500 - accuracy: 0.6360\n",
            "Epoch 607/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2191063074980803.7500 - accuracy: 0.6360\n",
            "Epoch 608/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2192478859273637.7500 - accuracy: 0.6360\n",
            "Epoch 609/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2193871521232052.7500 - accuracy: 0.6360\n",
            "Epoch 610/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2195383162826993.0000 - accuracy: 0.6360\n",
            "Epoch 611/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2196835508775273.5000 - accuracy: 0.6360\n",
            "Epoch 612/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2198195790922691.7500 - accuracy: 0.6360\n",
            "Epoch 613/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2199476815373733.7500 - accuracy: 0.6360\n",
            "Epoch 614/1000\n",
            "272/272 [==============================] - 0s 190us/step - loss: -2200817119896997.7500 - accuracy: 0.6360\n",
            "Epoch 615/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2202147780304414.0000 - accuracy: 0.6360\n",
            "Epoch 616/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2203507275106183.5000 - accuracy: 0.6360\n",
            "Epoch 617/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2204749297167661.2500 - accuracy: 0.6360\n",
            "Epoch 618/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2206048674119680.0000 - accuracy: 0.6360\n",
            "Epoch 619/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2207420170371072.0000 - accuracy: 0.6360\n",
            "Epoch 620/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2208720044764461.2500 - accuracy: 0.6360\n",
            "Epoch 621/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2210052667915324.2500 - accuracy: 0.6360\n",
            "Epoch 622/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2211395449049810.7500 - accuracy: 0.6360\n",
            "Epoch 623/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2212727475250597.7500 - accuracy: 0.6360\n",
            "Epoch 624/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2214100335560583.5000 - accuracy: 0.6360\n",
            "Epoch 625/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2215336983319250.7500 - accuracy: 0.6360\n",
            "Epoch 626/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2216778118787313.0000 - accuracy: 0.6360\n",
            "Epoch 627/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2218075486960700.2500 - accuracy: 0.6360\n",
            "Epoch 628/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2219378428438889.5000 - accuracy: 0.6360\n",
            "Epoch 629/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2220789826846720.0000 - accuracy: 0.6360\n",
            "Epoch 630/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2222099140767985.0000 - accuracy: 0.6360\n",
            "Epoch 631/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2223487506957251.7500 - accuracy: 0.6360\n",
            "Epoch 632/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2224775415494776.5000 - accuracy: 0.6360\n",
            "Epoch 633/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2226128264194409.5000 - accuracy: 0.6360\n",
            "Epoch 634/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2227417764717026.0000 - accuracy: 0.6360\n",
            "Epoch 635/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2228765974886520.5000 - accuracy: 0.6360\n",
            "Epoch 636/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2230122666632613.7500 - accuracy: 0.6360\n",
            "Epoch 637/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2231505912116404.7500 - accuracy: 0.6360\n",
            "Epoch 638/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2232872072179953.0000 - accuracy: 0.6360\n",
            "Epoch 639/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2234182952858804.7500 - accuracy: 0.6360\n",
            "Epoch 640/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2235578003966795.2500 - accuracy: 0.6360\n",
            "Epoch 641/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2236862856213684.7500 - accuracy: 0.6360\n",
            "Epoch 642/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2238260373757470.0000 - accuracy: 0.6360\n",
            "Epoch 643/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2239604455559890.7500 - accuracy: 0.6360\n",
            "Epoch 644/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2240982766830531.7500 - accuracy: 0.6360\n",
            "Epoch 645/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2242279706814825.5000 - accuracy: 0.6360\n",
            "Epoch 646/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2243672343884980.7500 - accuracy: 0.6360\n",
            "Epoch 647/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2245041952714993.0000 - accuracy: 0.6360\n",
            "Epoch 648/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2246400997727774.0000 - accuracy: 0.6360\n",
            "Epoch 649/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2247699954555482.2500 - accuracy: 0.6360\n",
            "Epoch 650/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2249188306167205.7500 - accuracy: 0.6360\n",
            "Epoch 651/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2250560400184018.7500 - accuracy: 0.6360\n",
            "Epoch 652/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2251885613316216.5000 - accuracy: 0.6360\n",
            "Epoch 653/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2253264254522066.5000 - accuracy: 0.6360\n",
            "Epoch 654/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2254618838649675.0000 - accuracy: 0.6360\n",
            "Epoch 655/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2256030691029233.0000 - accuracy: 0.6360\n",
            "Epoch 656/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2257394307370767.0000 - accuracy: 0.6360\n",
            "Epoch 657/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2258734798293835.0000 - accuracy: 0.6360\n",
            "Epoch 658/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2260119188899719.5000 - accuracy: 0.6360\n",
            "Epoch 659/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2261518465583947.0000 - accuracy: 0.6360\n",
            "Epoch 660/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2263052297020838.0000 - accuracy: 0.6360\n",
            "Epoch 661/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2264392220355885.5000 - accuracy: 0.6360\n",
            "Epoch 662/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2265856119545374.0000 - accuracy: 0.6360\n",
            "Epoch 663/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2267198109932122.0000 - accuracy: 0.6360\n",
            "Epoch 664/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2268631121781218.0000 - accuracy: 0.6360\n",
            "Epoch 665/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2269959024109929.5000 - accuracy: 0.6360\n",
            "Epoch 666/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2271392139822019.5000 - accuracy: 0.6360\n",
            "Epoch 667/1000\n",
            "272/272 [==============================] - 0s 124us/step - loss: -2272692379860028.5000 - accuracy: 0.6360\n",
            "Epoch 668/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2274071225071736.5000 - accuracy: 0.6360\n",
            "Epoch 669/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2275511905735378.5000 - accuracy: 0.6360\n",
            "Epoch 670/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2276858926308894.0000 - accuracy: 0.6360\n",
            "Epoch 671/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2278209980603934.0000 - accuracy: 0.6360\n",
            "Epoch 672/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2279702303943981.5000 - accuracy: 0.6360\n",
            "Epoch 673/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2281074287089302.5000 - accuracy: 0.6360\n",
            "Epoch 674/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2282669233853138.5000 - accuracy: 0.6360\n",
            "Epoch 675/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2284051104707765.0000 - accuracy: 0.6360\n",
            "Epoch 676/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2285425619416245.0000 - accuracy: 0.6360\n",
            "Epoch 677/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2286774073348818.5000 - accuracy: 0.6360\n",
            "Epoch 678/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2288098455677289.5000 - accuracy: 0.6360\n",
            "Epoch 679/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2289483454827339.0000 - accuracy: 0.6360\n",
            "Epoch 680/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2290891373628235.0000 - accuracy: 0.6360\n",
            "Epoch 681/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2292254007392376.5000 - accuracy: 0.6360\n",
            "Epoch 682/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2293619845604774.0000 - accuracy: 0.6360\n",
            "Epoch 683/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2295002065411614.0000 - accuracy: 0.6360\n",
            "Epoch 684/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2296394737609065.5000 - accuracy: 0.6360\n",
            "Epoch 685/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2297760708003719.5000 - accuracy: 0.6360\n",
            "Epoch 686/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2299110207181522.5000 - accuracy: 0.6360\n",
            "Epoch 687/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2300546172398893.5000 - accuracy: 0.6360\n",
            "Epoch 688/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2301979327124178.5000 - accuracy: 0.6360\n",
            "Epoch 689/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2303310927973195.0000 - accuracy: 0.6360\n",
            "Epoch 690/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2304728595631887.0000 - accuracy: 0.6360\n",
            "Epoch 691/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2306091797909263.0000 - accuracy: 0.6360\n",
            "Epoch 692/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2307515314802447.0000 - accuracy: 0.6360\n",
            "Epoch 693/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2308867071718581.0000 - accuracy: 0.6360\n",
            "Epoch 694/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2310191612443708.5000 - accuracy: 0.6360\n",
            "Epoch 695/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2311658371963482.0000 - accuracy: 0.6360\n",
            "Epoch 696/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2312981418282767.0000 - accuracy: 0.6360\n",
            "Epoch 697/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2314390362837714.5000 - accuracy: 0.6360\n",
            "Epoch 698/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2315772050584757.0000 - accuracy: 0.6360\n",
            "Epoch 699/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2317123339250025.5000 - accuracy: 0.6360\n",
            "Epoch 700/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2318582465259881.5000 - accuracy: 0.6360\n",
            "Epoch 701/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2320028741217581.5000 - accuracy: 0.6360\n",
            "Epoch 702/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2321434598749365.0000 - accuracy: 0.6360\n",
            "Epoch 703/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2322831594302765.5000 - accuracy: 0.6360\n",
            "Epoch 704/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2324195030350908.5000 - accuracy: 0.6360\n",
            "Epoch 705/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2325683827237346.0000 - accuracy: 0.6360\n",
            "Epoch 706/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2327110639712376.5000 - accuracy: 0.6360\n",
            "Epoch 707/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2328469238926155.0000 - accuracy: 0.6360\n",
            "Epoch 708/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2329897454430569.5000 - accuracy: 0.6360\n",
            "Epoch 709/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2331249315213553.0000 - accuracy: 0.6360\n",
            "Epoch 710/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2332673097859072.0000 - accuracy: 0.6360\n",
            "Epoch 711/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2334006558989854.0000 - accuracy: 0.6360\n",
            "Epoch 712/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2335393667643512.5000 - accuracy: 0.6360\n",
            "Epoch 713/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2336856874371915.0000 - accuracy: 0.6360\n",
            "Epoch 714/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2338274872116163.5000 - accuracy: 0.6360\n",
            "Epoch 715/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2339675134708555.0000 - accuracy: 0.6360\n",
            "Epoch 716/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2341040299735160.5000 - accuracy: 0.6360\n",
            "Epoch 717/1000\n",
            "272/272 [==============================] - 0s 173us/step - loss: -2342550125439337.5000 - accuracy: 0.6360\n",
            "Epoch 718/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2344180172856862.0000 - accuracy: 0.6360\n",
            "Epoch 719/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2345626416617110.5000 - accuracy: 0.6360\n",
            "Epoch 720/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2346970445867369.5000 - accuracy: 0.6360\n",
            "Epoch 721/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2348325097192508.5000 - accuracy: 0.6360\n",
            "Epoch 722/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2349735872314428.5000 - accuracy: 0.6360\n",
            "Epoch 723/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2351180544197571.5000 - accuracy: 0.6360\n",
            "Epoch 724/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2352586234034296.5000 - accuracy: 0.6360\n",
            "Epoch 725/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2353913800117067.0000 - accuracy: 0.6360\n",
            "Epoch 726/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2355345889612498.5000 - accuracy: 0.6360\n",
            "Epoch 727/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2356710754684687.0000 - accuracy: 0.6360\n",
            "Epoch 728/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2358037816865009.0000 - accuracy: 0.6360\n",
            "Epoch 729/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2359572663816914.5000 - accuracy: 0.6360\n",
            "Epoch 730/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2360976088667798.5000 - accuracy: 0.6360\n",
            "Epoch 731/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2362420868122503.5000 - accuracy: 0.6360\n",
            "Epoch 732/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2363880581519962.0000 - accuracy: 0.6360\n",
            "Epoch 733/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2365272201810642.5000 - accuracy: 0.6360\n",
            "Epoch 734/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2366809386439378.5000 - accuracy: 0.6360\n",
            "Epoch 735/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2368205759647503.0000 - accuracy: 0.6360\n",
            "Epoch 736/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2369596907153769.5000 - accuracy: 0.6360\n",
            "Epoch 737/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2370977438198121.5000 - accuracy: 0.6360\n",
            "Epoch 738/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2372449237914322.5000 - accuracy: 0.6360\n",
            "Epoch 739/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2373859736828988.5000 - accuracy: 0.6360\n",
            "Epoch 740/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2375248258084141.5000 - accuracy: 0.6360\n",
            "Epoch 741/1000\n",
            "272/272 [==============================] - 0s 151us/step - loss: -2376659162404502.5000 - accuracy: 0.6360\n",
            "Epoch 742/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2378067898346797.5000 - accuracy: 0.6360\n",
            "Epoch 743/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2379462808182302.0000 - accuracy: 0.6360\n",
            "Epoch 744/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2380853249619125.0000 - accuracy: 0.6360\n",
            "Epoch 745/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2382239244422927.0000 - accuracy: 0.6360\n",
            "Epoch 746/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2383631889665807.0000 - accuracy: 0.6360\n",
            "Epoch 747/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2385062654963953.0000 - accuracy: 0.6360\n",
            "Epoch 748/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2386391647942776.5000 - accuracy: 0.6360\n",
            "Epoch 749/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2387780484387538.5000 - accuracy: 0.6360\n",
            "Epoch 750/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2389226477815687.5000 - accuracy: 0.6360\n",
            "Epoch 751/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2390685877742893.5000 - accuracy: 0.6360\n",
            "Epoch 752/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2392132043330258.5000 - accuracy: 0.6360\n",
            "Epoch 753/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2393577844591435.0000 - accuracy: 0.6360\n",
            "Epoch 754/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2394977605532732.5000 - accuracy: 0.6360\n",
            "Epoch 755/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2396415721533681.0000 - accuracy: 0.6360\n",
            "Epoch 756/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2397820397274051.5000 - accuracy: 0.6360\n",
            "Epoch 757/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2399281550304678.0000 - accuracy: 0.6360\n",
            "Epoch 758/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2400652554049174.5000 - accuracy: 0.6360\n",
            "Epoch 759/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2402101069395125.0000 - accuracy: 0.6360\n",
            "Epoch 760/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2403513215807729.0000 - accuracy: 0.6360\n",
            "Epoch 761/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2405175958530409.5000 - accuracy: 0.6360\n",
            "Epoch 762/1000\n",
            "272/272 [==============================] - 0s 125us/step - loss: -2406718463973014.5000 - accuracy: 0.6360\n",
            "Epoch 763/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2408119749913901.5000 - accuracy: 0.6360\n",
            "Epoch 764/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2409533036097777.0000 - accuracy: 0.6360\n",
            "Epoch 765/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2410910551375872.0000 - accuracy: 0.6360\n",
            "Epoch 766/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2412292738800218.0000 - accuracy: 0.6360\n",
            "Epoch 767/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2413749150293534.0000 - accuracy: 0.6360\n",
            "Epoch 768/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2415136736774023.5000 - accuracy: 0.6360\n",
            "Epoch 769/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2416538126523934.0000 - accuracy: 0.6360\n",
            "Epoch 770/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2417912746375288.5000 - accuracy: 0.6360\n",
            "Epoch 771/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2419243418679416.5000 - accuracy: 0.6360\n",
            "Epoch 772/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2420671990607149.5000 - accuracy: 0.6360\n",
            "Epoch 773/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2422143763981734.0000 - accuracy: 0.6360\n",
            "Epoch 774/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2423708502196224.0000 - accuracy: 0.6360\n",
            "Epoch 775/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2425253430373677.5000 - accuracy: 0.6360\n",
            "Epoch 776/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2426632055261063.5000 - accuracy: 0.6360\n",
            "Epoch 777/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2428006902591729.0000 - accuracy: 0.6360\n",
            "Epoch 778/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2429369849109082.0000 - accuracy: 0.6360\n",
            "Epoch 779/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2430726193113449.5000 - accuracy: 0.6360\n",
            "Epoch 780/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2432093868307637.0000 - accuracy: 0.6360\n",
            "Epoch 781/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2433521712149685.0000 - accuracy: 0.6360\n",
            "Epoch 782/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2434923764534091.0000 - accuracy: 0.6360\n",
            "Epoch 783/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2436247908592941.5000 - accuracy: 0.6360\n",
            "Epoch 784/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2437725401521694.0000 - accuracy: 0.6360\n",
            "Epoch 785/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2439131568460498.5000 - accuracy: 0.6360\n",
            "Epoch 786/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2440447737971651.5000 - accuracy: 0.6360\n",
            "Epoch 787/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2441853829659708.5000 - accuracy: 0.6360\n",
            "Epoch 788/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2443246708241589.0000 - accuracy: 0.6360\n",
            "Epoch 789/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2444653456338221.5000 - accuracy: 0.6360\n",
            "Epoch 790/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2446075315236141.5000 - accuracy: 0.6360\n",
            "Epoch 791/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2447472519544832.0000 - accuracy: 0.6360\n",
            "Epoch 792/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2448861365920226.0000 - accuracy: 0.6360\n",
            "Epoch 793/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2450291595519398.0000 - accuracy: 0.6360\n",
            "Epoch 794/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2451714506736579.5000 - accuracy: 0.6360\n",
            "Epoch 795/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2453172125202552.5000 - accuracy: 0.6360\n",
            "Epoch 796/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2454570227975107.5000 - accuracy: 0.6360\n",
            "Epoch 797/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2456141880931509.0000 - accuracy: 0.6360\n",
            "Epoch 798/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2457546298105374.0000 - accuracy: 0.6360\n",
            "Epoch 799/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2459013844735638.5000 - accuracy: 0.6360\n",
            "Epoch 800/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2460526677512914.5000 - accuracy: 0.6360\n",
            "Epoch 801/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2461874903349368.5000 - accuracy: 0.6360\n",
            "Epoch 802/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2463364113066345.5000 - accuracy: 0.6360\n",
            "Epoch 803/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2464882747059501.5000 - accuracy: 0.6360\n",
            "Epoch 804/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2466442672248470.5000 - accuracy: 0.6360\n",
            "Epoch 805/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2467838768046562.0000 - accuracy: 0.6360\n",
            "Epoch 806/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2469228144076198.0000 - accuracy: 0.6360\n",
            "Epoch 807/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2470700997734641.0000 - accuracy: 0.6360\n",
            "Epoch 808/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2472054458401731.5000 - accuracy: 0.6360\n",
            "Epoch 809/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2473491591794447.0000 - accuracy: 0.6360\n",
            "Epoch 810/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2474857862575285.0000 - accuracy: 0.6360\n",
            "Epoch 811/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2476324310606306.0000 - accuracy: 0.6360\n",
            "Epoch 812/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2477783612931132.5000 - accuracy: 0.6360\n",
            "Epoch 813/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2479497019607883.0000 - accuracy: 0.6360\n",
            "Epoch 814/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2480980056512150.5000 - accuracy: 0.6360\n",
            "Epoch 815/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2482394916762805.0000 - accuracy: 0.6360\n",
            "Epoch 816/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2483704645303356.5000 - accuracy: 0.6360\n",
            "Epoch 817/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2485239533211407.0000 - accuracy: 0.6360\n",
            "Epoch 818/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2486541146143804.5000 - accuracy: 0.6360\n",
            "Epoch 819/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2488015683691942.0000 - accuracy: 0.6360\n",
            "Epoch 820/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2489409578343966.0000 - accuracy: 0.6360\n",
            "Epoch 821/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2490874905077158.0000 - accuracy: 0.6360\n",
            "Epoch 822/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2492244160290334.0000 - accuracy: 0.6360\n",
            "Epoch 823/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2493630596729675.0000 - accuracy: 0.6360\n",
            "Epoch 824/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2494992065001592.5000 - accuracy: 0.6360\n",
            "Epoch 825/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2496428690081671.5000 - accuracy: 0.6360\n",
            "Epoch 826/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2497896701231104.0000 - accuracy: 0.6360\n",
            "Epoch 827/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2499287687996838.0000 - accuracy: 0.6360\n",
            "Epoch 828/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2500762572993717.0000 - accuracy: 0.6360\n",
            "Epoch 829/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2502123930949270.5000 - accuracy: 0.6360\n",
            "Epoch 830/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2503608533840113.0000 - accuracy: 0.6360\n",
            "Epoch 831/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2505017838118310.0000 - accuracy: 0.6360\n",
            "Epoch 832/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2506451967371625.5000 - accuracy: 0.6360\n",
            "Epoch 833/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2507891394747934.0000 - accuracy: 0.6360\n",
            "Epoch 834/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2509324176357496.5000 - accuracy: 0.6360\n",
            "Epoch 835/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2510722705653760.0000 - accuracy: 0.6360\n",
            "Epoch 836/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2512122869556646.0000 - accuracy: 0.6360\n",
            "Epoch 837/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2513515523681581.5000 - accuracy: 0.6360\n",
            "Epoch 838/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2514932037844992.0000 - accuracy: 0.6360\n",
            "Epoch 839/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2516448741101327.0000 - accuracy: 0.6360\n",
            "Epoch 840/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2517775080195975.5000 - accuracy: 0.6360\n",
            "Epoch 841/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2519285130854400.0000 - accuracy: 0.6360\n",
            "Epoch 842/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2520688682212894.0000 - accuracy: 0.6360\n",
            "Epoch 843/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2522137022384971.0000 - accuracy: 0.6360\n",
            "Epoch 844/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2523650456674786.0000 - accuracy: 0.6360\n",
            "Epoch 845/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2525053887447040.0000 - accuracy: 0.6360\n",
            "Epoch 846/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2526615159130955.0000 - accuracy: 0.6360\n",
            "Epoch 847/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2528027377876028.5000 - accuracy: 0.6360\n",
            "Epoch 848/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2529527866157417.5000 - accuracy: 0.6360\n",
            "Epoch 849/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2531018426779527.5000 - accuracy: 0.6360\n",
            "Epoch 850/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2532552354962974.0000 - accuracy: 0.6360\n",
            "Epoch 851/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2534043632811068.5000 - accuracy: 0.6360\n",
            "Epoch 852/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2535510529848501.0000 - accuracy: 0.6360\n",
            "Epoch 853/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2537048825135104.0000 - accuracy: 0.6360\n",
            "Epoch 854/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2538516646985728.0000 - accuracy: 0.6360\n",
            "Epoch 855/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2540086174786982.0000 - accuracy: 0.6360\n",
            "Epoch 856/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2541519641093542.0000 - accuracy: 0.6360\n",
            "Epoch 857/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2543014695326418.5000 - accuracy: 0.6360\n",
            "Epoch 858/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2544494919298349.5000 - accuracy: 0.6360\n",
            "Epoch 859/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2545980477199058.5000 - accuracy: 0.6360\n",
            "Epoch 860/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2547508365075878.0000 - accuracy: 0.6360\n",
            "Epoch 861/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2548918954769468.5000 - accuracy: 0.6360\n",
            "Epoch 862/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2550365181012329.5000 - accuracy: 0.6360\n",
            "Epoch 863/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2551847684777382.0000 - accuracy: 0.6360\n",
            "Epoch 864/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2553406415376143.0000 - accuracy: 0.6360\n",
            "Epoch 865/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2554877170972792.5000 - accuracy: 0.6360\n",
            "Epoch 866/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2556406304542479.0000 - accuracy: 0.6360\n",
            "Epoch 867/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2557922036262310.0000 - accuracy: 0.6360\n",
            "Epoch 868/1000\n",
            "272/272 [==============================] - 0s 149us/step - loss: -2559459711963858.5000 - accuracy: 0.6360\n",
            "Epoch 869/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2560985950661933.5000 - accuracy: 0.6360\n",
            "Epoch 870/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2562486235993750.5000 - accuracy: 0.6360\n",
            "Epoch 871/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2563971860012574.0000 - accuracy: 0.6360\n",
            "Epoch 872/1000\n",
            "272/272 [==============================] - 0s 156us/step - loss: -2565599341127078.0000 - accuracy: 0.6360\n",
            "Epoch 873/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2567255914616470.5000 - accuracy: 0.6360\n",
            "Epoch 874/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2568669287554590.0000 - accuracy: 0.6360\n",
            "Epoch 875/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2570305677866165.0000 - accuracy: 0.6360\n",
            "Epoch 876/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2571800619407962.0000 - accuracy: 0.6360\n",
            "Epoch 877/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2573257654680636.5000 - accuracy: 0.6360\n",
            "Epoch 878/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2574682328677436.5000 - accuracy: 0.6360\n",
            "Epoch 879/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2576245305623491.5000 - accuracy: 0.6360\n",
            "Epoch 880/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2577757862902844.5000 - accuracy: 0.6360\n",
            "Epoch 881/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2579207235367153.0000 - accuracy: 0.6360\n",
            "Epoch 882/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2580656976190042.0000 - accuracy: 0.6360\n",
            "Epoch 883/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2582102561105318.0000 - accuracy: 0.6360\n",
            "Epoch 884/1000\n",
            "272/272 [==============================] - 0s 159us/step - loss: -2583566981221195.0000 - accuracy: 0.6360\n",
            "Epoch 885/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2585084094840832.0000 - accuracy: 0.6360\n",
            "Epoch 886/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2586597075590686.0000 - accuracy: 0.6360\n",
            "Epoch 887/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2588082310402770.5000 - accuracy: 0.6360\n",
            "Epoch 888/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2589569750678588.5000 - accuracy: 0.6360\n",
            "Epoch 889/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2591035129162089.5000 - accuracy: 0.6360\n",
            "Epoch 890/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2592564074666586.0000 - accuracy: 0.6360\n",
            "Epoch 891/1000\n",
            "272/272 [==============================] - 0s 170us/step - loss: -2593985409688998.0000 - accuracy: 0.6360\n",
            "Epoch 892/1000\n",
            "272/272 [==============================] - 0s 160us/step - loss: -2595471247093037.5000 - accuracy: 0.6360\n",
            "Epoch 893/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2596993628912700.5000 - accuracy: 0.6360\n",
            "Epoch 894/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2598577350207849.5000 - accuracy: 0.6360\n",
            "Epoch 895/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2600058678858450.5000 - accuracy: 0.6360\n",
            "Epoch 896/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2601547135790381.5000 - accuracy: 0.6360\n",
            "Epoch 897/1000\n",
            "272/272 [==============================] - 0s 153us/step - loss: -2603042393817088.0000 - accuracy: 0.6360\n",
            "Epoch 898/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2604506036691426.0000 - accuracy: 0.6360\n",
            "Epoch 899/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2606001856384783.0000 - accuracy: 0.6360\n",
            "Epoch 900/1000\n",
            "272/272 [==============================] - 0s 140us/step - loss: -2607698569577411.5000 - accuracy: 0.6360\n",
            "Epoch 901/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2609233745134531.5000 - accuracy: 0.6360\n",
            "Epoch 902/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2610737754083087.0000 - accuracy: 0.6360\n",
            "Epoch 903/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2612199778326166.5000 - accuracy: 0.6360\n",
            "Epoch 904/1000\n",
            "272/272 [==============================] - 0s 150us/step - loss: -2613626041871661.5000 - accuracy: 0.6360\n",
            "Epoch 905/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2615152356005526.5000 - accuracy: 0.6360\n",
            "Epoch 906/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2616605248231062.5000 - accuracy: 0.6360\n",
            "Epoch 907/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2618047554094983.5000 - accuracy: 0.6360\n",
            "Epoch 908/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2619577756472018.5000 - accuracy: 0.6360\n",
            "Epoch 909/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2621025487699004.5000 - accuracy: 0.6360\n",
            "Epoch 910/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2622546204503341.5000 - accuracy: 0.6360\n",
            "Epoch 911/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2624006150437948.5000 - accuracy: 0.6360\n",
            "Epoch 912/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2625516589747983.0000 - accuracy: 0.6360\n",
            "Epoch 913/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2627020542443520.0000 - accuracy: 0.6360\n",
            "Epoch 914/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2628450782713494.5000 - accuracy: 0.6360\n",
            "Epoch 915/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2630046932440726.5000 - accuracy: 0.6360\n",
            "Epoch 916/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2631493129385141.0000 - accuracy: 0.6360\n",
            "Epoch 917/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2633035408026925.5000 - accuracy: 0.6360\n",
            "Epoch 918/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2634540158380393.5000 - accuracy: 0.6360\n",
            "Epoch 919/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2636182499422449.0000 - accuracy: 0.6360\n",
            "Epoch 920/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2637774018761426.5000 - accuracy: 0.6360\n",
            "Epoch 921/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2639137911371896.5000 - accuracy: 0.6360\n",
            "Epoch 922/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2640780337903736.5000 - accuracy: 0.6360\n",
            "Epoch 923/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2642379760236664.5000 - accuracy: 0.6360\n",
            "Epoch 924/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2643873298228646.0000 - accuracy: 0.6360\n",
            "Epoch 925/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2645444717722684.5000 - accuracy: 0.6360\n",
            "Epoch 926/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2647053581989045.0000 - accuracy: 0.6360\n",
            "Epoch 927/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2648616156374438.0000 - accuracy: 0.6360\n",
            "Epoch 928/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2650123777482270.0000 - accuracy: 0.6360\n",
            "Epoch 929/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2651615386834582.5000 - accuracy: 0.6360\n",
            "Epoch 930/1000\n",
            "272/272 [==============================] - 0s 155us/step - loss: -2653226415176764.5000 - accuracy: 0.6360\n",
            "Epoch 931/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2654972510223420.5000 - accuracy: 0.6360\n",
            "Epoch 932/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2656750806668950.5000 - accuracy: 0.6360\n",
            "Epoch 933/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2658327694425148.5000 - accuracy: 0.6360\n",
            "Epoch 934/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2659794962195878.0000 - accuracy: 0.6360\n",
            "Epoch 935/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2661302458584847.0000 - accuracy: 0.6360\n",
            "Epoch 936/1000\n",
            "272/272 [==============================] - 0s 142us/step - loss: -2662736263519774.0000 - accuracy: 0.6360\n",
            "Epoch 937/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2664203927960636.5000 - accuracy: 0.6360\n",
            "Epoch 938/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2665724598905193.5000 - accuracy: 0.6360\n",
            "Epoch 939/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2667520889655055.0000 - accuracy: 0.6360\n",
            "Epoch 940/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2669057827220781.5000 - accuracy: 0.6360\n",
            "Epoch 941/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2670555823511190.5000 - accuracy: 0.6360\n",
            "Epoch 942/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2672121491504188.5000 - accuracy: 0.6360\n",
            "Epoch 943/1000\n",
            "272/272 [==============================] - 0s 147us/step - loss: -2673615210406370.0000 - accuracy: 0.6360\n",
            "Epoch 944/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2675107644124461.5000 - accuracy: 0.6360\n",
            "Epoch 945/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2676602507948273.0000 - accuracy: 0.6360\n",
            "Epoch 946/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2678019202590118.0000 - accuracy: 0.6360\n",
            "Epoch 947/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2679527422589048.5000 - accuracy: 0.6360\n",
            "Epoch 948/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2681042587121784.5000 - accuracy: 0.6360\n",
            "Epoch 949/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2682568615117763.5000 - accuracy: 0.6360\n",
            "Epoch 950/1000\n",
            "272/272 [==============================] - 0s 154us/step - loss: -2684154678592451.5000 - accuracy: 0.6360\n",
            "Epoch 951/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2685680355438833.0000 - accuracy: 0.6360\n",
            "Epoch 952/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2687214502065333.0000 - accuracy: 0.6360\n",
            "Epoch 953/1000\n",
            "272/272 [==============================] - 0s 141us/step - loss: -2688648963192952.5000 - accuracy: 0.6360\n",
            "Epoch 954/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2690182299023480.5000 - accuracy: 0.6360\n",
            "Epoch 955/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2691871313447755.0000 - accuracy: 0.6360\n",
            "Epoch 956/1000\n",
            "272/272 [==============================] - 0s 143us/step - loss: -2693460400768903.5000 - accuracy: 0.6360\n",
            "Epoch 957/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2694902860773496.5000 - accuracy: 0.6360\n",
            "Epoch 958/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2696404220525628.5000 - accuracy: 0.6360\n",
            "Epoch 959/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2697934267219968.0000 - accuracy: 0.6360\n",
            "Epoch 960/1000\n",
            "272/272 [==============================] - 0s 139us/step - loss: -2699400657209223.5000 - accuracy: 0.6360\n",
            "Epoch 961/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2700940578652160.0000 - accuracy: 0.6360\n",
            "Epoch 962/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2702440717491742.0000 - accuracy: 0.6360\n",
            "Epoch 963/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2703895984186790.0000 - accuracy: 0.6360\n",
            "Epoch 964/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2705424924941854.0000 - accuracy: 0.6360\n",
            "Epoch 965/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2706886646670758.0000 - accuracy: 0.6360\n",
            "Epoch 966/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2708476738774438.0000 - accuracy: 0.6360\n",
            "Epoch 967/1000\n",
            "272/272 [==============================] - 0s 123us/step - loss: -2709856339115068.5000 - accuracy: 0.6360\n",
            "Epoch 968/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2711406182800926.0000 - accuracy: 0.6360\n",
            "Epoch 969/1000\n",
            "272/272 [==============================] - 0s 145us/step - loss: -2712899270892122.0000 - accuracy: 0.6360\n",
            "Epoch 970/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2714409332591435.0000 - accuracy: 0.6360\n",
            "Epoch 971/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2715882646328018.5000 - accuracy: 0.6360\n",
            "Epoch 972/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2717425491632610.0000 - accuracy: 0.6360\n",
            "Epoch 973/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2718915314211418.0000 - accuracy: 0.6360\n",
            "Epoch 974/1000\n",
            "272/272 [==============================] - 0s 134us/step - loss: -2720441775392647.5000 - accuracy: 0.6360\n",
            "Epoch 975/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2722015615308498.5000 - accuracy: 0.6360\n",
            "Epoch 976/1000\n",
            "272/272 [==============================] - 0s 163us/step - loss: -2723535867932913.0000 - accuracy: 0.6360\n",
            "Epoch 977/1000\n",
            "272/272 [==============================] - 0s 148us/step - loss: -2725022387836566.5000 - accuracy: 0.6360\n",
            "Epoch 978/1000\n",
            "272/272 [==============================] - 0s 146us/step - loss: -2726572836026488.5000 - accuracy: 0.6360\n",
            "Epoch 979/1000\n",
            "272/272 [==============================] - 0s 136us/step - loss: -2728100723271078.0000 - accuracy: 0.6360\n",
            "Epoch 980/1000\n",
            "272/272 [==============================] - 0s 127us/step - loss: -2729625627501990.0000 - accuracy: 0.6360\n",
            "Epoch 981/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2731196643294268.5000 - accuracy: 0.6360\n",
            "Epoch 982/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2732670165080666.0000 - accuracy: 0.6360\n",
            "Epoch 983/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2734424078803426.0000 - accuracy: 0.6360\n",
            "Epoch 984/1000\n",
            "272/272 [==============================] - 0s 128us/step - loss: -2735975228644894.0000 - accuracy: 0.6360\n",
            "Epoch 985/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2737393683691640.5000 - accuracy: 0.6360\n",
            "Epoch 986/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2739044997676092.5000 - accuracy: 0.6360\n",
            "Epoch 987/1000\n",
            "272/272 [==============================] - 0s 137us/step - loss: -2740573574883689.5000 - accuracy: 0.6360\n",
            "Epoch 988/1000\n",
            "272/272 [==============================] - 0s 126us/step - loss: -2742233647345905.0000 - accuracy: 0.6360\n",
            "Epoch 989/1000\n",
            "272/272 [==============================] - 0s 132us/step - loss: -2743885143042409.5000 - accuracy: 0.6360\n",
            "Epoch 990/1000\n",
            "272/272 [==============================] - 0s 144us/step - loss: -2745376833852114.5000 - accuracy: 0.6360\n",
            "Epoch 991/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2746918921676198.0000 - accuracy: 0.6360\n",
            "Epoch 992/1000\n",
            "272/272 [==============================] - 0s 138us/step - loss: -2748449168926118.0000 - accuracy: 0.6360\n",
            "Epoch 993/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2749960824460950.5000 - accuracy: 0.6360\n",
            "Epoch 994/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2751543680711740.5000 - accuracy: 0.6360\n",
            "Epoch 995/1000\n",
            "272/272 [==============================] - 0s 131us/step - loss: -2753042247489174.5000 - accuracy: 0.6360\n",
            "Epoch 996/1000\n",
            "272/272 [==============================] - 0s 129us/step - loss: -2754610077029074.5000 - accuracy: 0.6360\n",
            "Epoch 997/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2756135559827215.0000 - accuracy: 0.6360\n",
            "Epoch 998/1000\n",
            "272/272 [==============================] - 0s 135us/step - loss: -2757662298942765.5000 - accuracy: 0.6360\n",
            "Epoch 999/1000\n",
            "272/272 [==============================] - 0s 133us/step - loss: -2759138913473234.5000 - accuracy: 0.6360\n",
            "Epoch 1000/1000\n",
            "272/272 [==============================] - 0s 130us/step - loss: -2760757266272978.5000 - accuracy: 0.6360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f162dc939e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Y3cmhJOkfO"
      },
      "source": [
        "ANNPred = model.predict(XTest)\n",
        "EVALUATION['ANN'] = list(ANNPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2qB1D_KtmhL"
      },
      "source": [
        "###Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCU8dMUku33L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e8d1c0-2591-463e-d9cf-50fa67931ede"
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(64, activation = 'relu', input_shape=(15,)))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n",
        "/\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 64)                1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 149,505\n",
            "Trainable params: 149,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAsMvoECgG55"
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights.best.cnn.hdf5', verbose=1, save_best_only=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPVn9qXKu3vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d5f2e6-bb3c-4a8e-c8d6-a96b8f1e4343"
      },
      "source": [
        "NN_model.fit(X_train, Y_train, epochs=1000, batch_size=30, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 868 samples, validate on 218 samples\n",
            "Epoch 1/1000\n",
            "868/868 [==============================] - 0s 245us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.28899, saving model to Weights-001--0.28899.hdf5\n",
            "Epoch 2/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.28899\n",
            "Epoch 3/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.28899\n",
            "Epoch 4/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.28899\n",
            "Epoch 5/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.28899\n",
            "Epoch 6/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.28899\n",
            "Epoch 7/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.28899\n",
            "Epoch 8/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.28899\n",
            "Epoch 9/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.28899\n",
            "Epoch 10/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.28899\n",
            "Epoch 11/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.28899\n",
            "Epoch 12/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.28899\n",
            "Epoch 13/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.28899\n",
            "Epoch 14/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.28899\n",
            "Epoch 15/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.28899\n",
            "Epoch 16/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.28899\n",
            "Epoch 17/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.28899\n",
            "Epoch 18/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.28899\n",
            "Epoch 19/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.28899\n",
            "Epoch 20/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.28899\n",
            "Epoch 21/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.28899\n",
            "Epoch 22/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.28899\n",
            "Epoch 23/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.28899\n",
            "Epoch 24/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.28899\n",
            "Epoch 25/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.28899\n",
            "Epoch 26/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.28899\n",
            "Epoch 27/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.28899\n",
            "Epoch 28/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.28899\n",
            "Epoch 29/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.28899\n",
            "Epoch 30/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.28899\n",
            "Epoch 31/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.28899\n",
            "Epoch 32/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.28899\n",
            "Epoch 33/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.28899\n",
            "Epoch 34/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.28899\n",
            "Epoch 35/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.28899\n",
            "Epoch 36/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.28899\n",
            "Epoch 37/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.28899\n",
            "Epoch 38/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.28899\n",
            "Epoch 39/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.28899\n",
            "Epoch 40/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.28899\n",
            "Epoch 41/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.28899\n",
            "Epoch 42/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.28899\n",
            "Epoch 43/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.28899\n",
            "Epoch 44/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.28899\n",
            "Epoch 45/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.28899\n",
            "Epoch 46/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.28899\n",
            "Epoch 47/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.28899\n",
            "Epoch 48/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.28899\n",
            "Epoch 49/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.28899\n",
            "Epoch 50/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.28899\n",
            "Epoch 51/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.28899\n",
            "Epoch 52/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.28899\n",
            "Epoch 53/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.28899\n",
            "Epoch 54/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.28899\n",
            "Epoch 55/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.28899\n",
            "Epoch 56/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.28899\n",
            "Epoch 57/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.28899\n",
            "Epoch 58/1000\n",
            "868/868 [==============================] - 0s 148us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.28899\n",
            "Epoch 59/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.28899\n",
            "Epoch 60/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.28899\n",
            "Epoch 61/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.28899\n",
            "Epoch 62/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.28899\n",
            "Epoch 63/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.28899\n",
            "Epoch 64/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.28899\n",
            "Epoch 65/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.28899\n",
            "Epoch 66/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.28899\n",
            "Epoch 67/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.28899\n",
            "Epoch 68/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.28899\n",
            "Epoch 69/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.28899\n",
            "Epoch 70/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.28899\n",
            "Epoch 71/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.28899\n",
            "Epoch 72/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.28899\n",
            "Epoch 73/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.28899\n",
            "Epoch 74/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.28899\n",
            "Epoch 75/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.28899\n",
            "Epoch 76/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.28899\n",
            "Epoch 77/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.28899\n",
            "Epoch 78/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.28899\n",
            "Epoch 79/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.28899\n",
            "Epoch 80/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.28899\n",
            "Epoch 81/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.28899\n",
            "Epoch 82/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.28899\n",
            "Epoch 83/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.28899\n",
            "Epoch 84/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.28899\n",
            "Epoch 85/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.28899\n",
            "Epoch 86/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.28899\n",
            "Epoch 87/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.28899\n",
            "Epoch 88/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.28899\n",
            "Epoch 89/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.28899\n",
            "Epoch 90/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.28899\n",
            "Epoch 91/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.28899\n",
            "Epoch 92/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.28899\n",
            "Epoch 93/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.28899\n",
            "Epoch 94/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.28899\n",
            "Epoch 95/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.28899\n",
            "Epoch 96/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.28899\n",
            "Epoch 97/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.28899\n",
            "Epoch 98/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.28899\n",
            "Epoch 99/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.28899\n",
            "Epoch 100/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.28899\n",
            "Epoch 101/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.28899\n",
            "Epoch 102/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.28899\n",
            "Epoch 103/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.28899\n",
            "Epoch 104/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.28899\n",
            "Epoch 105/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.28899\n",
            "Epoch 106/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.28899\n",
            "Epoch 107/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.28899\n",
            "Epoch 108/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.28899\n",
            "Epoch 109/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.28899\n",
            "Epoch 110/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.28899\n",
            "Epoch 111/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.28899\n",
            "Epoch 112/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.28899\n",
            "Epoch 113/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.28899\n",
            "Epoch 114/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.28899\n",
            "Epoch 115/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.28899\n",
            "Epoch 116/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.28899\n",
            "Epoch 117/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.28899\n",
            "Epoch 118/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.28899\n",
            "Epoch 119/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.28899\n",
            "Epoch 120/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.28899\n",
            "Epoch 121/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.28899\n",
            "Epoch 122/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.28899\n",
            "Epoch 123/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.28899\n",
            "Epoch 124/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.28899\n",
            "Epoch 125/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.28899\n",
            "Epoch 126/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.28899\n",
            "Epoch 127/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.28899\n",
            "Epoch 128/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.28899\n",
            "Epoch 129/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.28899\n",
            "Epoch 130/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.28899\n",
            "Epoch 131/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.28899\n",
            "Epoch 132/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.28899\n",
            "Epoch 133/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.28899\n",
            "Epoch 134/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.28899\n",
            "Epoch 135/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.28899\n",
            "Epoch 136/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.28899\n",
            "Epoch 137/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.28899\n",
            "Epoch 138/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.28899\n",
            "Epoch 139/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.28899\n",
            "Epoch 140/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.28899\n",
            "Epoch 141/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.28899\n",
            "Epoch 142/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.28899\n",
            "Epoch 143/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.28899\n",
            "Epoch 144/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.28899\n",
            "Epoch 145/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.28899\n",
            "Epoch 146/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.28899\n",
            "Epoch 147/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.28899\n",
            "Epoch 148/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.28899\n",
            "Epoch 149/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.28899\n",
            "Epoch 150/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.28899\n",
            "Epoch 151/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.28899\n",
            "Epoch 152/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.28899\n",
            "Epoch 153/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.28899\n",
            "Epoch 154/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.28899\n",
            "Epoch 155/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.28899\n",
            "Epoch 156/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.28899\n",
            "Epoch 157/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.28899\n",
            "Epoch 158/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.28899\n",
            "Epoch 159/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.28899\n",
            "Epoch 160/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.28899\n",
            "Epoch 161/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.28899\n",
            "Epoch 162/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.28899\n",
            "Epoch 163/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.28899\n",
            "Epoch 164/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.28899\n",
            "Epoch 165/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.28899\n",
            "Epoch 166/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.28899\n",
            "Epoch 167/1000\n",
            "868/868 [==============================] - 0s 155us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.28899\n",
            "Epoch 168/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.28899\n",
            "Epoch 169/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.28899\n",
            "Epoch 170/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.28899\n",
            "Epoch 171/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.28899\n",
            "Epoch 172/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.28899\n",
            "Epoch 173/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.28899\n",
            "Epoch 174/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.28899\n",
            "Epoch 175/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.28899\n",
            "Epoch 176/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.28899\n",
            "Epoch 177/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.28899\n",
            "Epoch 178/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.28899\n",
            "Epoch 179/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.28899\n",
            "Epoch 180/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.28899\n",
            "Epoch 181/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.28899\n",
            "Epoch 182/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.28899\n",
            "Epoch 183/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.28899\n",
            "Epoch 184/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.28899\n",
            "Epoch 185/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.28899\n",
            "Epoch 186/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.28899\n",
            "Epoch 187/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.28899\n",
            "Epoch 188/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.28899\n",
            "Epoch 189/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.28899\n",
            "Epoch 190/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.28899\n",
            "Epoch 191/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.28899\n",
            "Epoch 192/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.28899\n",
            "Epoch 193/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.28899\n",
            "Epoch 194/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.28899\n",
            "Epoch 195/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.28899\n",
            "Epoch 196/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.28899\n",
            "Epoch 197/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.28899\n",
            "Epoch 198/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.28899\n",
            "Epoch 199/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.28899\n",
            "Epoch 200/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.28899\n",
            "Epoch 201/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.28899\n",
            "Epoch 202/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.28899\n",
            "Epoch 203/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.28899\n",
            "Epoch 204/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.28899\n",
            "Epoch 205/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.28899\n",
            "Epoch 206/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.28899\n",
            "Epoch 207/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.28899\n",
            "Epoch 208/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.28899\n",
            "Epoch 209/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.28899\n",
            "Epoch 210/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.28899\n",
            "Epoch 211/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.28899\n",
            "Epoch 212/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.28899\n",
            "Epoch 213/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.28899\n",
            "Epoch 214/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.28899\n",
            "Epoch 215/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.28899\n",
            "Epoch 216/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.28899\n",
            "Epoch 217/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.28899\n",
            "Epoch 218/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.28899\n",
            "Epoch 219/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.28899\n",
            "Epoch 220/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.28899\n",
            "Epoch 221/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.28899\n",
            "Epoch 222/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.28899\n",
            "Epoch 223/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.28899\n",
            "Epoch 224/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.28899\n",
            "Epoch 225/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.28899\n",
            "Epoch 226/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.28899\n",
            "Epoch 227/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.28899\n",
            "Epoch 228/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.28899\n",
            "Epoch 229/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.28899\n",
            "Epoch 230/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.28899\n",
            "Epoch 231/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.28899\n",
            "Epoch 232/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.28899\n",
            "Epoch 233/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.28899\n",
            "Epoch 234/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.28899\n",
            "Epoch 235/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.28899\n",
            "Epoch 236/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.28899\n",
            "Epoch 237/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.28899\n",
            "Epoch 238/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.28899\n",
            "Epoch 239/1000\n",
            "868/868 [==============================] - 0s 133us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.28899\n",
            "Epoch 240/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.28899\n",
            "Epoch 241/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.28899\n",
            "Epoch 242/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.28899\n",
            "Epoch 243/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.28899\n",
            "Epoch 244/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.28899\n",
            "Epoch 245/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.28899\n",
            "Epoch 246/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.28899\n",
            "Epoch 247/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.28899\n",
            "Epoch 248/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.28899\n",
            "Epoch 249/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.28899\n",
            "Epoch 250/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.28899\n",
            "Epoch 251/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.28899\n",
            "Epoch 252/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.28899\n",
            "Epoch 253/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.28899\n",
            "Epoch 254/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.28899\n",
            "Epoch 255/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.28899\n",
            "Epoch 256/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.28899\n",
            "Epoch 257/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.28899\n",
            "Epoch 258/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.28899\n",
            "Epoch 259/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.28899\n",
            "Epoch 260/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.28899\n",
            "Epoch 261/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.28899\n",
            "Epoch 262/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.28899\n",
            "Epoch 263/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.28899\n",
            "Epoch 264/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.28899\n",
            "Epoch 265/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.28899\n",
            "Epoch 266/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.28899\n",
            "Epoch 267/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.28899\n",
            "Epoch 268/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.28899\n",
            "Epoch 269/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.28899\n",
            "Epoch 270/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.28899\n",
            "Epoch 271/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.28899\n",
            "Epoch 272/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.28899\n",
            "Epoch 273/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.28899\n",
            "Epoch 274/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.28899\n",
            "Epoch 275/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.28899\n",
            "Epoch 276/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.28899\n",
            "Epoch 277/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.28899\n",
            "Epoch 278/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.28899\n",
            "Epoch 279/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.28899\n",
            "Epoch 280/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.28899\n",
            "Epoch 281/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.28899\n",
            "Epoch 282/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.28899\n",
            "Epoch 283/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.28899\n",
            "Epoch 284/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.28899\n",
            "Epoch 285/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.28899\n",
            "Epoch 286/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.28899\n",
            "Epoch 287/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.28899\n",
            "Epoch 288/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.28899\n",
            "Epoch 289/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.28899\n",
            "Epoch 290/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.28899\n",
            "Epoch 291/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.28899\n",
            "Epoch 292/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.28899\n",
            "Epoch 293/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.28899\n",
            "Epoch 294/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.28899\n",
            "Epoch 295/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.28899\n",
            "Epoch 296/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.28899\n",
            "Epoch 297/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.28899\n",
            "Epoch 298/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.28899\n",
            "Epoch 299/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.28899\n",
            "Epoch 300/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.28899\n",
            "Epoch 301/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.28899\n",
            "Epoch 302/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.28899\n",
            "Epoch 303/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.28899\n",
            "Epoch 304/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.28899\n",
            "Epoch 305/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.28899\n",
            "Epoch 306/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.28899\n",
            "Epoch 307/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.28899\n",
            "Epoch 308/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.28899\n",
            "Epoch 309/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.28899\n",
            "Epoch 310/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.28899\n",
            "Epoch 311/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.28899\n",
            "Epoch 312/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.28899\n",
            "Epoch 313/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.28899\n",
            "Epoch 314/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.28899\n",
            "Epoch 315/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.28899\n",
            "Epoch 316/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.28899\n",
            "Epoch 317/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.28899\n",
            "Epoch 318/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.28899\n",
            "Epoch 319/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.28899\n",
            "Epoch 320/1000\n",
            "868/868 [==============================] - 0s 139us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.28899\n",
            "Epoch 321/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.28899\n",
            "Epoch 322/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.28899\n",
            "Epoch 323/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.28899\n",
            "Epoch 324/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.28899\n",
            "Epoch 325/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.28899\n",
            "Epoch 326/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.28899\n",
            "Epoch 327/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.28899\n",
            "Epoch 328/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.28899\n",
            "Epoch 329/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.28899\n",
            "Epoch 330/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.28899\n",
            "Epoch 331/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.28899\n",
            "Epoch 332/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.28899\n",
            "Epoch 333/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.28899\n",
            "Epoch 334/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.28899\n",
            "Epoch 335/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.28899\n",
            "Epoch 336/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.28899\n",
            "Epoch 337/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.28899\n",
            "Epoch 338/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.28899\n",
            "Epoch 339/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.28899\n",
            "Epoch 340/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.28899\n",
            "Epoch 341/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.28899\n",
            "Epoch 342/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.28899\n",
            "Epoch 343/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.28899\n",
            "Epoch 344/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.28899\n",
            "Epoch 345/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.28899\n",
            "Epoch 346/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.28899\n",
            "Epoch 347/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.28899\n",
            "Epoch 348/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.28899\n",
            "Epoch 349/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.28899\n",
            "Epoch 350/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.28899\n",
            "Epoch 351/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.28899\n",
            "Epoch 352/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.28899\n",
            "Epoch 353/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.28899\n",
            "Epoch 354/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.28899\n",
            "Epoch 355/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.28899\n",
            "Epoch 356/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.28899\n",
            "Epoch 357/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.28899\n",
            "Epoch 358/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.28899\n",
            "Epoch 359/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.28899\n",
            "Epoch 360/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.28899\n",
            "Epoch 361/1000\n",
            "868/868 [==============================] - 0s 128us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.28899\n",
            "Epoch 362/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.28899\n",
            "Epoch 363/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.28899\n",
            "Epoch 364/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.28899\n",
            "Epoch 365/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.28899\n",
            "Epoch 366/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.28899\n",
            "Epoch 367/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.28899\n",
            "Epoch 368/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.28899\n",
            "Epoch 369/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.28899\n",
            "Epoch 370/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.28899\n",
            "Epoch 371/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.28899\n",
            "Epoch 372/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.28899\n",
            "Epoch 373/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.28899\n",
            "Epoch 374/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.28899\n",
            "Epoch 375/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.28899\n",
            "Epoch 376/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.28899\n",
            "Epoch 377/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.28899\n",
            "Epoch 378/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.28899\n",
            "Epoch 379/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.28899\n",
            "Epoch 380/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.28899\n",
            "Epoch 381/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.28899\n",
            "Epoch 382/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.28899\n",
            "Epoch 383/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.28899\n",
            "Epoch 384/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.28899\n",
            "Epoch 385/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.28899\n",
            "Epoch 386/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.28899\n",
            "Epoch 387/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.28899\n",
            "Epoch 388/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.28899\n",
            "Epoch 389/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.28899\n",
            "Epoch 390/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.28899\n",
            "Epoch 391/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.28899\n",
            "Epoch 392/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.28899\n",
            "Epoch 393/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.28899\n",
            "Epoch 394/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.28899\n",
            "Epoch 395/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.28899\n",
            "Epoch 396/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.28899\n",
            "Epoch 397/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.28899\n",
            "Epoch 398/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.28899\n",
            "Epoch 399/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.28899\n",
            "Epoch 400/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.28899\n",
            "Epoch 401/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.28899\n",
            "Epoch 402/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.28899\n",
            "Epoch 403/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.28899\n",
            "Epoch 404/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.28899\n",
            "Epoch 405/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.28899\n",
            "Epoch 406/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.28899\n",
            "Epoch 407/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.28899\n",
            "Epoch 408/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.28899\n",
            "Epoch 409/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.28899\n",
            "Epoch 410/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.28899\n",
            "Epoch 411/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.28899\n",
            "Epoch 412/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.28899\n",
            "Epoch 413/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.28899\n",
            "Epoch 414/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.28899\n",
            "Epoch 415/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.28899\n",
            "Epoch 416/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.28899\n",
            "Epoch 417/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.28899\n",
            "Epoch 418/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.28899\n",
            "Epoch 419/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.28899\n",
            "Epoch 420/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.28899\n",
            "Epoch 421/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.28899\n",
            "Epoch 422/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.28899\n",
            "Epoch 423/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.28899\n",
            "Epoch 424/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.28899\n",
            "Epoch 425/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.28899\n",
            "Epoch 426/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.28899\n",
            "Epoch 427/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.28899\n",
            "Epoch 428/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.28899\n",
            "Epoch 429/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.28899\n",
            "Epoch 430/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.28899\n",
            "Epoch 431/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.28899\n",
            "Epoch 432/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.28899\n",
            "Epoch 433/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.28899\n",
            "Epoch 434/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.28899\n",
            "Epoch 435/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.28899\n",
            "Epoch 436/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.28899\n",
            "Epoch 437/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.28899\n",
            "Epoch 438/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.28899\n",
            "Epoch 439/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.28899\n",
            "Epoch 440/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.28899\n",
            "Epoch 441/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.28899\n",
            "Epoch 442/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.28899\n",
            "Epoch 443/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.28899\n",
            "Epoch 444/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.28899\n",
            "Epoch 445/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.28899\n",
            "Epoch 446/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.28899\n",
            "Epoch 447/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.28899\n",
            "Epoch 448/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.28899\n",
            "Epoch 449/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.28899\n",
            "Epoch 450/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.28899\n",
            "Epoch 451/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.28899\n",
            "Epoch 452/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.28899\n",
            "Epoch 453/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.28899\n",
            "Epoch 454/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.28899\n",
            "Epoch 455/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.28899\n",
            "Epoch 456/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.28899\n",
            "Epoch 457/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.28899\n",
            "Epoch 458/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.28899\n",
            "Epoch 459/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.28899\n",
            "Epoch 460/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.28899\n",
            "Epoch 461/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.28899\n",
            "Epoch 462/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.28899\n",
            "Epoch 463/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.28899\n",
            "Epoch 464/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.28899\n",
            "Epoch 465/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.28899\n",
            "Epoch 466/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.28899\n",
            "Epoch 467/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.28899\n",
            "Epoch 468/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.28899\n",
            "Epoch 469/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.28899\n",
            "Epoch 470/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.28899\n",
            "Epoch 471/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.28899\n",
            "Epoch 472/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.28899\n",
            "Epoch 473/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.28899\n",
            "Epoch 474/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.28899\n",
            "Epoch 475/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.28899\n",
            "Epoch 476/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.28899\n",
            "Epoch 477/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.28899\n",
            "Epoch 478/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.28899\n",
            "Epoch 479/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.28899\n",
            "Epoch 480/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.28899\n",
            "Epoch 481/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.28899\n",
            "Epoch 482/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.28899\n",
            "Epoch 483/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.28899\n",
            "Epoch 484/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.28899\n",
            "Epoch 485/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.28899\n",
            "Epoch 486/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.28899\n",
            "Epoch 487/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.28899\n",
            "Epoch 488/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.28899\n",
            "Epoch 489/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.28899\n",
            "Epoch 490/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.28899\n",
            "Epoch 491/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.28899\n",
            "Epoch 492/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.28899\n",
            "Epoch 493/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.28899\n",
            "Epoch 494/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.28899\n",
            "Epoch 495/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.28899\n",
            "Epoch 496/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.28899\n",
            "Epoch 497/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.28899\n",
            "Epoch 498/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.28899\n",
            "Epoch 499/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.28899\n",
            "Epoch 500/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.28899\n",
            "Epoch 501/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.28899\n",
            "Epoch 502/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.28899\n",
            "Epoch 503/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.28899\n",
            "Epoch 504/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.28899\n",
            "Epoch 505/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.28899\n",
            "Epoch 506/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.28899\n",
            "Epoch 507/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.28899\n",
            "Epoch 508/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.28899\n",
            "Epoch 509/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.28899\n",
            "Epoch 510/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.28899\n",
            "Epoch 511/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.28899\n",
            "Epoch 512/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.28899\n",
            "Epoch 513/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.28899\n",
            "Epoch 514/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.28899\n",
            "Epoch 515/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.28899\n",
            "Epoch 516/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.28899\n",
            "Epoch 517/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.28899\n",
            "Epoch 518/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.28899\n",
            "Epoch 519/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.28899\n",
            "Epoch 520/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.28899\n",
            "Epoch 521/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.28899\n",
            "Epoch 522/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.28899\n",
            "Epoch 523/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.28899\n",
            "Epoch 524/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.28899\n",
            "Epoch 525/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.28899\n",
            "Epoch 526/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.28899\n",
            "Epoch 527/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.28899\n",
            "Epoch 528/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.28899\n",
            "Epoch 529/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.28899\n",
            "Epoch 530/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.28899\n",
            "Epoch 531/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.28899\n",
            "Epoch 532/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.28899\n",
            "Epoch 533/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.28899\n",
            "Epoch 534/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.28899\n",
            "Epoch 535/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.28899\n",
            "Epoch 536/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.28899\n",
            "Epoch 537/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.28899\n",
            "Epoch 538/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.28899\n",
            "Epoch 539/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.28899\n",
            "Epoch 540/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.28899\n",
            "Epoch 541/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.28899\n",
            "Epoch 542/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.28899\n",
            "Epoch 543/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.28899\n",
            "Epoch 544/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.28899\n",
            "Epoch 545/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.28899\n",
            "Epoch 546/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.28899\n",
            "Epoch 547/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.28899\n",
            "Epoch 548/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.28899\n",
            "Epoch 549/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.28899\n",
            "Epoch 550/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.28899\n",
            "Epoch 551/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.28899\n",
            "Epoch 552/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.28899\n",
            "Epoch 553/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.28899\n",
            "Epoch 554/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.28899\n",
            "Epoch 555/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.28899\n",
            "Epoch 556/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.28899\n",
            "Epoch 557/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.28899\n",
            "Epoch 558/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.28899\n",
            "Epoch 559/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.28899\n",
            "Epoch 560/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.28899\n",
            "Epoch 561/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.28899\n",
            "Epoch 562/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.28899\n",
            "Epoch 563/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.28899\n",
            "Epoch 564/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.28899\n",
            "Epoch 565/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.28899\n",
            "Epoch 566/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.28899\n",
            "Epoch 567/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.28899\n",
            "Epoch 568/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.28899\n",
            "Epoch 569/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.28899\n",
            "Epoch 570/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.28899\n",
            "Epoch 571/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.28899\n",
            "Epoch 572/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.28899\n",
            "Epoch 573/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.28899\n",
            "Epoch 574/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.28899\n",
            "Epoch 575/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.28899\n",
            "Epoch 576/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.28899\n",
            "Epoch 577/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.28899\n",
            "Epoch 578/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.28899\n",
            "Epoch 579/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.28899\n",
            "Epoch 580/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.28899\n",
            "Epoch 581/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.28899\n",
            "Epoch 582/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.28899\n",
            "Epoch 583/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.28899\n",
            "Epoch 584/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.28899\n",
            "Epoch 585/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.28899\n",
            "Epoch 586/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.28899\n",
            "Epoch 587/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.28899\n",
            "Epoch 588/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.28899\n",
            "Epoch 589/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.28899\n",
            "Epoch 590/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.28899\n",
            "Epoch 591/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.28899\n",
            "Epoch 592/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.28899\n",
            "Epoch 593/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.28899\n",
            "Epoch 594/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.28899\n",
            "Epoch 595/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.28899\n",
            "Epoch 596/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.28899\n",
            "Epoch 597/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.28899\n",
            "Epoch 598/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.28899\n",
            "Epoch 599/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.28899\n",
            "Epoch 600/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.28899\n",
            "Epoch 601/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.28899\n",
            "Epoch 602/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.28899\n",
            "Epoch 603/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.28899\n",
            "Epoch 604/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.28899\n",
            "Epoch 605/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.28899\n",
            "Epoch 606/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.28899\n",
            "Epoch 607/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.28899\n",
            "Epoch 608/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.28899\n",
            "Epoch 609/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.28899\n",
            "Epoch 610/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.28899\n",
            "Epoch 611/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.28899\n",
            "Epoch 612/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.28899\n",
            "Epoch 613/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.28899\n",
            "Epoch 614/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.28899\n",
            "Epoch 615/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.28899\n",
            "Epoch 616/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.28899\n",
            "Epoch 617/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.28899\n",
            "Epoch 618/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.28899\n",
            "Epoch 619/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.28899\n",
            "Epoch 620/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.28899\n",
            "Epoch 621/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.28899\n",
            "Epoch 622/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.28899\n",
            "Epoch 623/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.28899\n",
            "Epoch 624/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.28899\n",
            "Epoch 625/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.28899\n",
            "Epoch 626/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.28899\n",
            "Epoch 627/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.28899\n",
            "Epoch 628/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.28899\n",
            "Epoch 629/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.28899\n",
            "Epoch 630/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.28899\n",
            "Epoch 631/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.28899\n",
            "Epoch 632/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.28899\n",
            "Epoch 633/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.28899\n",
            "Epoch 634/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.28899\n",
            "Epoch 635/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.28899\n",
            "Epoch 636/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.28899\n",
            "Epoch 637/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.28899\n",
            "Epoch 638/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.28899\n",
            "Epoch 639/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.28899\n",
            "Epoch 640/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.28899\n",
            "Epoch 641/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.28899\n",
            "Epoch 642/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.28899\n",
            "Epoch 643/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.28899\n",
            "Epoch 644/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.28899\n",
            "Epoch 645/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.28899\n",
            "Epoch 646/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.28899\n",
            "Epoch 647/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.28899\n",
            "Epoch 648/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.28899\n",
            "Epoch 649/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.28899\n",
            "Epoch 650/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.28899\n",
            "Epoch 651/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.28899\n",
            "Epoch 652/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.28899\n",
            "Epoch 653/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.28899\n",
            "Epoch 654/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.28899\n",
            "Epoch 655/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.28899\n",
            "Epoch 656/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.28899\n",
            "Epoch 657/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.28899\n",
            "Epoch 658/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.28899\n",
            "Epoch 659/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.28899\n",
            "Epoch 660/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.28899\n",
            "Epoch 661/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.28899\n",
            "Epoch 662/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.28899\n",
            "Epoch 663/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.28899\n",
            "Epoch 664/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.28899\n",
            "Epoch 665/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.28899\n",
            "Epoch 666/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.28899\n",
            "Epoch 667/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.28899\n",
            "Epoch 668/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.28899\n",
            "Epoch 669/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.28899\n",
            "Epoch 670/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.28899\n",
            "Epoch 671/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.28899\n",
            "Epoch 672/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.28899\n",
            "Epoch 673/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.28899\n",
            "Epoch 674/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.28899\n",
            "Epoch 675/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.28899\n",
            "Epoch 676/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.28899\n",
            "Epoch 677/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.28899\n",
            "Epoch 678/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.28899\n",
            "Epoch 679/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.28899\n",
            "Epoch 680/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.28899\n",
            "Epoch 681/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.28899\n",
            "Epoch 682/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.28899\n",
            "Epoch 683/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.28899\n",
            "Epoch 684/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.28899\n",
            "Epoch 685/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.28899\n",
            "Epoch 686/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.28899\n",
            "Epoch 687/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.28899\n",
            "Epoch 688/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.28899\n",
            "Epoch 689/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.28899\n",
            "Epoch 690/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.28899\n",
            "Epoch 691/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.28899\n",
            "Epoch 692/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.28899\n",
            "Epoch 693/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.28899\n",
            "Epoch 694/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.28899\n",
            "Epoch 695/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.28899\n",
            "Epoch 696/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.28899\n",
            "Epoch 697/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.28899\n",
            "Epoch 698/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.28899\n",
            "Epoch 699/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.28899\n",
            "Epoch 700/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.28899\n",
            "Epoch 701/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.28899\n",
            "Epoch 702/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.28899\n",
            "Epoch 703/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.28899\n",
            "Epoch 704/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.28899\n",
            "Epoch 705/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.28899\n",
            "Epoch 706/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.28899\n",
            "Epoch 707/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.28899\n",
            "Epoch 708/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.28899\n",
            "Epoch 709/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.28899\n",
            "Epoch 710/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.28899\n",
            "Epoch 711/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.28899\n",
            "Epoch 712/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.28899\n",
            "Epoch 713/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.28899\n",
            "Epoch 714/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.28899\n",
            "Epoch 715/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.28899\n",
            "Epoch 716/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.28899\n",
            "Epoch 717/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.28899\n",
            "Epoch 718/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.28899\n",
            "Epoch 719/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.28899\n",
            "Epoch 720/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.28899\n",
            "Epoch 721/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.28899\n",
            "Epoch 722/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.28899\n",
            "Epoch 723/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.28899\n",
            "Epoch 724/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.28899\n",
            "Epoch 725/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.28899\n",
            "Epoch 726/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.28899\n",
            "Epoch 727/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.28899\n",
            "Epoch 728/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.28899\n",
            "Epoch 729/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.28899\n",
            "Epoch 730/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.28899\n",
            "Epoch 731/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.28899\n",
            "Epoch 732/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.28899\n",
            "Epoch 733/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.28899\n",
            "Epoch 734/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.28899\n",
            "Epoch 735/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.28899\n",
            "Epoch 736/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.28899\n",
            "Epoch 737/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.28899\n",
            "Epoch 738/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.28899\n",
            "Epoch 739/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.28899\n",
            "Epoch 740/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.28899\n",
            "Epoch 741/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.28899\n",
            "Epoch 742/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.28899\n",
            "Epoch 743/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.28899\n",
            "Epoch 744/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.28899\n",
            "Epoch 745/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.28899\n",
            "Epoch 746/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.28899\n",
            "Epoch 747/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.28899\n",
            "Epoch 748/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.28899\n",
            "Epoch 749/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.28899\n",
            "Epoch 750/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.28899\n",
            "Epoch 751/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.28899\n",
            "Epoch 752/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.28899\n",
            "Epoch 753/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.28899\n",
            "Epoch 754/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.28899\n",
            "Epoch 755/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.28899\n",
            "Epoch 756/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.28899\n",
            "Epoch 757/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.28899\n",
            "Epoch 758/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.28899\n",
            "Epoch 759/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.28899\n",
            "Epoch 760/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.28899\n",
            "Epoch 761/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.28899\n",
            "Epoch 762/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.28899\n",
            "Epoch 763/1000\n",
            "868/868 [==============================] - 0s 105us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.28899\n",
            "Epoch 764/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.28899\n",
            "Epoch 765/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.28899\n",
            "Epoch 766/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.28899\n",
            "Epoch 767/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.28899\n",
            "Epoch 768/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.28899\n",
            "Epoch 769/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.28899\n",
            "Epoch 770/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.28899\n",
            "Epoch 771/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.28899\n",
            "Epoch 772/1000\n",
            "868/868 [==============================] - 0s 123us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.28899\n",
            "Epoch 773/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.28899\n",
            "Epoch 774/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.28899\n",
            "Epoch 775/1000\n",
            "868/868 [==============================] - 0s 104us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.28899\n",
            "Epoch 776/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.28899\n",
            "Epoch 777/1000\n",
            "868/868 [==============================] - 0s 103us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.28899\n",
            "Epoch 778/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.28899\n",
            "Epoch 779/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 0.28899\n",
            "Epoch 780/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.28899\n",
            "Epoch 781/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 0.28899\n",
            "Epoch 782/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.28899\n",
            "Epoch 783/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.28899\n",
            "Epoch 784/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 0.28899\n",
            "Epoch 785/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.28899\n",
            "Epoch 786/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.28899\n",
            "Epoch 787/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.28899\n",
            "Epoch 788/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.28899\n",
            "Epoch 789/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.28899\n",
            "Epoch 790/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.28899\n",
            "Epoch 791/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.28899\n",
            "Epoch 792/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 0.28899\n",
            "Epoch 793/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.28899\n",
            "Epoch 794/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 0.28899\n",
            "Epoch 795/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.28899\n",
            "Epoch 796/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.28899\n",
            "Epoch 797/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.28899\n",
            "Epoch 798/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.28899\n",
            "Epoch 799/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.28899\n",
            "Epoch 800/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.28899\n",
            "Epoch 801/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.28899\n",
            "Epoch 802/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.28899\n",
            "Epoch 803/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.28899\n",
            "Epoch 804/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 0.28899\n",
            "Epoch 805/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.28899\n",
            "Epoch 806/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.28899\n",
            "Epoch 807/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.28899\n",
            "Epoch 808/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 0.28899\n",
            "Epoch 809/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.28899\n",
            "Epoch 810/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.28899\n",
            "Epoch 811/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.28899\n",
            "Epoch 812/1000\n",
            "868/868 [==============================] - 0s 134us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.28899\n",
            "Epoch 813/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.28899\n",
            "Epoch 814/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.28899\n",
            "Epoch 815/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.28899\n",
            "Epoch 816/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.28899\n",
            "Epoch 817/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.28899\n",
            "Epoch 818/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.28899\n",
            "Epoch 819/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 0.28899\n",
            "Epoch 820/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.28899\n",
            "Epoch 821/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.28899\n",
            "Epoch 822/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.28899\n",
            "Epoch 823/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.28899\n",
            "Epoch 824/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 0.28899\n",
            "Epoch 825/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.28899\n",
            "Epoch 826/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 0.28899\n",
            "Epoch 827/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.28899\n",
            "Epoch 828/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.28899\n",
            "Epoch 829/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.28899\n",
            "Epoch 830/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.28899\n",
            "Epoch 831/1000\n",
            "868/868 [==============================] - 0s 120us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.28899\n",
            "Epoch 832/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.28899\n",
            "Epoch 833/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.28899\n",
            "Epoch 834/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.28899\n",
            "Epoch 835/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.28899\n",
            "Epoch 836/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.28899\n",
            "Epoch 837/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.28899\n",
            "Epoch 838/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.28899\n",
            "Epoch 839/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.28899\n",
            "Epoch 840/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.28899\n",
            "Epoch 841/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 0.28899\n",
            "Epoch 842/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.28899\n",
            "Epoch 843/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.28899\n",
            "Epoch 844/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.28899\n",
            "Epoch 845/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.28899\n",
            "Epoch 846/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.28899\n",
            "Epoch 847/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.28899\n",
            "Epoch 848/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.28899\n",
            "Epoch 849/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.28899\n",
            "Epoch 850/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.28899\n",
            "Epoch 851/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 0.28899\n",
            "Epoch 852/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.28899\n",
            "Epoch 853/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.28899\n",
            "Epoch 854/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.28899\n",
            "Epoch 855/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.28899\n",
            "Epoch 856/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.28899\n",
            "Epoch 857/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.28899\n",
            "Epoch 858/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.28899\n",
            "Epoch 859/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.28899\n",
            "Epoch 860/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.28899\n",
            "Epoch 861/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.28899\n",
            "Epoch 862/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.28899\n",
            "Epoch 863/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.28899\n",
            "Epoch 864/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.28899\n",
            "Epoch 865/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.28899\n",
            "Epoch 866/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.28899\n",
            "Epoch 867/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.28899\n",
            "Epoch 868/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.28899\n",
            "Epoch 869/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.28899\n",
            "Epoch 870/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.28899\n",
            "Epoch 871/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.28899\n",
            "Epoch 872/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.28899\n",
            "Epoch 873/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.28899\n",
            "Epoch 874/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 0.28899\n",
            "Epoch 875/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.28899\n",
            "Epoch 876/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.28899\n",
            "Epoch 877/1000\n",
            "868/868 [==============================] - 0s 107us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.28899\n",
            "Epoch 878/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.28899\n",
            "Epoch 879/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.28899\n",
            "Epoch 880/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.28899\n",
            "Epoch 881/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.28899\n",
            "Epoch 882/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.28899\n",
            "Epoch 883/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.28899\n",
            "Epoch 884/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.28899\n",
            "Epoch 885/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 0.28899\n",
            "Epoch 886/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.28899\n",
            "Epoch 887/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 0.28899\n",
            "Epoch 888/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.28899\n",
            "Epoch 889/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.28899\n",
            "Epoch 890/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.28899\n",
            "Epoch 891/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.28899\n",
            "Epoch 892/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.28899\n",
            "Epoch 893/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.28899\n",
            "Epoch 894/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.28899\n",
            "Epoch 895/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.28899\n",
            "Epoch 896/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.28899\n",
            "Epoch 897/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.28899\n",
            "Epoch 898/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 0.28899\n",
            "Epoch 899/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.28899\n",
            "Epoch 900/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.28899\n",
            "Epoch 901/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 0.28899\n",
            "Epoch 902/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 0.28899\n",
            "Epoch 903/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.28899\n",
            "Epoch 904/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.28899\n",
            "Epoch 905/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.28899\n",
            "Epoch 906/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.28899\n",
            "Epoch 907/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.28899\n",
            "Epoch 908/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.28899\n",
            "Epoch 909/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.28899\n",
            "Epoch 910/1000\n",
            "868/868 [==============================] - 0s 125us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.28899\n",
            "Epoch 911/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.28899\n",
            "Epoch 912/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.28899\n",
            "Epoch 913/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.28899\n",
            "Epoch 914/1000\n",
            "868/868 [==============================] - 0s 126us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.28899\n",
            "Epoch 915/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.28899\n",
            "Epoch 916/1000\n",
            "868/868 [==============================] - 0s 132us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.28899\n",
            "Epoch 917/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.28899\n",
            "Epoch 918/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.28899\n",
            "Epoch 919/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.28899\n",
            "Epoch 920/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.28899\n",
            "Epoch 921/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.28899\n",
            "Epoch 922/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.28899\n",
            "Epoch 923/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.28899\n",
            "Epoch 924/1000\n",
            "868/868 [==============================] - 0s 127us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.28899\n",
            "Epoch 925/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.28899\n",
            "Epoch 926/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.28899\n",
            "Epoch 927/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 0.28899\n",
            "Epoch 928/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.28899\n",
            "Epoch 929/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.28899\n",
            "Epoch 930/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.28899\n",
            "Epoch 931/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.28899\n",
            "Epoch 932/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.28899\n",
            "Epoch 933/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.28899\n",
            "Epoch 934/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.28899\n",
            "Epoch 935/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.28899\n",
            "Epoch 936/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.28899\n",
            "Epoch 937/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.28899\n",
            "Epoch 938/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.28899\n",
            "Epoch 939/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.28899\n",
            "Epoch 940/1000\n",
            "868/868 [==============================] - 0s 143us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.28899\n",
            "Epoch 941/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 0.28899\n",
            "Epoch 942/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.28899\n",
            "Epoch 943/1000\n",
            "868/868 [==============================] - 0s 122us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.28899\n",
            "Epoch 944/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.28899\n",
            "Epoch 945/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.28899\n",
            "Epoch 946/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.28899\n",
            "Epoch 947/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.28899\n",
            "Epoch 948/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.28899\n",
            "Epoch 949/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.28899\n",
            "Epoch 950/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.28899\n",
            "Epoch 951/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.28899\n",
            "Epoch 952/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.28899\n",
            "Epoch 953/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.28899\n",
            "Epoch 954/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.28899\n",
            "Epoch 955/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.28899\n",
            "Epoch 956/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.28899\n",
            "Epoch 957/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.28899\n",
            "Epoch 958/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.28899\n",
            "Epoch 959/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.28899\n",
            "Epoch 960/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.28899\n",
            "Epoch 961/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.28899\n",
            "Epoch 962/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.28899\n",
            "Epoch 963/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.28899\n",
            "Epoch 964/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.28899\n",
            "Epoch 965/1000\n",
            "868/868 [==============================] - 0s 130us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.28899\n",
            "Epoch 966/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.28899\n",
            "Epoch 967/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.28899\n",
            "Epoch 968/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.28899\n",
            "Epoch 969/1000\n",
            "868/868 [==============================] - 0s 121us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.28899\n",
            "Epoch 970/1000\n",
            "868/868 [==============================] - 0s 118us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.28899\n",
            "Epoch 971/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.28899\n",
            "Epoch 972/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.28899\n",
            "Epoch 973/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.28899\n",
            "Epoch 974/1000\n",
            "868/868 [==============================] - 0s 115us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 0.28899\n",
            "Epoch 975/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.28899\n",
            "Epoch 976/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.28899\n",
            "Epoch 977/1000\n",
            "868/868 [==============================] - 0s 106us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.28899\n",
            "Epoch 978/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.28899\n",
            "Epoch 979/1000\n",
            "868/868 [==============================] - 0s 110us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.28899\n",
            "Epoch 980/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.28899\n",
            "Epoch 981/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.28899\n",
            "Epoch 982/1000\n",
            "868/868 [==============================] - 0s 111us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.28899\n",
            "Epoch 983/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.28899\n",
            "Epoch 984/1000\n",
            "868/868 [==============================] - 0s 124us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 0.28899\n",
            "Epoch 985/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 0.28899\n",
            "Epoch 986/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.28899\n",
            "Epoch 987/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.28899\n",
            "Epoch 988/1000\n",
            "868/868 [==============================] - 0s 114us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.28899\n",
            "Epoch 989/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.28899\n",
            "Epoch 990/1000\n",
            "868/868 [==============================] - 0s 112us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.28899\n",
            "Epoch 991/1000\n",
            "868/868 [==============================] - 0s 116us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.28899\n",
            "Epoch 992/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.28899\n",
            "Epoch 993/1000\n",
            "868/868 [==============================] - 0s 129us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.28899\n",
            "Epoch 994/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.28899\n",
            "Epoch 995/1000\n",
            "868/868 [==============================] - 0s 109us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.28899\n",
            "Epoch 996/1000\n",
            "868/868 [==============================] - 0s 113us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.28899\n",
            "Epoch 997/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.28899\n",
            "Epoch 998/1000\n",
            "868/868 [==============================] - 0s 119us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.28899\n",
            "Epoch 999/1000\n",
            "868/868 [==============================] - 0s 117us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.28899\n",
            "Epoch 1000/1000\n",
            "868/868 [==============================] - 0s 108us/step - loss: 0.3560 - mean_absolute_error: 0.3560 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.28899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f16315f7b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mUOEtB2dac"
      },
      "source": [
        "#NN_model.load_weights('weights.best.cnn.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi2v5_e2u3tt"
      },
      "source": [
        "#wights_file = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "#NN_model.load_weights(wights_file) \n",
        "#NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLFsCAkUu3oS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d390df4c-8d57-451f-df8c-f6d010ab5d07"
      },
      "source": [
        "DL_pred = model.predict(X_test)\n",
        "\n",
        "#Array of list of list to Array of list\n",
        "DLPred = []\n",
        "for i in DL_pred:\n",
        "  for k in i:\n",
        "    DLPred.append(int(k))\n",
        "  \n",
        "DLPred = np.array(DLPred) \n",
        "\n",
        "#PRINT\n",
        "#Y_test, Y_new\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, DLPred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[173   0]\n",
            " [ 99   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxsi0GnLrP3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6eb2ab-800d-4164-d158-ba1d101fb397"
      },
      "source": [
        "print(\"Accuracy score %f\" % accuracy_score(Y_test, DLPred))\n",
        "print(classification_report(Y_test, DLPred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score 0.636029\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      1.00      0.78       173\n",
            "           2       0.00      0.00      0.00        99\n",
            "\n",
            "    accuracy                           0.64       272\n",
            "   macro avg       0.32      0.50      0.39       272\n",
            "weighted avg       0.40      0.64      0.49       272\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKjPAUSOz-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d19632-2cec-4081-aa75-018cc9ab0bf4"
      },
      "source": [
        "NN_model.fit(X_test, Y_test, epochs=1000, batch_size=30, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 217 samples, validate on 55 samples\n",
            "Epoch 1/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00001: val_loss improved from 0.28899 to 0.27273, saving model to Weights-001--0.27273.hdf5\n",
            "Epoch 2/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.27273\n",
            "Epoch 3/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.27273\n",
            "Epoch 4/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.27273\n",
            "Epoch 5/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.27273\n",
            "Epoch 6/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.27273\n",
            "Epoch 7/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.27273\n",
            "Epoch 8/1000\n",
            "217/217 [==============================] - 0s 156us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.27273\n",
            "Epoch 9/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.27273\n",
            "Epoch 10/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.27273\n",
            "Epoch 11/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27273\n",
            "Epoch 12/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.27273\n",
            "Epoch 13/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.27273\n",
            "Epoch 14/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.27273\n",
            "Epoch 15/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.27273\n",
            "Epoch 16/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27273\n",
            "Epoch 17/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27273\n",
            "Epoch 18/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27273\n",
            "Epoch 19/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.27273\n",
            "Epoch 20/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27273\n",
            "Epoch 21/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.27273\n",
            "Epoch 22/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.27273\n",
            "Epoch 23/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.27273\n",
            "Epoch 24/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.27273\n",
            "Epoch 25/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.27273\n",
            "Epoch 26/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.27273\n",
            "Epoch 27/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.27273\n",
            "Epoch 28/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.27273\n",
            "Epoch 29/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.27273\n",
            "Epoch 30/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.27273\n",
            "Epoch 31/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.27273\n",
            "Epoch 32/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.27273\n",
            "Epoch 33/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.27273\n",
            "Epoch 34/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.27273\n",
            "Epoch 35/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.27273\n",
            "Epoch 36/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.27273\n",
            "Epoch 37/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.27273\n",
            "Epoch 38/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.27273\n",
            "Epoch 39/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.27273\n",
            "Epoch 40/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.27273\n",
            "Epoch 41/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.27273\n",
            "Epoch 42/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.27273\n",
            "Epoch 43/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.27273\n",
            "Epoch 44/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.27273\n",
            "Epoch 45/1000\n",
            "217/217 [==============================] - 0s 122us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.27273\n",
            "Epoch 46/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27273\n",
            "Epoch 47/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27273\n",
            "Epoch 48/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27273\n",
            "Epoch 49/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27273\n",
            "Epoch 50/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27273\n",
            "Epoch 51/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.27273\n",
            "Epoch 52/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.27273\n",
            "Epoch 53/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.27273\n",
            "Epoch 54/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.27273\n",
            "Epoch 55/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.27273\n",
            "Epoch 56/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.27273\n",
            "Epoch 57/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.27273\n",
            "Epoch 58/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.27273\n",
            "Epoch 59/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.27273\n",
            "Epoch 60/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.27273\n",
            "Epoch 61/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.27273\n",
            "Epoch 62/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.27273\n",
            "Epoch 63/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.27273\n",
            "Epoch 64/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.27273\n",
            "Epoch 65/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.27273\n",
            "Epoch 66/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.27273\n",
            "Epoch 67/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.27273\n",
            "Epoch 68/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.27273\n",
            "Epoch 69/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.27273\n",
            "Epoch 70/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.27273\n",
            "Epoch 71/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.27273\n",
            "Epoch 72/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.27273\n",
            "Epoch 73/1000\n",
            "217/217 [==============================] - 0s 202us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.27273\n",
            "Epoch 74/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.27273\n",
            "Epoch 75/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.27273\n",
            "Epoch 76/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.27273\n",
            "Epoch 77/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.27273\n",
            "Epoch 78/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.27273\n",
            "Epoch 79/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.27273\n",
            "Epoch 80/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.27273\n",
            "Epoch 81/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.27273\n",
            "Epoch 82/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.27273\n",
            "Epoch 83/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.27273\n",
            "Epoch 84/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.27273\n",
            "Epoch 85/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.27273\n",
            "Epoch 86/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.27273\n",
            "Epoch 87/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.27273\n",
            "Epoch 88/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.27273\n",
            "Epoch 89/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.27273\n",
            "Epoch 90/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.27273\n",
            "Epoch 91/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.27273\n",
            "Epoch 92/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.27273\n",
            "Epoch 93/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.27273\n",
            "Epoch 94/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.27273\n",
            "Epoch 95/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.27273\n",
            "Epoch 96/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.27273\n",
            "Epoch 97/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.27273\n",
            "Epoch 98/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.27273\n",
            "Epoch 99/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.27273\n",
            "Epoch 100/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.27273\n",
            "Epoch 101/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.27273\n",
            "Epoch 102/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.27273\n",
            "Epoch 103/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.27273\n",
            "Epoch 104/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.27273\n",
            "Epoch 105/1000\n",
            "217/217 [==============================] - 0s 175us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.27273\n",
            "Epoch 106/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.27273\n",
            "Epoch 107/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.27273\n",
            "Epoch 108/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.27273\n",
            "Epoch 109/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.27273\n",
            "Epoch 110/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.27273\n",
            "Epoch 111/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.27273\n",
            "Epoch 112/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.27273\n",
            "Epoch 113/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.27273\n",
            "Epoch 114/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.27273\n",
            "Epoch 115/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.27273\n",
            "Epoch 116/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.27273\n",
            "Epoch 117/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.27273\n",
            "Epoch 118/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.27273\n",
            "Epoch 119/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.27273\n",
            "Epoch 120/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.27273\n",
            "Epoch 121/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.27273\n",
            "Epoch 122/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.27273\n",
            "Epoch 123/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.27273\n",
            "Epoch 124/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.27273\n",
            "Epoch 125/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.27273\n",
            "Epoch 126/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.27273\n",
            "Epoch 127/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.27273\n",
            "Epoch 128/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.27273\n",
            "Epoch 129/1000\n",
            "217/217 [==============================] - 0s 183us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.27273\n",
            "Epoch 130/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.27273\n",
            "Epoch 131/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.27273\n",
            "Epoch 132/1000\n",
            "217/217 [==============================] - 0s 186us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.27273\n",
            "Epoch 133/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.27273\n",
            "Epoch 134/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.27273\n",
            "Epoch 135/1000\n",
            "217/217 [==============================] - 0s 167us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.27273\n",
            "Epoch 136/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.27273\n",
            "Epoch 137/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.27273\n",
            "Epoch 138/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.27273\n",
            "Epoch 139/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.27273\n",
            "Epoch 140/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.27273\n",
            "Epoch 141/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.27273\n",
            "Epoch 142/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.27273\n",
            "Epoch 143/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.27273\n",
            "Epoch 144/1000\n",
            "217/217 [==============================] - 0s 159us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.27273\n",
            "Epoch 145/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.27273\n",
            "Epoch 146/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.27273\n",
            "Epoch 147/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.27273\n",
            "Epoch 148/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.27273\n",
            "Epoch 149/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.27273\n",
            "Epoch 150/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.27273\n",
            "Epoch 151/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.27273\n",
            "Epoch 152/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.27273\n",
            "Epoch 153/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.27273\n",
            "Epoch 154/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.27273\n",
            "Epoch 155/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.27273\n",
            "Epoch 156/1000\n",
            "217/217 [==============================] - 0s 159us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.27273\n",
            "Epoch 157/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.27273\n",
            "Epoch 158/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.27273\n",
            "Epoch 159/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.27273\n",
            "Epoch 160/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.27273\n",
            "Epoch 161/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.27273\n",
            "Epoch 162/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.27273\n",
            "Epoch 163/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.27273\n",
            "Epoch 164/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.27273\n",
            "Epoch 165/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.27273\n",
            "Epoch 166/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.27273\n",
            "Epoch 167/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.27273\n",
            "Epoch 168/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.27273\n",
            "Epoch 169/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.27273\n",
            "Epoch 170/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.27273\n",
            "Epoch 171/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.27273\n",
            "Epoch 172/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.27273\n",
            "Epoch 173/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.27273\n",
            "Epoch 174/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.27273\n",
            "Epoch 175/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.27273\n",
            "Epoch 176/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.27273\n",
            "Epoch 177/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.27273\n",
            "Epoch 178/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.27273\n",
            "Epoch 179/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.27273\n",
            "Epoch 180/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.27273\n",
            "Epoch 181/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.27273\n",
            "Epoch 182/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.27273\n",
            "Epoch 183/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.27273\n",
            "Epoch 184/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.27273\n",
            "Epoch 185/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.27273\n",
            "Epoch 186/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.27273\n",
            "Epoch 187/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.27273\n",
            "Epoch 188/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.27273\n",
            "Epoch 189/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.27273\n",
            "Epoch 190/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.27273\n",
            "Epoch 191/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.27273\n",
            "Epoch 192/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.27273\n",
            "Epoch 193/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.27273\n",
            "Epoch 194/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.27273\n",
            "Epoch 195/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.27273\n",
            "Epoch 196/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.27273\n",
            "Epoch 197/1000\n",
            "217/217 [==============================] - 0s 172us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.27273\n",
            "Epoch 198/1000\n",
            "217/217 [==============================] - 0s 168us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.27273\n",
            "Epoch 199/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.27273\n",
            "Epoch 200/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.27273\n",
            "Epoch 201/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.27273\n",
            "Epoch 202/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.27273\n",
            "Epoch 203/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.27273\n",
            "Epoch 204/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.27273\n",
            "Epoch 205/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.27273\n",
            "Epoch 206/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.27273\n",
            "Epoch 207/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.27273\n",
            "Epoch 208/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.27273\n",
            "Epoch 209/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.27273\n",
            "Epoch 210/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.27273\n",
            "Epoch 211/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.27273\n",
            "Epoch 212/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.27273\n",
            "Epoch 213/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.27273\n",
            "Epoch 214/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.27273\n",
            "Epoch 215/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.27273\n",
            "Epoch 216/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.27273\n",
            "Epoch 217/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.27273\n",
            "Epoch 218/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.27273\n",
            "Epoch 219/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.27273\n",
            "Epoch 220/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.27273\n",
            "Epoch 221/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.27273\n",
            "Epoch 222/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.27273\n",
            "Epoch 223/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.27273\n",
            "Epoch 224/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.27273\n",
            "Epoch 225/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.27273\n",
            "Epoch 226/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.27273\n",
            "Epoch 227/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.27273\n",
            "Epoch 228/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.27273\n",
            "Epoch 229/1000\n",
            "217/217 [==============================] - 0s 162us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.27273\n",
            "Epoch 230/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.27273\n",
            "Epoch 231/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.27273\n",
            "Epoch 232/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.27273\n",
            "Epoch 233/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.27273\n",
            "Epoch 234/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.27273\n",
            "Epoch 235/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.27273\n",
            "Epoch 236/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.27273\n",
            "Epoch 237/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.27273\n",
            "Epoch 238/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.27273\n",
            "Epoch 239/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.27273\n",
            "Epoch 240/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.27273\n",
            "Epoch 241/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.27273\n",
            "Epoch 242/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.27273\n",
            "Epoch 243/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.27273\n",
            "Epoch 244/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.27273\n",
            "Epoch 245/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.27273\n",
            "Epoch 246/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.27273\n",
            "Epoch 247/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.27273\n",
            "Epoch 248/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.27273\n",
            "Epoch 249/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.27273\n",
            "Epoch 250/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.27273\n",
            "Epoch 251/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.27273\n",
            "Epoch 252/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.27273\n",
            "Epoch 253/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.27273\n",
            "Epoch 254/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.27273\n",
            "Epoch 255/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.27273\n",
            "Epoch 256/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.27273\n",
            "Epoch 257/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.27273\n",
            "Epoch 258/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.27273\n",
            "Epoch 259/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.27273\n",
            "Epoch 260/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.27273\n",
            "Epoch 261/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.27273\n",
            "Epoch 262/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.27273\n",
            "Epoch 263/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.27273\n",
            "Epoch 264/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.27273\n",
            "Epoch 265/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.27273\n",
            "Epoch 266/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.27273\n",
            "Epoch 267/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.27273\n",
            "Epoch 268/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.27273\n",
            "Epoch 269/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.27273\n",
            "Epoch 270/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.27273\n",
            "Epoch 271/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.27273\n",
            "Epoch 272/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.27273\n",
            "Epoch 273/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.27273\n",
            "Epoch 274/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.27273\n",
            "Epoch 275/1000\n",
            "217/217 [==============================] - 0s 156us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.27273\n",
            "Epoch 276/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.27273\n",
            "Epoch 277/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.27273\n",
            "Epoch 278/1000\n",
            "217/217 [==============================] - 0s 121us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.27273\n",
            "Epoch 279/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.27273\n",
            "Epoch 280/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.27273\n",
            "Epoch 281/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.27273\n",
            "Epoch 282/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.27273\n",
            "Epoch 283/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.27273\n",
            "Epoch 284/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.27273\n",
            "Epoch 285/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.27273\n",
            "Epoch 286/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.27273\n",
            "Epoch 287/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.27273\n",
            "Epoch 288/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.27273\n",
            "Epoch 289/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.27273\n",
            "Epoch 290/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.27273\n",
            "Epoch 291/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.27273\n",
            "Epoch 292/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.27273\n",
            "Epoch 293/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.27273\n",
            "Epoch 294/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.27273\n",
            "Epoch 295/1000\n",
            "217/217 [==============================] - 0s 165us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.27273\n",
            "Epoch 296/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.27273\n",
            "Epoch 297/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.27273\n",
            "Epoch 298/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.27273\n",
            "Epoch 299/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.27273\n",
            "Epoch 300/1000\n",
            "217/217 [==============================] - 0s 122us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.27273\n",
            "Epoch 301/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.27273\n",
            "Epoch 302/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.27273\n",
            "Epoch 303/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.27273\n",
            "Epoch 304/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.27273\n",
            "Epoch 305/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.27273\n",
            "Epoch 306/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.27273\n",
            "Epoch 307/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.27273\n",
            "Epoch 308/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.27273\n",
            "Epoch 309/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.27273\n",
            "Epoch 310/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.27273\n",
            "Epoch 311/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.27273\n",
            "Epoch 312/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.27273\n",
            "Epoch 313/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.27273\n",
            "Epoch 314/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.27273\n",
            "Epoch 315/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.27273\n",
            "Epoch 316/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.27273\n",
            "Epoch 317/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.27273\n",
            "Epoch 318/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.27273\n",
            "Epoch 319/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.27273\n",
            "Epoch 320/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.27273\n",
            "Epoch 321/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.27273\n",
            "Epoch 322/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.27273\n",
            "Epoch 323/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.27273\n",
            "Epoch 324/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.27273\n",
            "Epoch 325/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.27273\n",
            "Epoch 326/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.27273\n",
            "Epoch 327/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.27273\n",
            "Epoch 328/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.27273\n",
            "Epoch 329/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.27273\n",
            "Epoch 330/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.27273\n",
            "Epoch 331/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.27273\n",
            "Epoch 332/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.27273\n",
            "Epoch 333/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.27273\n",
            "Epoch 334/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.27273\n",
            "Epoch 335/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.27273\n",
            "Epoch 336/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.27273\n",
            "Epoch 337/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.27273\n",
            "Epoch 338/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.27273\n",
            "Epoch 339/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.27273\n",
            "Epoch 340/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.27273\n",
            "Epoch 341/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.27273\n",
            "Epoch 342/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.27273\n",
            "Epoch 343/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.27273\n",
            "Epoch 344/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.27273\n",
            "Epoch 345/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.27273\n",
            "Epoch 346/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.27273\n",
            "Epoch 347/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.27273\n",
            "Epoch 348/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.27273\n",
            "Epoch 349/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.27273\n",
            "Epoch 350/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.27273\n",
            "Epoch 351/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.27273\n",
            "Epoch 352/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.27273\n",
            "Epoch 353/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.27273\n",
            "Epoch 354/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.27273\n",
            "Epoch 355/1000\n",
            "217/217 [==============================] - 0s 122us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.27273\n",
            "Epoch 356/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.27273\n",
            "Epoch 357/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.27273\n",
            "Epoch 358/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.27273\n",
            "Epoch 359/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.27273\n",
            "Epoch 360/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.27273\n",
            "Epoch 361/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.27273\n",
            "Epoch 362/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.27273\n",
            "Epoch 363/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.27273\n",
            "Epoch 364/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.27273\n",
            "Epoch 365/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.27273\n",
            "Epoch 366/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.27273\n",
            "Epoch 367/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.27273\n",
            "Epoch 368/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.27273\n",
            "Epoch 369/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.27273\n",
            "Epoch 370/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.27273\n",
            "Epoch 371/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.27273\n",
            "Epoch 372/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.27273\n",
            "Epoch 373/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.27273\n",
            "Epoch 374/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.27273\n",
            "Epoch 375/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.27273\n",
            "Epoch 376/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.27273\n",
            "Epoch 377/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.27273\n",
            "Epoch 378/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.27273\n",
            "Epoch 379/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.27273\n",
            "Epoch 380/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.27273\n",
            "Epoch 381/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.27273\n",
            "Epoch 382/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.27273\n",
            "Epoch 383/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.27273\n",
            "Epoch 384/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.27273\n",
            "Epoch 385/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.27273\n",
            "Epoch 386/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.27273\n",
            "Epoch 387/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.27273\n",
            "Epoch 388/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.27273\n",
            "Epoch 389/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.27273\n",
            "Epoch 390/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.27273\n",
            "Epoch 391/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.27273\n",
            "Epoch 392/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.27273\n",
            "Epoch 393/1000\n",
            "217/217 [==============================] - 0s 172us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.27273\n",
            "Epoch 394/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.27273\n",
            "Epoch 395/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.27273\n",
            "Epoch 396/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.27273\n",
            "Epoch 397/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.27273\n",
            "Epoch 398/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.27273\n",
            "Epoch 399/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.27273\n",
            "Epoch 400/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.27273\n",
            "Epoch 401/1000\n",
            "217/217 [==============================] - 0s 164us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.27273\n",
            "Epoch 402/1000\n",
            "217/217 [==============================] - 0s 187us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.27273\n",
            "Epoch 403/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.27273\n",
            "Epoch 404/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.27273\n",
            "Epoch 405/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.27273\n",
            "Epoch 406/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.27273\n",
            "Epoch 407/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.27273\n",
            "Epoch 408/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.27273\n",
            "Epoch 409/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.27273\n",
            "Epoch 410/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.27273\n",
            "Epoch 411/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.27273\n",
            "Epoch 412/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.27273\n",
            "Epoch 413/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.27273\n",
            "Epoch 414/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.27273\n",
            "Epoch 415/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.27273\n",
            "Epoch 416/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.27273\n",
            "Epoch 417/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.27273\n",
            "Epoch 418/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.27273\n",
            "Epoch 419/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.27273\n",
            "Epoch 420/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.27273\n",
            "Epoch 421/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.27273\n",
            "Epoch 422/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.27273\n",
            "Epoch 423/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.27273\n",
            "Epoch 424/1000\n",
            "217/217 [==============================] - 0s 173us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.27273\n",
            "Epoch 425/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.27273\n",
            "Epoch 426/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.27273\n",
            "Epoch 427/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.27273\n",
            "Epoch 428/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.27273\n",
            "Epoch 429/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.27273\n",
            "Epoch 430/1000\n",
            "217/217 [==============================] - 0s 159us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.27273\n",
            "Epoch 431/1000\n",
            "217/217 [==============================] - 0s 169us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.27273\n",
            "Epoch 432/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.27273\n",
            "Epoch 433/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.27273\n",
            "Epoch 434/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.27273\n",
            "Epoch 435/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.27273\n",
            "Epoch 436/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.27273\n",
            "Epoch 437/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.27273\n",
            "Epoch 438/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.27273\n",
            "Epoch 439/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.27273\n",
            "Epoch 440/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.27273\n",
            "Epoch 441/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.27273\n",
            "Epoch 442/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.27273\n",
            "Epoch 443/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.27273\n",
            "Epoch 444/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.27273\n",
            "Epoch 445/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.27273\n",
            "Epoch 446/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.27273\n",
            "Epoch 447/1000\n",
            "217/217 [==============================] - 0s 178us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.27273\n",
            "Epoch 448/1000\n",
            "217/217 [==============================] - 0s 156us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.27273\n",
            "Epoch 449/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.27273\n",
            "Epoch 450/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.27273\n",
            "Epoch 451/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.27273\n",
            "Epoch 452/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.27273\n",
            "Epoch 453/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.27273\n",
            "Epoch 454/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.27273\n",
            "Epoch 455/1000\n",
            "217/217 [==============================] - 0s 165us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.27273\n",
            "Epoch 456/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.27273\n",
            "Epoch 457/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.27273\n",
            "Epoch 458/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.27273\n",
            "Epoch 459/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.27273\n",
            "Epoch 460/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.27273\n",
            "Epoch 461/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.27273\n",
            "Epoch 462/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.27273\n",
            "Epoch 463/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.27273\n",
            "Epoch 464/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.27273\n",
            "Epoch 465/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.27273\n",
            "Epoch 466/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.27273\n",
            "Epoch 467/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.27273\n",
            "Epoch 468/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.27273\n",
            "Epoch 469/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.27273\n",
            "Epoch 470/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.27273\n",
            "Epoch 471/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.27273\n",
            "Epoch 472/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.27273\n",
            "Epoch 473/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.27273\n",
            "Epoch 474/1000\n",
            "217/217 [==============================] - 0s 159us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.27273\n",
            "Epoch 475/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.27273\n",
            "Epoch 476/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.27273\n",
            "Epoch 477/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.27273\n",
            "Epoch 478/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.27273\n",
            "Epoch 479/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.27273\n",
            "Epoch 480/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.27273\n",
            "Epoch 481/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.27273\n",
            "Epoch 482/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.27273\n",
            "Epoch 483/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.27273\n",
            "Epoch 484/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.27273\n",
            "Epoch 485/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.27273\n",
            "Epoch 486/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.27273\n",
            "Epoch 487/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.27273\n",
            "Epoch 488/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.27273\n",
            "Epoch 489/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.27273\n",
            "Epoch 490/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.27273\n",
            "Epoch 491/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.27273\n",
            "Epoch 492/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.27273\n",
            "Epoch 493/1000\n",
            "217/217 [==============================] - 0s 162us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.27273\n",
            "Epoch 494/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.27273\n",
            "Epoch 495/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.27273\n",
            "Epoch 496/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.27273\n",
            "Epoch 497/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.27273\n",
            "Epoch 498/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.27273\n",
            "Epoch 499/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.27273\n",
            "Epoch 500/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.27273\n",
            "Epoch 501/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.27273\n",
            "Epoch 502/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.27273\n",
            "Epoch 503/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.27273\n",
            "Epoch 504/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.27273\n",
            "Epoch 505/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.27273\n",
            "Epoch 506/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.27273\n",
            "Epoch 507/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.27273\n",
            "Epoch 508/1000\n",
            "217/217 [==============================] - 0s 118us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.27273\n",
            "Epoch 509/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.27273\n",
            "Epoch 510/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.27273\n",
            "Epoch 511/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.27273\n",
            "Epoch 512/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.27273\n",
            "Epoch 513/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.27273\n",
            "Epoch 514/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.27273\n",
            "Epoch 515/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.27273\n",
            "Epoch 516/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.27273\n",
            "Epoch 517/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.27273\n",
            "Epoch 518/1000\n",
            "217/217 [==============================] - 0s 119us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.27273\n",
            "Epoch 519/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.27273\n",
            "Epoch 520/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.27273\n",
            "Epoch 521/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.27273\n",
            "Epoch 522/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.27273\n",
            "Epoch 523/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.27273\n",
            "Epoch 524/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.27273\n",
            "Epoch 525/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.27273\n",
            "Epoch 526/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.27273\n",
            "Epoch 527/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.27273\n",
            "Epoch 528/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.27273\n",
            "Epoch 529/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.27273\n",
            "Epoch 530/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.27273\n",
            "Epoch 531/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.27273\n",
            "Epoch 532/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.27273\n",
            "Epoch 533/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.27273\n",
            "Epoch 534/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.27273\n",
            "Epoch 535/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.27273\n",
            "Epoch 536/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.27273\n",
            "Epoch 537/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.27273\n",
            "Epoch 538/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.27273\n",
            "Epoch 539/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.27273\n",
            "Epoch 540/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.27273\n",
            "Epoch 541/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.27273\n",
            "Epoch 542/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.27273\n",
            "Epoch 543/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.27273\n",
            "Epoch 544/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.27273\n",
            "Epoch 545/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.27273\n",
            "Epoch 546/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.27273\n",
            "Epoch 547/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.27273\n",
            "Epoch 548/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.27273\n",
            "Epoch 549/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.27273\n",
            "Epoch 550/1000\n",
            "217/217 [==============================] - 0s 161us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.27273\n",
            "Epoch 551/1000\n",
            "217/217 [==============================] - 0s 179us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.27273\n",
            "Epoch 552/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.27273\n",
            "Epoch 553/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.27273\n",
            "Epoch 554/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.27273\n",
            "Epoch 555/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.27273\n",
            "Epoch 556/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.27273\n",
            "Epoch 557/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.27273\n",
            "Epoch 558/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.27273\n",
            "Epoch 559/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.27273\n",
            "Epoch 560/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.27273\n",
            "Epoch 561/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.27273\n",
            "Epoch 562/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.27273\n",
            "Epoch 563/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.27273\n",
            "Epoch 564/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.27273\n",
            "Epoch 565/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.27273\n",
            "Epoch 566/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.27273\n",
            "Epoch 567/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.27273\n",
            "Epoch 568/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.27273\n",
            "Epoch 569/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.27273\n",
            "Epoch 570/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.27273\n",
            "Epoch 571/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.27273\n",
            "Epoch 572/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.27273\n",
            "Epoch 573/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.27273\n",
            "Epoch 574/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.27273\n",
            "Epoch 575/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.27273\n",
            "Epoch 576/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.27273\n",
            "Epoch 577/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.27273\n",
            "Epoch 578/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.27273\n",
            "Epoch 579/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.27273\n",
            "Epoch 580/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.27273\n",
            "Epoch 581/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.27273\n",
            "Epoch 582/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.27273\n",
            "Epoch 583/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.27273\n",
            "Epoch 584/1000\n",
            "217/217 [==============================] - 0s 164us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.27273\n",
            "Epoch 585/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.27273\n",
            "Epoch 586/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.27273\n",
            "Epoch 587/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.27273\n",
            "Epoch 588/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.27273\n",
            "Epoch 589/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.27273\n",
            "Epoch 590/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.27273\n",
            "Epoch 591/1000\n",
            "217/217 [==============================] - 0s 168us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.27273\n",
            "Epoch 592/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.27273\n",
            "Epoch 593/1000\n",
            "217/217 [==============================] - 0s 176us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.27273\n",
            "Epoch 594/1000\n",
            "217/217 [==============================] - 0s 179us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.27273\n",
            "Epoch 595/1000\n",
            "217/217 [==============================] - 0s 172us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.27273\n",
            "Epoch 596/1000\n",
            "217/217 [==============================] - 0s 176us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.27273\n",
            "Epoch 597/1000\n",
            "217/217 [==============================] - 0s 179us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.27273\n",
            "Epoch 598/1000\n",
            "217/217 [==============================] - 0s 177us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.27273\n",
            "Epoch 599/1000\n",
            "217/217 [==============================] - 0s 164us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.27273\n",
            "Epoch 600/1000\n",
            "217/217 [==============================] - 0s 204us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.27273\n",
            "Epoch 601/1000\n",
            "217/217 [==============================] - 0s 216us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.27273\n",
            "Epoch 602/1000\n",
            "217/217 [==============================] - 0s 166us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.27273\n",
            "Epoch 603/1000\n",
            "217/217 [==============================] - 0s 194us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.27273\n",
            "Epoch 604/1000\n",
            "217/217 [==============================] - 0s 182us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.27273\n",
            "Epoch 605/1000\n",
            "217/217 [==============================] - 0s 185us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.27273\n",
            "Epoch 606/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.27273\n",
            "Epoch 607/1000\n",
            "217/217 [==============================] - 0s 120us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.27273\n",
            "Epoch 608/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.27273\n",
            "Epoch 609/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.27273\n",
            "Epoch 610/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.27273\n",
            "Epoch 611/1000\n",
            "217/217 [==============================] - 0s 167us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.27273\n",
            "Epoch 612/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.27273\n",
            "Epoch 613/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.27273\n",
            "Epoch 614/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.27273\n",
            "Epoch 615/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.27273\n",
            "Epoch 616/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.27273\n",
            "Epoch 617/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.27273\n",
            "Epoch 618/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.27273\n",
            "Epoch 619/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.27273\n",
            "Epoch 620/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.27273\n",
            "Epoch 621/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.27273\n",
            "Epoch 622/1000\n",
            "217/217 [==============================] - 0s 154us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.27273\n",
            "Epoch 623/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.27273\n",
            "Epoch 624/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.27273\n",
            "Epoch 625/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.27273\n",
            "Epoch 626/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.27273\n",
            "Epoch 627/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.27273\n",
            "Epoch 628/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.27273\n",
            "Epoch 629/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.27273\n",
            "Epoch 630/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.27273\n",
            "Epoch 631/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.27273\n",
            "Epoch 632/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.27273\n",
            "Epoch 633/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.27273\n",
            "Epoch 634/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.27273\n",
            "Epoch 635/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.27273\n",
            "Epoch 636/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.27273\n",
            "Epoch 637/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.27273\n",
            "Epoch 638/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.27273\n",
            "Epoch 639/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.27273\n",
            "Epoch 640/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.27273\n",
            "Epoch 641/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.27273\n",
            "Epoch 642/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.27273\n",
            "Epoch 643/1000\n",
            "217/217 [==============================] - 0s 201us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.27273\n",
            "Epoch 644/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.27273\n",
            "Epoch 645/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.27273\n",
            "Epoch 646/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.27273\n",
            "Epoch 647/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.27273\n",
            "Epoch 648/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.27273\n",
            "Epoch 649/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.27273\n",
            "Epoch 650/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.27273\n",
            "Epoch 651/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.27273\n",
            "Epoch 652/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.27273\n",
            "Epoch 653/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.27273\n",
            "Epoch 654/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.27273\n",
            "Epoch 655/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.27273\n",
            "Epoch 656/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.27273\n",
            "Epoch 657/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.27273\n",
            "Epoch 658/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.27273\n",
            "Epoch 659/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.27273\n",
            "Epoch 660/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.27273\n",
            "Epoch 661/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.27273\n",
            "Epoch 662/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.27273\n",
            "Epoch 663/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.27273\n",
            "Epoch 664/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.27273\n",
            "Epoch 665/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.27273\n",
            "Epoch 666/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.27273\n",
            "Epoch 667/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.27273\n",
            "Epoch 668/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.27273\n",
            "Epoch 669/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.27273\n",
            "Epoch 670/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.27273\n",
            "Epoch 671/1000\n",
            "217/217 [==============================] - 0s 164us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.27273\n",
            "Epoch 672/1000\n",
            "217/217 [==============================] - 0s 161us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.27273\n",
            "Epoch 673/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.27273\n",
            "Epoch 674/1000\n",
            "217/217 [==============================] - 0s 165us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.27273\n",
            "Epoch 675/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.27273\n",
            "Epoch 676/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.27273\n",
            "Epoch 677/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.27273\n",
            "Epoch 678/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.27273\n",
            "Epoch 679/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.27273\n",
            "Epoch 680/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.27273\n",
            "Epoch 681/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.27273\n",
            "Epoch 682/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.27273\n",
            "Epoch 683/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.27273\n",
            "Epoch 684/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.27273\n",
            "Epoch 685/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.27273\n",
            "Epoch 686/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.27273\n",
            "Epoch 687/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.27273\n",
            "Epoch 688/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.27273\n",
            "Epoch 689/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.27273\n",
            "Epoch 690/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.27273\n",
            "Epoch 691/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.27273\n",
            "Epoch 692/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.27273\n",
            "Epoch 693/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.27273\n",
            "Epoch 694/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.27273\n",
            "Epoch 695/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.27273\n",
            "Epoch 696/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.27273\n",
            "Epoch 697/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.27273\n",
            "Epoch 698/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.27273\n",
            "Epoch 699/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.27273\n",
            "Epoch 700/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.27273\n",
            "Epoch 701/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.27273\n",
            "Epoch 702/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.27273\n",
            "Epoch 703/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.27273\n",
            "Epoch 704/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.27273\n",
            "Epoch 705/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.27273\n",
            "Epoch 706/1000\n",
            "217/217 [==============================] - 0s 185us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.27273\n",
            "Epoch 707/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.27273\n",
            "Epoch 708/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.27273\n",
            "Epoch 709/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.27273\n",
            "Epoch 710/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.27273\n",
            "Epoch 711/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.27273\n",
            "Epoch 712/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.27273\n",
            "Epoch 713/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.27273\n",
            "Epoch 714/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.27273\n",
            "Epoch 715/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.27273\n",
            "Epoch 716/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.27273\n",
            "Epoch 717/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.27273\n",
            "Epoch 718/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.27273\n",
            "Epoch 719/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.27273\n",
            "Epoch 720/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.27273\n",
            "Epoch 721/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.27273\n",
            "Epoch 722/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.27273\n",
            "Epoch 723/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.27273\n",
            "Epoch 724/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.27273\n",
            "Epoch 725/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.27273\n",
            "Epoch 726/1000\n",
            "217/217 [==============================] - 0s 183us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.27273\n",
            "Epoch 727/1000\n",
            "217/217 [==============================] - 0s 178us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.27273\n",
            "Epoch 728/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.27273\n",
            "Epoch 729/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.27273\n",
            "Epoch 730/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.27273\n",
            "Epoch 731/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.27273\n",
            "Epoch 732/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.27273\n",
            "Epoch 733/1000\n",
            "217/217 [==============================] - 0s 179us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.27273\n",
            "Epoch 734/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.27273\n",
            "Epoch 735/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.27273\n",
            "Epoch 736/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.27273\n",
            "Epoch 737/1000\n",
            "217/217 [==============================] - 0s 177us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.27273\n",
            "Epoch 738/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.27273\n",
            "Epoch 739/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.27273\n",
            "Epoch 740/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.27273\n",
            "Epoch 741/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.27273\n",
            "Epoch 742/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.27273\n",
            "Epoch 743/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.27273\n",
            "Epoch 744/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.27273\n",
            "Epoch 745/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.27273\n",
            "Epoch 746/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.27273\n",
            "Epoch 747/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.27273\n",
            "Epoch 748/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.27273\n",
            "Epoch 749/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.27273\n",
            "Epoch 750/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.27273\n",
            "Epoch 751/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.27273\n",
            "Epoch 752/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.27273\n",
            "Epoch 753/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.27273\n",
            "Epoch 754/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.27273\n",
            "Epoch 755/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.27273\n",
            "Epoch 756/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.27273\n",
            "Epoch 757/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.27273\n",
            "Epoch 758/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.27273\n",
            "Epoch 759/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.27273\n",
            "Epoch 760/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.27273\n",
            "Epoch 761/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.27273\n",
            "Epoch 762/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.27273\n",
            "Epoch 763/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.27273\n",
            "Epoch 764/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.27273\n",
            "Epoch 765/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.27273\n",
            "Epoch 766/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.27273\n",
            "Epoch 767/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.27273\n",
            "Epoch 768/1000\n",
            "217/217 [==============================] - 0s 153us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.27273\n",
            "Epoch 769/1000\n",
            "217/217 [==============================] - 0s 166us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.27273\n",
            "Epoch 770/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.27273\n",
            "Epoch 771/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.27273\n",
            "Epoch 772/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.27273\n",
            "Epoch 773/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.27273\n",
            "Epoch 774/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.27273\n",
            "Epoch 775/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.27273\n",
            "Epoch 776/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.27273\n",
            "Epoch 777/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.27273\n",
            "Epoch 778/1000\n",
            "217/217 [==============================] - 0s 152us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.27273\n",
            "Epoch 779/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 0.27273\n",
            "Epoch 780/1000\n",
            "217/217 [==============================] - 0s 156us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.27273\n",
            "Epoch 781/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 0.27273\n",
            "Epoch 782/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.27273\n",
            "Epoch 783/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.27273\n",
            "Epoch 784/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 0.27273\n",
            "Epoch 785/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.27273\n",
            "Epoch 786/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.27273\n",
            "Epoch 787/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.27273\n",
            "Epoch 788/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.27273\n",
            "Epoch 789/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.27273\n",
            "Epoch 790/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.27273\n",
            "Epoch 791/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.27273\n",
            "Epoch 792/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 0.27273\n",
            "Epoch 793/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.27273\n",
            "Epoch 794/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 0.27273\n",
            "Epoch 795/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.27273\n",
            "Epoch 796/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.27273\n",
            "Epoch 797/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.27273\n",
            "Epoch 798/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.27273\n",
            "Epoch 799/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.27273\n",
            "Epoch 800/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.27273\n",
            "Epoch 801/1000\n",
            "217/217 [==============================] - 0s 161us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.27273\n",
            "Epoch 802/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.27273\n",
            "Epoch 803/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.27273\n",
            "Epoch 804/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 0.27273\n",
            "Epoch 805/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.27273\n",
            "Epoch 806/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.27273\n",
            "Epoch 807/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.27273\n",
            "Epoch 808/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 0.27273\n",
            "Epoch 809/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.27273\n",
            "Epoch 810/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.27273\n",
            "Epoch 811/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.27273\n",
            "Epoch 812/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.27273\n",
            "Epoch 813/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.27273\n",
            "Epoch 814/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.27273\n",
            "Epoch 815/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.27273\n",
            "Epoch 816/1000\n",
            "217/217 [==============================] - 0s 151us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.27273\n",
            "Epoch 817/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.27273\n",
            "Epoch 818/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.27273\n",
            "Epoch 819/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 0.27273\n",
            "Epoch 820/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.27273\n",
            "Epoch 821/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.27273\n",
            "Epoch 822/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.27273\n",
            "Epoch 823/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.27273\n",
            "Epoch 824/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 0.27273\n",
            "Epoch 825/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.27273\n",
            "Epoch 826/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 0.27273\n",
            "Epoch 827/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.27273\n",
            "Epoch 828/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.27273\n",
            "Epoch 829/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.27273\n",
            "Epoch 830/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.27273\n",
            "Epoch 831/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.27273\n",
            "Epoch 832/1000\n",
            "217/217 [==============================] - 0s 167us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.27273\n",
            "Epoch 833/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.27273\n",
            "Epoch 834/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.27273\n",
            "Epoch 835/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.27273\n",
            "Epoch 836/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.27273\n",
            "Epoch 837/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.27273\n",
            "Epoch 838/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.27273\n",
            "Epoch 839/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.27273\n",
            "Epoch 840/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.27273\n",
            "Epoch 841/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 0.27273\n",
            "Epoch 842/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.27273\n",
            "Epoch 843/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.27273\n",
            "Epoch 844/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.27273\n",
            "Epoch 845/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.27273\n",
            "Epoch 846/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.27273\n",
            "Epoch 847/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.27273\n",
            "Epoch 848/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.27273\n",
            "Epoch 849/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.27273\n",
            "Epoch 850/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.27273\n",
            "Epoch 851/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 0.27273\n",
            "Epoch 852/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.27273\n",
            "Epoch 853/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.27273\n",
            "Epoch 854/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.27273\n",
            "Epoch 855/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.27273\n",
            "Epoch 856/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.27273\n",
            "Epoch 857/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.27273\n",
            "Epoch 858/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.27273\n",
            "Epoch 859/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.27273\n",
            "Epoch 860/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.27273\n",
            "Epoch 861/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.27273\n",
            "Epoch 862/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.27273\n",
            "Epoch 863/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.27273\n",
            "Epoch 864/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.27273\n",
            "Epoch 865/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.27273\n",
            "Epoch 866/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.27273\n",
            "Epoch 867/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.27273\n",
            "Epoch 868/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.27273\n",
            "Epoch 869/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.27273\n",
            "Epoch 870/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.27273\n",
            "Epoch 871/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.27273\n",
            "Epoch 872/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.27273\n",
            "Epoch 873/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.27273\n",
            "Epoch 874/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 0.27273\n",
            "Epoch 875/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.27273\n",
            "Epoch 876/1000\n",
            "217/217 [==============================] - 0s 122us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.27273\n",
            "Epoch 877/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.27273\n",
            "Epoch 878/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.27273\n",
            "Epoch 879/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.27273\n",
            "Epoch 880/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.27273\n",
            "Epoch 881/1000\n",
            "217/217 [==============================] - 0s 118us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.27273\n",
            "Epoch 882/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.27273\n",
            "Epoch 883/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.27273\n",
            "Epoch 884/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.27273\n",
            "Epoch 885/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 0.27273\n",
            "Epoch 886/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.27273\n",
            "Epoch 887/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 0.27273\n",
            "Epoch 888/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.27273\n",
            "Epoch 889/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.27273\n",
            "Epoch 890/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.27273\n",
            "Epoch 891/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.27273\n",
            "Epoch 892/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.27273\n",
            "Epoch 893/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.27273\n",
            "Epoch 894/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.27273\n",
            "Epoch 895/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.27273\n",
            "Epoch 896/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.27273\n",
            "Epoch 897/1000\n",
            "217/217 [==============================] - 0s 172us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.27273\n",
            "Epoch 898/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 0.27273\n",
            "Epoch 899/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.27273\n",
            "Epoch 900/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.27273\n",
            "Epoch 901/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 0.27273\n",
            "Epoch 902/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 0.27273\n",
            "Epoch 903/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.27273\n",
            "Epoch 904/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.27273\n",
            "Epoch 905/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.27273\n",
            "Epoch 906/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.27273\n",
            "Epoch 907/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.27273\n",
            "Epoch 908/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.27273\n",
            "Epoch 909/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.27273\n",
            "Epoch 910/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.27273\n",
            "Epoch 911/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.27273\n",
            "Epoch 912/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.27273\n",
            "Epoch 913/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.27273\n",
            "Epoch 914/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.27273\n",
            "Epoch 915/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.27273\n",
            "Epoch 916/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.27273\n",
            "Epoch 917/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.27273\n",
            "Epoch 918/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.27273\n",
            "Epoch 919/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.27273\n",
            "Epoch 920/1000\n",
            "217/217 [==============================] - 0s 126us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.27273\n",
            "Epoch 921/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.27273\n",
            "Epoch 922/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.27273\n",
            "Epoch 923/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.27273\n",
            "Epoch 924/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.27273\n",
            "Epoch 925/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.27273\n",
            "Epoch 926/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.27273\n",
            "Epoch 927/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 0.27273\n",
            "Epoch 928/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.27273\n",
            "Epoch 929/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.27273\n",
            "Epoch 930/1000\n",
            "217/217 [==============================] - 0s 162us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.27273\n",
            "Epoch 931/1000\n",
            "217/217 [==============================] - 0s 143us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.27273\n",
            "Epoch 932/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.27273\n",
            "Epoch 933/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.27273\n",
            "Epoch 934/1000\n",
            "217/217 [==============================] - 0s 147us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.27273\n",
            "Epoch 935/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.27273\n",
            "Epoch 936/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.27273\n",
            "Epoch 937/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.27273\n",
            "Epoch 938/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.27273\n",
            "Epoch 939/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.27273\n",
            "Epoch 940/1000\n",
            "217/217 [==============================] - 0s 138us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.27273\n",
            "Epoch 941/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 0.27273\n",
            "Epoch 942/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.27273\n",
            "Epoch 943/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.27273\n",
            "Epoch 944/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.27273\n",
            "Epoch 945/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.27273\n",
            "Epoch 946/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.27273\n",
            "Epoch 947/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.27273\n",
            "Epoch 948/1000\n",
            "217/217 [==============================] - 0s 124us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.27273\n",
            "Epoch 949/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.27273\n",
            "Epoch 950/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.27273\n",
            "Epoch 951/1000\n",
            "217/217 [==============================] - 0s 160us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.27273\n",
            "Epoch 952/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.27273\n",
            "Epoch 953/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.27273\n",
            "Epoch 954/1000\n",
            "217/217 [==============================] - 0s 132us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.27273\n",
            "Epoch 955/1000\n",
            "217/217 [==============================] - 0s 158us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.27273\n",
            "Epoch 956/1000\n",
            "217/217 [==============================] - 0s 121us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.27273\n",
            "Epoch 957/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.27273\n",
            "Epoch 958/1000\n",
            "217/217 [==============================] - 0s 150us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.27273\n",
            "Epoch 959/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.27273\n",
            "Epoch 960/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.27273\n",
            "Epoch 961/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.27273\n",
            "Epoch 962/1000\n",
            "217/217 [==============================] - 0s 149us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.27273\n",
            "Epoch 963/1000\n",
            "217/217 [==============================] - 0s 140us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.27273\n",
            "Epoch 964/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.27273\n",
            "Epoch 965/1000\n",
            "217/217 [==============================] - 0s 131us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.27273\n",
            "Epoch 966/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.27273\n",
            "Epoch 967/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.27273\n",
            "Epoch 968/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.27273\n",
            "Epoch 969/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.27273\n",
            "Epoch 970/1000\n",
            "217/217 [==============================] - 0s 136us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.27273\n",
            "Epoch 971/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.27273\n",
            "Epoch 972/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.27273\n",
            "Epoch 973/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.27273\n",
            "Epoch 974/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 0.27273\n",
            "Epoch 975/1000\n",
            "217/217 [==============================] - 0s 146us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.27273\n",
            "Epoch 976/1000\n",
            "217/217 [==============================] - 0s 139us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.27273\n",
            "Epoch 977/1000\n",
            "217/217 [==============================] - 0s 144us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.27273\n",
            "Epoch 978/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.27273\n",
            "Epoch 979/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.27273\n",
            "Epoch 980/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.27273\n",
            "Epoch 981/1000\n",
            "217/217 [==============================] - 0s 157us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.27273\n",
            "Epoch 982/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.27273\n",
            "Epoch 983/1000\n",
            "217/217 [==============================] - 0s 148us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.27273\n",
            "Epoch 984/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 0.27273\n",
            "Epoch 985/1000\n",
            "217/217 [==============================] - 0s 127us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 0.27273\n",
            "Epoch 986/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.27273\n",
            "Epoch 987/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.27273\n",
            "Epoch 988/1000\n",
            "217/217 [==============================] - 0s 141us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.27273\n",
            "Epoch 989/1000\n",
            "217/217 [==============================] - 0s 137us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.27273\n",
            "Epoch 990/1000\n",
            "217/217 [==============================] - 0s 145us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.27273\n",
            "Epoch 991/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.27273\n",
            "Epoch 992/1000\n",
            "217/217 [==============================] - 0s 135us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.27273\n",
            "Epoch 993/1000\n",
            "217/217 [==============================] - 0s 142us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.27273\n",
            "Epoch 994/1000\n",
            "217/217 [==============================] - 0s 134us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.27273\n",
            "Epoch 995/1000\n",
            "217/217 [==============================] - 0s 155us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.27273\n",
            "Epoch 996/1000\n",
            "217/217 [==============================] - 0s 128us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.27273\n",
            "Epoch 997/1000\n",
            "217/217 [==============================] - 0s 133us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.27273\n",
            "Epoch 998/1000\n",
            "217/217 [==============================] - 0s 130us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.27273\n",
            "Epoch 999/1000\n",
            "217/217 [==============================] - 0s 125us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.27273\n",
            "Epoch 1000/1000\n",
            "217/217 [==============================] - 0s 129us/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 0.2727 - val_mean_absolute_error: 0.2727\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.27273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f16315f76a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjNlpIttOz7j"
      },
      "source": [
        "DLPred = model.predict(XTest)\n",
        "DL_Pred = []\n",
        "for i in DLPred:\n",
        "  for k in i:\n",
        "    DL_Pred.append(int(k))\n",
        "  \n",
        "DLPred = np.array(DL_Pred) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jELkiR-fXZiD"
      },
      "source": [
        "EVALUATION['DL'] = list(DLPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Kk3bFAzfl1"
      },
      "source": [
        "#EVALUATION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MXZkrJQYe_R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "d7e4ff82-d52f-4039-df62-a65fdea718d6"
      },
      "source": [
        "pd.DataFrame(EVALUATION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SVC</th>\n",
              "      <th>LinearSVC</th>\n",
              "      <th>CaliberatedCV</th>\n",
              "      <th>NB</th>\n",
              "      <th>DT</th>\n",
              "      <th>Bagging</th>\n",
              "      <th>ExtraTree</th>\n",
              "      <th>RF</th>\n",
              "      <th>GB</th>\n",
              "      <th>ADB</th>\n",
              "      <th>ANN</th>\n",
              "      <th>DL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>583 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SVC  LinearSVC  CaliberatedCV  NB  DT  ...  RF  GB  ADB    ANN  DL\n",
              "0      1          1              1   1   1  ...   1   1    1  [1.0]   1\n",
              "1      2          2              2   1   2  ...   2   2    2  [1.0]   1\n",
              "2      1          1              1   2   1  ...   1   1    1  [1.0]   1\n",
              "3      1          1              1   1   1  ...   1   1    1  [1.0]   1\n",
              "4      1          1              1   1   2  ...   1   1    2  [1.0]   1\n",
              "..   ...        ...            ...  ..  ..  ...  ..  ..  ...    ...  ..\n",
              "578    1          2              2   2   1  ...   2   1    1  [1.0]   1\n",
              "579    1          1              1   1   1  ...   1   1    1  [1.0]   1\n",
              "580    1          1              1   1   1  ...   1   1    1  [1.0]   1\n",
              "581    1          1              1   1   1  ...   1   2    1  [1.0]   1\n",
              "582    1          1              1   2   1  ...   1   1    1  [1.0]   1\n",
              "\n",
              "[583 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    }
  ]
}